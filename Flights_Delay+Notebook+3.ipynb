{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%pylab inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "airlines=pd.read_csv('./airlines.csv')\n",
    "airports=pd.read_csv('./airports.csv')\n",
    "flights=pd.read_csv('./flights.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-adb2b5289ffa>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-adb2b5289ffa>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    airports.head(./)\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "airports.head(./)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>...</th>\n",
       "      <th>ARRIVAL_TIME</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_REASON</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>98</td>\n",
       "      <td>N407AS</td>\n",
       "      <td>ANC</td>\n",
       "      <td>SEA</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>2336</td>\n",
       "      <td>N3KUAA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PBI</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>741.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>840</td>\n",
       "      <td>N171US</td>\n",
       "      <td>SFO</td>\n",
       "      <td>CLT</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>258</td>\n",
       "      <td>N3HYAA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>MIA</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>756.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>135</td>\n",
       "      <td>N527AS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>ANC</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER TAIL_NUMBER  \\\n",
       "0  2015      1    1            4      AS             98      N407AS   \n",
       "1  2015      1    1            4      AA           2336      N3KUAA   \n",
       "2  2015      1    1            4      US            840      N171US   \n",
       "3  2015      1    1            4      AA            258      N3HYAA   \n",
       "4  2015      1    1            4      AS            135      N527AS   \n",
       "\n",
       "  ORIGIN_AIRPORT DESTINATION_AIRPORT  SCHEDULED_DEPARTURE      ...        \\\n",
       "0            ANC                 SEA                    5      ...         \n",
       "1            LAX                 PBI                   10      ...         \n",
       "2            SFO                 CLT                   20      ...         \n",
       "3            LAX                 MIA                   20      ...         \n",
       "4            SEA                 ANC                   25      ...         \n",
       "\n",
       "   ARRIVAL_TIME  ARRIVAL_DELAY  DIVERTED  CANCELLED  CANCELLATION_REASON  \\\n",
       "0         408.0          -22.0         0          0                  NaN   \n",
       "1         741.0           -9.0         0          0                  NaN   \n",
       "2         811.0            5.0         0          0                  NaN   \n",
       "3         756.0           -9.0         0          0                  NaN   \n",
       "4         259.0          -21.0         0          0                  NaN   \n",
       "\n",
       "   AIR_SYSTEM_DELAY  SECURITY_DELAY  AIRLINE_DELAY  LATE_AIRCRAFT_DELAY  \\\n",
       "0               NaN             NaN            NaN                  NaN   \n",
       "1               NaN             NaN            NaN                  NaN   \n",
       "2               NaN             NaN            NaN                  NaN   \n",
       "3               NaN             NaN            NaN                  NaN   \n",
       "4               NaN             NaN            NaN                  NaN   \n",
       "\n",
       "   WEATHER_DELAY  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>...</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>ARRIVAL_TIME</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5819079.0</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.732926e+06</td>\n",
       "      <td>5.732926e+06</td>\n",
       "      <td>5.730032e+06</td>\n",
       "      <td>5.730032e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.726566e+06</td>\n",
       "      <td>5.714008e+06</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>5.819079e+06</td>\n",
       "      <td>1.063439e+06</td>\n",
       "      <td>1.063439e+06</td>\n",
       "      <td>1.063439e+06</td>\n",
       "      <td>1.063439e+06</td>\n",
       "      <td>1.063439e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>6.524085e+00</td>\n",
       "      <td>1.570459e+01</td>\n",
       "      <td>3.926941e+00</td>\n",
       "      <td>2.173093e+03</td>\n",
       "      <td>1.329602e+03</td>\n",
       "      <td>1.335204e+03</td>\n",
       "      <td>9.370158e+00</td>\n",
       "      <td>1.607166e+01</td>\n",
       "      <td>1.357171e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.493808e+03</td>\n",
       "      <td>1.476491e+03</td>\n",
       "      <td>4.407057e+00</td>\n",
       "      <td>2.609863e-03</td>\n",
       "      <td>1.544643e-02</td>\n",
       "      <td>1.348057e+01</td>\n",
       "      <td>7.615387e-02</td>\n",
       "      <td>1.896955e+01</td>\n",
       "      <td>2.347284e+01</td>\n",
       "      <td>2.915290e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.405137e+00</td>\n",
       "      <td>8.783425e+00</td>\n",
       "      <td>1.988845e+00</td>\n",
       "      <td>1.757064e+03</td>\n",
       "      <td>4.837518e+02</td>\n",
       "      <td>4.964233e+02</td>\n",
       "      <td>3.708094e+01</td>\n",
       "      <td>8.895574e+00</td>\n",
       "      <td>4.980094e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.071647e+02</td>\n",
       "      <td>5.263197e+02</td>\n",
       "      <td>3.927130e+01</td>\n",
       "      <td>5.102012e-02</td>\n",
       "      <td>1.233201e-01</td>\n",
       "      <td>2.800368e+01</td>\n",
       "      <td>2.143460e+00</td>\n",
       "      <td>4.816164e+01</td>\n",
       "      <td>4.319702e+01</td>\n",
       "      <td>2.043334e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-8.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-8.700000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>7.300000e+02</td>\n",
       "      <td>9.170000e+02</td>\n",
       "      <td>9.210000e+02</td>\n",
       "      <td>-5.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>9.350000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.110000e+03</td>\n",
       "      <td>1.059000e+03</td>\n",
       "      <td>-1.300000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.690000e+03</td>\n",
       "      <td>1.325000e+03</td>\n",
       "      <td>1.330000e+03</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.343000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.520000e+03</td>\n",
       "      <td>1.512000e+03</td>\n",
       "      <td>-5.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.230000e+03</td>\n",
       "      <td>1.730000e+03</td>\n",
       "      <td>1.740000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>1.754000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.918000e+03</td>\n",
       "      <td>1.917000e+03</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>9.855000e+03</td>\n",
       "      <td>2.359000e+03</td>\n",
       "      <td>2.400000e+03</td>\n",
       "      <td>1.988000e+03</td>\n",
       "      <td>2.250000e+02</td>\n",
       "      <td>2.400000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.400000e+03</td>\n",
       "      <td>2.400000e+03</td>\n",
       "      <td>1.971000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.134000e+03</td>\n",
       "      <td>5.730000e+02</td>\n",
       "      <td>1.971000e+03</td>\n",
       "      <td>1.331000e+03</td>\n",
       "      <td>1.211000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            YEAR         MONTH           DAY   DAY_OF_WEEK  FLIGHT_NUMBER  \\\n",
       "count  5819079.0  5.819079e+06  5.819079e+06  5.819079e+06   5.819079e+06   \n",
       "mean      2015.0  6.524085e+00  1.570459e+01  3.926941e+00   2.173093e+03   \n",
       "std          0.0  3.405137e+00  8.783425e+00  1.988845e+00   1.757064e+03   \n",
       "min       2015.0  1.000000e+00  1.000000e+00  1.000000e+00   1.000000e+00   \n",
       "25%       2015.0  4.000000e+00  8.000000e+00  2.000000e+00   7.300000e+02   \n",
       "50%       2015.0  7.000000e+00  1.600000e+01  4.000000e+00   1.690000e+03   \n",
       "75%       2015.0  9.000000e+00  2.300000e+01  6.000000e+00   3.230000e+03   \n",
       "max       2015.0  1.200000e+01  3.100000e+01  7.000000e+00   9.855000e+03   \n",
       "\n",
       "       SCHEDULED_DEPARTURE  DEPARTURE_TIME  DEPARTURE_DELAY      TAXI_OUT  \\\n",
       "count         5.819079e+06    5.732926e+06     5.732926e+06  5.730032e+06   \n",
       "mean          1.329602e+03    1.335204e+03     9.370158e+00  1.607166e+01   \n",
       "std           4.837518e+02    4.964233e+02     3.708094e+01  8.895574e+00   \n",
       "min           1.000000e+00    1.000000e+00    -8.200000e+01  1.000000e+00   \n",
       "25%           9.170000e+02    9.210000e+02    -5.000000e+00  1.100000e+01   \n",
       "50%           1.325000e+03    1.330000e+03    -2.000000e+00  1.400000e+01   \n",
       "75%           1.730000e+03    1.740000e+03     7.000000e+00  1.900000e+01   \n",
       "max           2.359000e+03    2.400000e+03     1.988000e+03  2.250000e+02   \n",
       "\n",
       "         WHEELS_OFF      ...        SCHEDULED_ARRIVAL  ARRIVAL_TIME  \\\n",
       "count  5.730032e+06      ...             5.819079e+06  5.726566e+06   \n",
       "mean   1.357171e+03      ...             1.493808e+03  1.476491e+03   \n",
       "std    4.980094e+02      ...             5.071647e+02  5.263197e+02   \n",
       "min    1.000000e+00      ...             1.000000e+00  1.000000e+00   \n",
       "25%    9.350000e+02      ...             1.110000e+03  1.059000e+03   \n",
       "50%    1.343000e+03      ...             1.520000e+03  1.512000e+03   \n",
       "75%    1.754000e+03      ...             1.918000e+03  1.917000e+03   \n",
       "max    2.400000e+03      ...             2.400000e+03  2.400000e+03   \n",
       "\n",
       "       ARRIVAL_DELAY      DIVERTED     CANCELLED  AIR_SYSTEM_DELAY  \\\n",
       "count   5.714008e+06  5.819079e+06  5.819079e+06      1.063439e+06   \n",
       "mean    4.407057e+00  2.609863e-03  1.544643e-02      1.348057e+01   \n",
       "std     3.927130e+01  5.102012e-02  1.233201e-01      2.800368e+01   \n",
       "min    -8.700000e+01  0.000000e+00  0.000000e+00      0.000000e+00   \n",
       "25%    -1.300000e+01  0.000000e+00  0.000000e+00      0.000000e+00   \n",
       "50%    -5.000000e+00  0.000000e+00  0.000000e+00      2.000000e+00   \n",
       "75%     8.000000e+00  0.000000e+00  0.000000e+00      1.800000e+01   \n",
       "max     1.971000e+03  1.000000e+00  1.000000e+00      1.134000e+03   \n",
       "\n",
       "       SECURITY_DELAY  AIRLINE_DELAY  LATE_AIRCRAFT_DELAY  WEATHER_DELAY  \n",
       "count    1.063439e+06   1.063439e+06         1.063439e+06   1.063439e+06  \n",
       "mean     7.615387e-02   1.896955e+01         2.347284e+01   2.915290e+00  \n",
       "std      2.143460e+00   4.816164e+01         4.319702e+01   2.043334e+01  \n",
       "min      0.000000e+00   0.000000e+00         0.000000e+00   0.000000e+00  \n",
       "25%      0.000000e+00   0.000000e+00         0.000000e+00   0.000000e+00  \n",
       "50%      0.000000e+00   2.000000e+00         3.000000e+00   0.000000e+00  \n",
       "75%      0.000000e+00   1.900000e+01         2.900000e+01   0.000000e+00  \n",
       "max      5.730000e+02   1.971000e+03         1.331000e+03   1.211000e+03  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['DESTINATION_AIRPORT', 'ARRIVAL_DELAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = flights[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEA</td>\n",
       "      <td>-22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PBI</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLT</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIA</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANC</td>\n",
       "      <td>-21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DESTINATION_AIRPORT  ARRIVAL_DELAY\n",
       "0                 SEA          -22.0\n",
       "1                 PBI           -9.0\n",
       "2                 CLT            5.0\n",
       "3                 MIA           -9.0\n",
       "4                 ANC          -21.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_name = airports[['IATA_CODE', 'AIRPORT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRPORT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Lehigh Valley International Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abilene Regional Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>Aberdeen Regional Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Southwest Georgia Regional Airport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                              AIRPORT\n",
       "0       ABE  Lehigh Valley International Airport\n",
       "1       ABI             Abilene Regional Airport\n",
       "2       ABQ    Albuquerque International Sunport\n",
       "3       ABR            Aberdeen Regional Airport\n",
       "4       ABY   Southwest Georgia Regional Airport"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRPORT</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Lehigh Valley International Airport</td>\n",
       "      <td>Allentown</td>\n",
       "      <td>PA</td>\n",
       "      <td>USA</td>\n",
       "      <td>40.65236</td>\n",
       "      <td>-75.44040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abilene Regional Airport</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>32.41132</td>\n",
       "      <td>-99.68190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "      <td>USA</td>\n",
       "      <td>35.04022</td>\n",
       "      <td>-106.60919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>Aberdeen Regional Airport</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "      <td>USA</td>\n",
       "      <td>45.44906</td>\n",
       "      <td>-98.42183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Southwest Georgia Regional Airport</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.53552</td>\n",
       "      <td>-84.19447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                              AIRPORT         CITY STATE COUNTRY  \\\n",
       "0       ABE  Lehigh Valley International Airport    Allentown    PA     USA   \n",
       "1       ABI             Abilene Regional Airport      Abilene    TX     USA   \n",
       "2       ABQ    Albuquerque International Sunport  Albuquerque    NM     USA   \n",
       "3       ABR            Aberdeen Regional Airport     Aberdeen    SD     USA   \n",
       "4       ABY   Southwest Georgia Regional Airport       Albany    GA     USA   \n",
       "\n",
       "   LATITUDE  LONGITUDE  \n",
       "0  40.65236  -75.44040  \n",
       "1  32.41132  -99.68190  \n",
       "2  35.04022 -106.60919  \n",
       "3  45.44906  -98.42183  \n",
       "4  31.53552  -84.19447  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DESTINATION_AIRPORT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-93f1091e4e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflights_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairports_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DESTINATION_AIRPORT'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'IATA_CODE'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m   4720\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4721\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4722\u001b[0;31m                      copy=copy, indicator=indicator)\n\u001b[0m\u001b[1;32m   4723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     51\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m    556\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    557\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DESTINATION_AIRPORT'"
     ]
    }
   ],
   "source": [
    "flights_df = flights_df.merge(airports_name, how='left' , left_on='DESTINATION_AIRPORT' , right_on='IATA_CODE' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRPORT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANC</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>ANC</td>\n",
       "      <td>Ted Stevens Anchorage International Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LAX</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>LAX</td>\n",
       "      <td>Los Angeles International Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SFO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco International Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LAX</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>LAX</td>\n",
       "      <td>Los Angeles International Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEA</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle-Tacoma International Airport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORIGIN_AIRPORT  ARRIVAL_DELAY IATA_CODE  \\\n",
       "0            ANC          -22.0       ANC   \n",
       "1            LAX           -9.0       LAX   \n",
       "2            SFO            5.0       SFO   \n",
       "3            LAX           -9.0       LAX   \n",
       "4            SEA          -21.0       SEA   \n",
       "\n",
       "                                       AIRPORT  \n",
       "0  Ted Stevens Anchorage International Airport  \n",
       "1            Los Angeles International Airport  \n",
       "2          San Francisco International Airport  \n",
       "3            Los Angeles International Airport  \n",
       "4         Seattle-Tacoma International Airport  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'DESTINATION_AIRPORT', u'ARRIVAL_DELAY'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(flights_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_delay_by_airport = flights_df.groupby(['DESTINATION_AIRPORT'])['ARRIVAL_DELAY'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_by_airport = flights_df.groupby(['DESTINATION_AIRPORT'])['ARRIVAL_DELAY'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = arr_by_airport.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {k:{} for k in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in key:\n",
    "    data_dict[k]['delay_mean'] = arr_delay_by_airport[k]\n",
    "    data_dict[k]['arrival'] = arr_by_airport[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [data_dict[k]['arrival'] for k in data_dict]\n",
    "y = [data_dict[k]['delay_mean'] for k in data_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10299': {'arrival': 74, 'delay_mean': 2.6486486486486487}, '14679': {'arrival': 356, 'delay_mean': -2.9831460674157304}, 'SIT': {'arrival': 1325, 'delay_mean': 2.1086792452830188}, '14674': {'arrival': 6, 'delay_mean': 17.666666666666668}, 'ELP': {'arrival': 11843, 'delay_mean': 7.2284049649582034}, 'PSE': {'arrival': 737, 'delay_mean': 10.048846675712348}, '11823': {'arrival': 29, 'delay_mean': -4.8275862068965516}, '15024': {'arrival': 21, 'delay_mean': -7.6190476190476186}, 10257: {'arrival': 692, 'delay_mean': -1.0563583815028901}, '11540': {'arrival': 58, 'delay_mean': 15.431034482758621}, 'YUM': {'arrival': 1856, 'delay_mean': 3.2591594827586206}, 'SCC': {'arrival': 832, 'delay_mean': 0.47836538461538464}, '11982': {'arrival': 18, 'delay_mean': 8.7222222222222214}, '11980': {'arrival': 3, 'delay_mean': -5.666666666666667}, 10268: {'arrival': 54, 'delay_mean': -7.6111111111111107}, 12323: {'arrival': 203, 'delay_mean': 2.3497536945812807}, 'LSE': {'arrival': 1584, 'delay_mean': 1.8371212121212122}, '12523': {'arrival': 20, 'delay_mean': -4.9500000000000002}, 10279: {'arrival': 287, 'delay_mean': 0.28222996515679444}, 'SPS': {'arrival': 1170, 'delay_mean': 4.947863247863248}, '11267': {'arrival': 40, 'delay_mean': -6.8250000000000002}, 12335: {'arrival': 53, 'delay_mean': -13.924528301886792}, '10581': {'arrival': 2, 'delay_mean': 41.0}, 'SPI': {'arrival': 1521, 'delay_mean': 8.7284681130834976}, '13241': {'arrival': 5, 'delay_mean': 8.8000000000000007}, 12339: {'arrival': 2238, 'delay_mean': -2.2497765862377124}, 12343: {'arrival': 49, 'delay_mean': 5.1632653061224492}, 'BWI': {'arrival': 84272, 'delay_mean': 3.602833681412569}, 10299: {'arrival': 1148, 'delay_mean': -0.46864111498257838}, 15370: {'arrival': 1244, 'delay_mean': 0.027331189710610933}, 'WRG': {'arrival': 652, 'delay_mean': 5.3757668711656441}, '11603': {'arrival': 16, 'delay_mean': 2.5625}, 'ALO': {'arrival': 585, 'delay_mean': 5.9452991452991455}, 'AGS': {'arrival': 2347, 'delay_mean': 6.3681295270558156}, 'BGM': {'arrival': 258, 'delay_mean': 4.2131782945736438}, 'CHO': {'arrival': 2111, 'delay_mean': 10.732354334438655}, 10333: {'arrival': 47, 'delay_mean': -6.0638297872340425}, '10208': {'arrival': 10, 'delay_mean': -8.8000000000000007}, '14570': {'arrival': 68, 'delay_mean': 6.7205882352941178}, 'LAN': {'arrival': 1788, 'delay_mean': 4.8775167785234901}, 12389: {'arrival': 206, 'delay_mean': -1.733009708737864}, 'JNU': {'arrival': 4038, 'delay_mean': 2.447251114413076}, 12391: {'arrival': 343, 'delay_mean': 4.4402332361516033}, '13459': {'arrival': 1, 'delay_mean': -6.0}, 12402: {'arrival': 472, 'delay_mean': -2.2457627118644066}, 'TVC': {'arrival': 2666, 'delay_mean': 2.6174043510877718}, 'LAX': {'arrival': 192136, 'delay_mean': 6.109568222509056}, 'PDX': {'arrival': 46796, 'delay_mean': 1.6669373450722285}, 'MAF': {'arrival': 7285, 'delay_mean': 7.7349347975291698}, 14457: {'arrival': 217, 'delay_mean': -2.6543778801843319}, 'LAW': {'arrival': 1238, 'delay_mean': 6.7681744749596122}, '10800': {'arrival': 93, 'delay_mean': -0.70967741935483875}, 'LAR': {'arrival': 554, 'delay_mean': -1.9566787003610109}, 'LAS': {'arrival': 132124, 'delay_mean': 3.6105476673427992}, 'GUC': {'arrival': 470, 'delay_mean': 10.593617021276597}, 'MGM': {'arrival': 3121, 'delay_mean': 8.3255366869593086}, 10372: {'arrival': 54, 'delay_mean': 5.3888888888888893}, 'CMI': {'arrival': 2001, 'delay_mean': 3.6631684157921041}, 'CMH': {'arrival': 23645, 'delay_mean': 5.277606259251427}, 'ALB': {'arrival': 7341, 'delay_mean': 6.5763519956409207}, '14828': {'arrival': 5, 'delay_mean': -12.199999999999999}, 'GUM': {'arrival': 331, 'delay_mean': 17.320241691842899}, '14457': {'arrival': 13, 'delay_mean': 8.1538461538461533}, 'PAH': {'arrival': 624, 'delay_mean': 6.0961538461538458}, '14321': {'arrival': 25, 'delay_mean': -4.5199999999999996}, '11067': {'arrival': 11, 'delay_mean': -2.8181818181818183}, 14487: {'arrival': 58, 'delay_mean': 7.3448275862068968}, 12441: {'arrival': 149, 'delay_mean': -0.55033557046979864}, 14492: {'arrival': 2835, 'delay_mean': -0.26737213403880072}, 10397: {'arrival': 30783, 'delay_mean': -2.1512848000519766}, 'BET': {'arrival': 879, 'delay_mean': 2.7804323094425483}, 'EWN': {'arrival': 564, 'delay_mean': 8.2748226950354606}, 12448: {'arrival': 613, 'delay_mean': -1.6084828711256118}, 12451: {'arrival': 1599, 'delay_mean': -0.67542213883677293}, 10408: {'arrival': 247, 'delay_mean': 0.20647773279352227}, '11986': {'arrival': 46, 'delay_mean': -8.0869565217391308}, '12016': {'arrival': 2, 'delay_mean': -2.5}, '14698': {'arrival': 15, 'delay_mean': -5.9333333333333336}, 'AUS': {'arrival': 41445, 'delay_mean': 6.7179876945349255}, 'SCE': {'arrival': 881, 'delay_mean': 10.206583427922816}, 'SBA': {'arrival': 5902, 'delay_mean': 3.99203659776347}, '11447': {'arrival': 2, 'delay_mean': -16.0}, 'TTN': {'arrival': 2761, 'delay_mean': 17.433900760593989}, 10423: {'arrival': 3741, 'delay_mean': 1.4544239508152901}, 14520: {'arrival': 81, 'delay_mean': -8.5555555555555554}, 14524: {'arrival': 1545, 'delay_mean': 0.36245954692556637}, 'GCK': {'arrival': 635, 'delay_mean': 4.8661417322834648}, 12478: {'arrival': 7765, 'delay_mean': 3.5068898905344494}, 10431: {'arrival': 259, 'delay_mean': 8.1003861003860997}, '10333': {'arrival': 3, 'delay_mean': -11.333333333333334}, 10434: {'arrival': 107, 'delay_mean': -0.7289719626168224}, 'GRR': {'arrival': 10572, 'delay_mean': 5.9366250472947408}, 'ORH': {'arrival': 636, 'delay_mean': 0.4889937106918239}, 14543: {'arrival': 52, 'delay_mean': -11.461538461538462}, 'ORF': {'arrival': 9116, 'delay_mean': 6.1027863097849933}, '15401': {'arrival': 5, 'delay_mean': 16.199999999999999}, 'DSM': {'arrival': 8130, 'delay_mean': 7.4312423124231239}, 'BLI': {'arrival': 702, 'delay_mean': -6.8034188034188032}, '11953': {'arrival': 13, 'delay_mean': 3.1538461538461537}, 'PLN': {'arrival': 731, 'delay_mean': 7.1162790697674421}, '14986': {'arrival': 18, 'delay_mean': 5.6111111111111107}, 'GRK': {'arrival': 3875, 'delay_mean': 5.6978064516129034}, 12511: {'arrival': 53, 'delay_mean': -1.2075471698113207}, '11003': {'arrival': 28, 'delay_mean': 0.7142857142857143}, '14520': {'arrival': 6, 'delay_mean': -12.5}, 10469: {'arrival': 113, 'delay_mean': -9.6017699115044248}, 12519: {'arrival': 70, 'delay_mean': -5.6428571428571432}, 'SBN': {'arrival': 4344, 'delay_mean': 8.6581491712707184}, 14570: {'arrival': 1076, 'delay_mean': 0.47676579925650558}, 12523: {'arrival': 306, 'delay_mean': 1.7908496732026145}, '10781': {'arrival': 35, 'delay_mean': 17.171428571428571}, 14574: {'arrival': 212, 'delay_mean': -2.1933962264150941}, 14576: {'arrival': 692, 'delay_mean': 0.41618497109826591}, '11617': {'arrival': 3, 'delay_mean': -14.0}, 'OKC': {'arrival': 15818, 'delay_mean': 7.2314451890251616}, '11612': {'arrival': 21, 'delay_mean': -8.3333333333333339}, 'IND': {'arrival': 25107, 'delay_mean': 4.9201816226550363}, 14588: {'arrival': 83, 'delay_mean': 6.2289156626506026}, '10631': {'arrival': 4, 'delay_mean': -8.75}, '10141': {'arrival': 4, 'delay_mean': -6.25}, '10140': {'arrival': 105, 'delay_mean': 6.3619047619047615}, 'PBI': {'arrival': 22206, 'delay_mean': 6.866747725839863}, '10146': {'arrival': 5, 'delay_mean': -10.800000000000001}, 'AKN': {'arrival': 63, 'delay_mean': 2.3809523809523809}, 'EWR': {'arrival': 98223, 'delay_mean': 7.4648911151156039}, 'BZN': {'arrival': 3564, 'delay_mean': 2.7168911335578003}, 'SNA': {'arrival': 36723, 'delay_mean': 1.5836669117446831}, '10792': {'arrival': 90, 'delay_mean': -8.5}, '15249': {'arrival': 12, 'delay_mean': -4.5}, 10529: {'arrival': 1612, 'delay_mean': -0.50620347394540943}, '13204': {'arrival': 575, 'delay_mean': -0.30086956521739128}, '15323': {'arrival': 8, 'delay_mean': -4.625}, 'EUG': {'arrival': 3561, 'delay_mean': 4.60685200786296}, '12441': {'arrival': 8, 'delay_mean': -1.875}, 14633: {'arrival': 173, 'delay_mean': -7.3410404624277454}, 14635: {'arrival': 1656, 'delay_mean': -2.8417874396135265}, '12448': {'arrival': 33, 'delay_mean': 19.424242424242426}, 'FCA': {'arrival': 1997, 'delay_mean': 1.2814221331997997}, 'GGG': {'arrival': 616, 'delay_mean': 4.037337662337662}, 15411: {'arrival': 210, 'delay_mean': -0.77142857142857146}, '12339': {'arrival': 133, 'delay_mean': -1.0375939849624061}, 10551: {'arrival': 79, 'delay_mean': -1.860759493670886}, 'AEX': {'arrival': 3055, 'delay_mean': 6.2317512274959084}, '12335': {'arrival': 3, 'delay_mean': -21.333333333333332}, 'MDW': {'arrival': 78478, 'delay_mean': 2.9928769846326357}, 'MDT': {'arrival': 3228, 'delay_mean': 4.742565055762082}, 10561: {'arrival': 172, 'delay_mean': -1.8837209302325582}, '14960': {'arrival': 6, 'delay_mean': 4.166666666666667}, 'MVY': {'arrival': 205, 'delay_mean': 1.7853658536585366}, 'DFW': {'arrival': 231764, 'delay_mean': 5.8662432474413624}, '13244': {'arrival': 71, 'delay_mean': 2.0}, '14783': {'arrival': 28, 'delay_mean': 8.3928571428571423}, 10577: {'arrival': 58, 'delay_mean': -0.53448275862068961}, 14674: {'arrival': 83, 'delay_mean': -5.4819277108433733}, 'BTV': {'arrival': 2906, 'delay_mean': 8.9332415691672402}, 10581: {'arrival': 24, 'delay_mean': 5.458333333333333}, 14679: {'arrival': 5829, 'delay_mean': -2.6153714187682278}, 'VEL': {'arrival': 197, 'delay_mean': -13.761421319796954}, 14683: {'arrival': 2464, 'delay_mean': -0.089691558441558447}, 14685: {'arrival': 621, 'delay_mean': -1.4202898550724639}, '14683': {'arrival': 138, 'delay_mean': 13.586956521739131}, 14689: {'arrival': 531, 'delay_mean': 0.1224105461393597}, 'SBP': {'arrival': 3065, 'delay_mean': 3.5800978792822185}, 10599: {'arrival': 1084, 'delay_mean': 0.79797047970479706}, 14696: {'arrival': 389, 'delay_mean': 1.7917737789203085}, '10849': {'arrival': 20, 'delay_mean': -10.6}, 14698: {'arrival': 230, 'delay_mean': 6.1478260869565213}, '12007': {'arrival': 4, 'delay_mean': -17.0}, '11537': {'arrival': 11, 'delay_mean': -1.0909090909090908}, 'PUB': {'arrival': 255, 'delay_mean': -0.94509803921568625}, 'ACT': {'arrival': 1541, 'delay_mean': 10.63724853990915}, '14814': {'arrival': 31, 'delay_mean': 3.903225806451613}, '11865': {'arrival': 5, 'delay_mean': -14.6}, 14709: {'arrival': 75, 'delay_mean': 0.66666666666666663}, '11867': {'arrival': 4, 'delay_mean': 41.5}, 14711: {'arrival': 58, 'delay_mean': 9.2241379310344822}, '12992': {'arrival': 57, 'delay_mean': 14.070175438596491}, 'ORD': {'arrival': 275864, 'delay_mean': 7.2408034393759246}, 10620: {'arrival': 249, 'delay_mean': -5.261044176706827}, 10627: {'arrival': 323, 'delay_mean': -3.8390092879256965}, 10631: {'arrival': 58, 'delay_mean': -0.58620689655172409}, 'KOA': {'arrival': 11078, 'delay_mean': 0.24959378949268821}, 14730: {'arrival': 797, 'delay_mean': -3.736511919698871}, '16218': {'arrival': 10, 'delay_mean': -6.5999999999999996}, 'CAK': {'arrival': 5985, 'delay_mean': 5.0957393483709277}, '14711': {'arrival': 4, 'delay_mean': -10.5}, 'BJI': {'arrival': 665, 'delay_mean': -1.8}, 'TOL': {'arrival': 901, 'delay_mean': 0.18867924528301888}, '14492': {'arrival': 153, 'delay_mean': -2.215686274509804}, '12191': {'arrival': 247, 'delay_mean': 24.384615384615383}, '13502': {'arrival': 2, 'delay_mean': 3.5}, 14747: {'arrival': 9733, 'delay_mean': -0.8687968766053632}, 'ERI': {'arrival': 707, 'delay_mean': 5.7355021216407351}, '12945': {'arrival': 28, 'delay_mean': -2.2857142857142856}, '12197': {'arrival': 39, 'delay_mean': 4.1282051282051286}, '12323': {'arrival': 12, 'delay_mean': -6.166666666666667}, '12758': {'arrival': 61, 'delay_mean': -0.081967213114754092}, 'SGU': {'arrival': 1773, 'delay_mean': 5.2272983643542021}, 'HIB': {'arrival': 876, 'delay_mean': -0.018264840182648401}, 14771: {'arrival': 13278, 'delay_mean': -0.26178641361650851}, 'CAE': {'arrival': 5049, 'delay_mean': 9.4648445236680523}, 'IAD': {'arrival': 33702, 'delay_mean': 3.9247225683935674}, 10685: {'arrival': 224, 'delay_mean': -5.6026785714285712}, 'ONT': {'arrival': 17806, 'delay_mean': 6.4122205997978208}, 14783: {'arrival': 493, 'delay_mean': -2.3225152129817443}, 10693: {'arrival': 4221, 'delay_mean': -0.87917555081734189}, '12888': {'arrival': 3, 'delay_mean': -16.666666666666668}, '12889': {'arrival': 746, 'delay_mean': 0.30428954423592491}, 'STL': {'arrival': 46273, 'delay_mean': 4.2044172627666239}, 'ABI': {'arrival': 2224, 'delay_mean': 4.0813848920863309}, 14794: {'arrival': 190, 'delay_mean': 4.8421052631578947}, '13495': {'arrival': 200, 'delay_mean': 12.015000000000001}, '15919': {'arrival': 48, 'delay_mean': -0.4375}, 'STT': {'arrival': 4306, 'delay_mean': 5.5868555503947981}, 'ABQ': {'arrival': 18953, 'delay_mean': 5.6146784150266447}, '13476': {'arrival': 14, 'delay_mean': 10.857142857142858}, 'LRD': {'arrival': 2134, 'delay_mean': 3.1208997188378631}, '12280': {'arrival': 11, 'delay_mean': 0.45454545454545453}, '11042': {'arrival': 184, 'delay_mean': 4.4782608695652177}, 12758: {'arrival': 906, 'delay_mean': -1.3377483443708609}, '13377': {'arrival': 13, 'delay_mean': 4.5384615384615383}, 10713: {'arrival': 1091, 'delay_mean': 0.70302474793767189}, 'STX': {'arrival': 933, 'delay_mean': 6.546623794212219}, 14814: {'arrival': 519, 'delay_mean': 2.5183044315992293}, 10721: {'arrival': 9545, 'delay_mean': 0.65028810895756939}, 'BHM': {'arrival': 12307, 'delay_mean': 5.6118469163890472}, 'VPS': {'arrival': 4743, 'delay_mean': 6.3590554501370438}, 10728: {'arrival': 82, 'delay_mean': 0.91463414634146345}, '13577': {'arrival': 33, 'delay_mean': -0.78787878787878785}, 10731: {'arrival': 81, 'delay_mean': 8.2716049382716044}, 14828: {'arrival': 86, 'delay_mean': 4.2209302325581399}, 14831: {'arrival': 3280, 'delay_mean': -2.0499999999999998}, 'FAR': {'arrival': 5588, 'delay_mean': 5.8704366499642093}, 10739: {'arrival': 50, 'delay_mean': -6.0}, 'RST': {'arrival': 1977, 'delay_mean': 3.7632776934749619}, 14842: {'arrival': 134, 'delay_mean': -0.86567164179104472}, 14843: {'arrival': 1742, 'delay_mean': -1.2502870264064294}, '13930': {'arrival': 1519, 'delay_mean': -1.1263989466754443}, 'JFK': {'arrival': 91587, 'delay_mean': 6.6994551628506231}, 10754: {'arrival': 73, 'delay_mean': -1.452054794520548}, 'FNT': {'arrival': 4621, 'delay_mean': 4.4323739450335422}, '13830': {'arrival': 117, 'delay_mean': -0.97435897435897434}, 'MIA': {'arrival': 68309, 'delay_mean': 5.9050491150507254}, 'LIH': {'arrival': 10506, 'delay_mean': 0.37169236626689511}, '11308': {'arrival': 6, 'delay_mean': -9.6666666666666661}, 12819: {'arrival': 172, 'delay_mean': 6.3895348837209305}, 14869: {'arrival': 8259, 'delay_mean': -3.9097953747427048}, '10693': {'arrival': 240, 'delay_mean': 0.5083333333333333}, 10779: {'arrival': 57, 'delay_mean': -6.2982456140350873}, '13931': {'arrival': 46, 'delay_mean': -1.5217391304347827}, 10781: {'arrival': 614, 'delay_mean': 3.3631921824104234}, '13933': {'arrival': 6, 'delay_mean': 6.666666666666667}, 11013: {'arrival': 54, 'delay_mean': -4.4814814814814818}, 10785: {'arrival': 327, 'delay_mean': 4.1987767584097861}, '15041': {'arrival': 4, 'delay_mean': -4.75}, 'SAN': {'arrival': 69294, 'delay_mean': 3.6768262764452913}, 10792: {'arrival': 1488, 'delay_mean': 0.057123655913978492}, '15048': {'arrival': 3, 'delay_mean': -15.333333333333334}, 14893: {'arrival': 3411, 'delay_mean': 0.20668425681618294}, 10800: {'arrival': 1662, 'delay_mean': -0.78158844765342961}, '12177': {'arrival': 3, 'delay_mean': 7.333333333333333}, '10666': {'arrival': 1, 'delay_mean': -6.0}, 'LBB': {'arrival': 4454, 'delay_mean': 8.0166142792995068}, 'LBE': {'arrival': 1247, 'delay_mean': 11.101844426623897}, '12173': {'arrival': 238, 'delay_mean': 0.9327731092436975}, 'RAP': {'arrival': 3211, 'delay_mean': 4.4325755216443472}, '10747': {'arrival': 7, 'delay_mean': 49.571428571428569}, 14905: {'arrival': 57, 'delay_mean': -0.64912280701754388}, 14908: {'arrival': 3328, 'delay_mean': -3.7007211538461537}, 'SAV': {'arrival': 7386, 'delay_mean': 5.241808827511508}, '13487': {'arrival': 571, 'delay_mean': 0.18213660245183888}, '13486': {'arrival': 8, 'delay_mean': 4.0}, '14100': {'arrival': 353, 'delay_mean': -4.2124645892351271}, 'ISP': {'arrival': 4250, 'delay_mean': 9.1830588235294126}, 'DIK': {'arrival': 914, 'delay_mean': -2.8457330415754925}, 10821: {'arrival': 7536, 'delay_mean': -1.5088906581740977}, '14107': {'arrival': 728, 'delay_mean': -2.4519230769230771}, '14109': {'arrival': 3, 'delay_mean': 4.0}, '14108': {'arrival': 26, 'delay_mean': -6.0769230769230766}, '10529': {'arrival': 97, 'delay_mean': -3.9072164948453607}, '14843': {'arrival': 129, 'delay_mean': 7.4573643410852712}, '11274': {'arrival': 4, 'delay_mean': -11.5}, 'HRL': {'arrival': 3025, 'delay_mean': 4.4472727272727273}, 'CHS': {'arrival': 11594, 'delay_mean': 4.0370018975332069}, 12884: {'arrival': 212, 'delay_mean': 1.3820754716981132}, 'EYW': {'arrival': 1948, 'delay_mean': 5.5580082135523616}, 12888: {'arrival': 47, 'delay_mean': -5.8297872340425529}, 12889: {'arrival': 11918, 'delay_mean': -1.2016277898976337}, 12891: {'arrival': 113, 'delay_mean': -1.7079646017699115}, 12892: {'arrival': 16630, 'delay_mean': -2.1500901984365606}, 'JAC': {'arrival': 3440, 'delay_mean': 7.2340116279069768}, 12896: {'arrival': 405, 'delay_mean': 7.7407407407407405}, 10849: {'arrival': 314, 'delay_mean': -5.1242038216560513}, 12898: {'arrival': 107, 'delay_mean': 1.6448598130841121}, 14952: {'arrival': 138, 'delay_mean': 2.152173913043478}, '11471': {'arrival': 4, 'delay_mean': -15.25}, '13158': {'arrival': 27, 'delay_mean': 25.703703703703702}, 14960: {'arrival': 111, 'delay_mean': -3.0360360360360361}, 'SFO': {'arrival': 145409, 'delay_mean': 6.5819584757477188}, 12915: {'arrival': 182, 'delay_mean': -3.5659340659340661}, 10868: {'arrival': 391, 'delay_mean': 0.67519181585677746}, '11905': {'arrival': 3, 'delay_mean': 0.33333333333333331}, 'DBQ': {'arrival': 867, 'delay_mean': 3.3748558246828142}, 'SAT': {'arrival': 29454, 'delay_mean': 5.8695932640727912}, 10874: {'arrival': 461, 'delay_mean': 1.2147505422993492}, '10599': {'arrival': 59, 'delay_mean': 12.203389830508474}, 'TYR': {'arrival': 2199, 'delay_mean': 7.4033651659845381}, 'TYS': {'arrival': 6764, 'delay_mean': 10.613690124186872}, 'PIA': {'arrival': 4501, 'delay_mean': 3.4570095534325707}, 'HDN': {'arrival': 686, 'delay_mean': 5.7026239067055391}, 14986: {'arrival': 288, 'delay_mean': -6.3715277777777777}, '15624': {'arrival': 25, 'delay_mean': 0.20000000000000001}, 12945: {'arrival': 554, 'delay_mean': -2.0595667870036101}, 'PVD': {'arrival': 10794, 'delay_mean': 6.4459885121363723}, 12951: {'arrival': 407, 'delay_mean': 2.4275184275184274}, 12953: {'arrival': 7985, 'delay_mean': 1.7964934251721978}, 12954: {'arrival': 727, 'delay_mean': 0.32599724896836313}, 'ABE': {'arrival': 2220, 'delay_mean': 5.7990990990990987}, 'CHA': {'arrival': 4135, 'delay_mean': 6.2278113663845227}, 'CEC': {'arrival': 175, 'delay_mean': 4.2800000000000002}, 10918: {'arrival': 50, 'delay_mean': 3.98}, 'SYR': {'arrival': 5475, 'delay_mean': 6.3055707762557081}, 15016: {'arrival': 4063, 'delay_mean': -2.3933054393305437}, '14193': {'arrival': 27, 'delay_mean': 9.6666666666666661}, 'BMI': {'arrival': 2631, 'delay_mean': 7.2356518434055488}, 10926: {'arrival': 58, 'delay_mean': 0.62068965517241381}, 15024: {'arrival': 215, 'delay_mean': 1.3069767441860465}, 15027: {'arrival': 64, 'delay_mean': 1.546875}, '14635': {'arrival': 118, 'delay_mean': -5.9406779661016946}, 12982: {'arrival': 824, 'delay_mean': 0.20266990291262135}, '13029': {'arrival': 15, 'delay_mean': 2.8666666666666667}, 'SUX': {'arrival': 591, 'delay_mean': 4.5871404399323179}, '10874': {'arrival': 29, 'delay_mean': -2.0}, 12992: {'arrival': 983, 'delay_mean': -1.45676500508647}, 15041: {'arrival': 56, 'delay_mean': -5.0535714285714288}, 'GCC': {'arrival': 982, 'delay_mean': -2.7291242362525456}, 'DLG': {'arrival': 77, 'delay_mean': -4.1948051948051948}, 'SHV': {'arrival': 5929, 'delay_mean': 7.7707876539045371}, 15048: {'arrival': 54, 'delay_mean': -10.185185185185185}, 'CSG': {'arrival': 1059, 'delay_mean': 8.4192634560906523}, 'DLH': {'arrival': 1660, 'delay_mean': 4.3108433734939755}, '11122': {'arrival': 8, 'delay_mean': -6.875}, 'OME': {'arrival': 626, 'delay_mean': 6.3162939297124598}, '13433': {'arrival': 8, 'delay_mean': -6.0}, 15070: {'arrival': 57, 'delay_mean': 12.719298245614034}, 'MTJ': {'arrival': 975, 'delay_mean': 8.6923076923076916}, '14685': {'arrival': 34, 'delay_mean': 0.61764705882352944}, 10980: {'arrival': 394, 'delay_mean': 0.37055837563451777}, 13029: {'arrival': 273, 'delay_mean': -0.2271062271062271}, 'BTR': {'arrival': 6993, 'delay_mean': 9.055770055770056}, '11697': {'arrival': 359, 'delay_mean': -0.3370473537604457}, '11695': {'arrival': 8, 'delay_mean': -3.75}, '12003': {'arrival': 10, 'delay_mean': 8.1999999999999993}, 10990: {'arrival': 224, 'delay_mean': 3.1116071428571428}, '14689': {'arrival': 33, 'delay_mean': -4.4242424242424239}, '12266': {'arrival': 712, 'delay_mean': 34.518258426966291}, 10994: {'arrival': 1135, 'delay_mean': 0.95066079295154182}, 15096: {'arrival': 605, 'delay_mean': -2.5619834710743801}, 11003: {'arrival': 503, 'delay_mean': -2.0457256461232602}, '10994': {'arrival': 69, 'delay_mean': 5.2028985507246377}, 'HPN': {'arrival': 7136, 'delay_mean': 8.9841647982062778}, '13970': {'arrival': 4, 'delay_mean': 9.25}, 'WYS': {'arrival': 207, 'delay_mean': 4.1932367149758454}, 13061: {'arrival': 187, 'delay_mean': 0.77005347593582885}, '13871': {'arrival': 79, 'delay_mean': 1.1139240506329113}, 'ATL': {'arrival': 343076, 'delay_mean': 2.2262618195385278}, '13873': {'arrival': 4, 'delay_mean': 2.5}, 'ATW': {'arrival': 2763, 'delay_mean': 3.5718422005066954}, 13076: {'arrival': 156, 'delay_mean': -3.3141025641025643}, 'ISN': {'arrival': 2499, 'delay_mean': 0.8231292517006803}, 'ABY': {'arrival': 864, 'delay_mean': 8.6817129629629637}, '15411': {'arrival': 12, 'delay_mean': 11.166666666666666}, '15412': {'arrival': 33, 'delay_mean': 6.9696969696969697}, 11042: {'arrival': 3025, 'delay_mean': -2.1490909090909089}, '11150': {'arrival': 6, 'delay_mean': 1.5}, 11049: {'arrival': 206, 'delay_mean': -3.3932038834951457}, '11013': {'arrival': 3, 'delay_mean': -5.0}, 11057: {'arrival': 9009, 'delay_mean': -1.656010656010656}, '11049': {'arrival': 11, 'delay_mean': 25.636363636363637}, 'ASE': {'arrival': 3260, 'delay_mean': 15.829447852760737}, '11624': {'arrival': 10, 'delay_mean': -14.1}, '10620': {'arrival': 14, 'delay_mean': -4.2142857142857144}, 11066: {'arrival': 2066, 'delay_mean': -3.0827686350435624}, 11067: {'arrival': 195, 'delay_mean': -16.953846153846154}, '10627': {'arrival': 16, 'delay_mean': -4.375}, 'CLL': {'arrival': 2198, 'delay_mean': 1.6060054595086442}, 11076: {'arrival': 55, 'delay_mean': -3.3090909090909091}, '10157': {'arrival': 8, 'delay_mean': 6.25}, 'GTF': {'arrival': 1948, 'delay_mean': 0.18531827515400412}, 13127: {'arrival': 50, 'delay_mean': -8.2599999999999998}, '10408': {'arrival': 12, 'delay_mean': -0.16666666666666666}, 'CLE': {'arrival': 33833, 'delay_mean': 4.5712765643011259}, '10158': {'arrival': 16, 'delay_mean': -4.625}, 'ECP': {'arrival': 4142, 'delay_mean': 3.0560115886045387}, '11977': {'arrival': 24, 'delay_mean': 5.25}, 'GTR': {'arrival': 912, 'delay_mean': 4.5098684210526319}, 'IMT': {'arrival': 610, 'delay_mean': -7.6016393442622947}, '11337': {'arrival': 12, 'delay_mean': -2.4166666666666665}, '10397': {'arrival': 1763, 'delay_mean': -3.4543391945547364}, 'CLT': {'arrival': 98830, 'delay_mean': 2.0697561469189516}, 11097: {'arrival': 29, 'delay_mean': -2.5172413793103448}, 'TLH': {'arrival': 3149, 'delay_mean': 5.3836138456652902}, '12896': {'arrival': 22, 'delay_mean': 48.136363636363633}, 'ITO': {'arrival': 5713, 'delay_mean': 1.4151934185191668}, 'OMA': {'arrival': 16490, 'delay_mean': 5.123044269254093}, '13198': {'arrival': 189, 'delay_mean': 3.8888888888888888}, 11109: {'arrival': 571, 'delay_mean': 3.0577933450087564}, 13158: {'arrival': 567, 'delay_mean': 1.4497354497354498}, 11111: {'arrival': 113, 'delay_mean': -6.6371681415929205}, '12478': {'arrival': 470, 'delay_mean': -2.4276595744680849}, '14893': {'arrival': 196, 'delay_mean': 4.9489795918367347}, 11122: {'arrival': 130, 'delay_mean': -6.9230769230769234}, 'TUL': {'arrival': 13748, 'delay_mean': 6.595868489962176}, '10713': {'arrival': 68, 'delay_mean': 2.6764705882352939}, 15919: {'arrival': 884, 'delay_mean': 1.5180995475113122}, '12343': {'arrival': 3, 'delay_mean': -11.0}, 'PNS': {'arrival': 6412, 'delay_mean': 5.4362133499688081}, '12982': {'arrival': 58, 'delay_mean': 1.9827586206896552}, 13184: {'arrival': 135, 'delay_mean': 5.4296296296296296}, 11140: {'arrival': 341, 'delay_mean': 3.0791788856304985}, 10732: {'arrival': 89, 'delay_mean': 7.202247191011236}, 11146: {'arrival': 211, 'delay_mean': 0.13744075829383887}, 'CMX': {'arrival': 615, 'delay_mean': 4.2032520325203251}, '14794': {'arrival': 12, 'delay_mean': -4.416666666666667}, 13198: {'arrival': 3356, 'delay_mean': -0.30125148986889155}, 'SWF': {'arrival': 680, 'delay_mean': 7.0941176470588232}, 15249: {'arrival': 266, 'delay_mean': -4.041353383458647}, 13204: {'arrival': 8447, 'delay_mean': -0.75139102639990529}, 'BPT': {'arrival': 915, 'delay_mean': 14.631693989071039}, 14489: {'arrival': 189, 'delay_mean': -1.2910052910052909}, '10577': {'arrival': 3, 'delay_mean': -8.6666666666666661}, '11481': {'arrival': 22, 'delay_mean': 4.4090909090909092}, '14027': {'arrival': 104, 'delay_mean': -5.6442307692307692}, 'BIL': {'arrival': 2852, 'delay_mean': 1.7847124824684433}, '13061': {'arrival': 11, 'delay_mean': 36.636363636363633}, '13891': {'arrival': 101, 'delay_mean': 5.4455445544554459}, 'DTW': {'arrival': 106969, 'delay_mean': 3.7124120072170443}, '10155': {'arrival': 6, 'delay_mean': 0.66666666666666663}, 13230: {'arrival': 270, 'delay_mean': -4.5444444444444443}, '13422': {'arrival': 21, 'delay_mean': 17.285714285714285}, 13232: {'arrival': 7172, 'delay_mean': -4.2481873954266591}, '11973': {'arrival': 13, 'delay_mean': 32.53846153846154}, 'EVV': {'arrival': 3716, 'delay_mean': 2.7884822389666306}, 13241: {'arrival': 78, 'delay_mean': 8.2948717948717956}, 'PHL': {'arrival': 64681, 'delay_mean': 5.1390516534994823}, 13244: {'arrival': 1441, 'delay_mean': -1.1027064538514921}, 'BIS': {'arrival': 3403, 'delay_mean': 4.8210402585953567}, 'IAH': {'arrival': 143587, 'delay_mean': 5.6512428005320814}, 15295: {'arrival': 83, 'delay_mean': -15.46987951807229}, '12954': {'arrival': 46, 'delay_mean': -1.3478260869565217}, 'DHN': {'arrival': 1242, 'delay_mean': 5.7375201288244764}, 11203: {'arrival': 137, 'delay_mean': -4.7956204379562042}, '12951': {'arrival': 18, 'delay_mean': 14.388888888888889}, '12953': {'arrival': 446, 'delay_mean': -3.1434977578475336}, 15304: {'arrival': 4822, 'delay_mean': -1.6482787225217752}, 'MLI': {'arrival': 3630, 'delay_mean': 6.8088154269972447}, '14489': {'arrival': 11, 'delay_mean': -3.6363636363636362}, 13264: {'arrival': 228, 'delay_mean': 0.20175438596491227}, '14696': {'arrival': 22, 'delay_mean': 21.045454545454547}, 'SJT': {'arrival': 1553, 'delay_mean': 3.4449452672247265}, 'MLU': {'arrival': 2884, 'delay_mean': 6.7267683772538138}, 'CWA': {'arrival': 1360, 'delay_mean': 8.3308823529411757}, 15323: {'arrival': 151, 'delay_mean': -3.5629139072847682}, 13277: {'arrival': 289, 'delay_mean': 0.39792387543252594}, 'SLC': {'arrival': 96557, 'delay_mean': 0.39525875907495056}, 'MCO': {'arrival': 109546, 'delay_mean': 5.4042411407080131}, 10747: {'arrival': 185, 'delay_mean': -2.3999999999999999}, 'MCI': {'arrival': 38141, 'delay_mean': 5.1624236386041265}, 'RDD': {'arrival': 706, 'delay_mean': 7.569405099150142}, 'LGB': {'arrival': 8693, 'delay_mean': 3.4533532727481884}, 13290: {'arrival': 141, 'delay_mean': -7.0921985815602833}, 'LGA': {'arrival': 94527, 'delay_mean': 10.396436996836883}, 13296: {'arrival': 543, 'delay_mean': -0.30570902394106814}, 'GFK': {'arrival': 713, 'delay_mean': 2.3969144460028051}, '14025': {'arrival': 1, 'delay_mean': 10.0}, '14256': {'arrival': 4, 'delay_mean': -2.0}, 11252: {'arrival': 112, 'delay_mean': -6.3839285714285712}, 13303: {'arrival': 5565, 'delay_mean': -1.7884995507637018}, 11259: {'arrival': 5519, 'delay_mean': 0.85359666606269247}, 15356: {'arrival': 259, 'delay_mean': 3.5057915057915059}, 11267: {'arrival': 705, 'delay_mean': -2.4241134751773048}, 'YAK': {'arrival': 652, 'delay_mean': 0.37116564417177916}, 11150: {'arrival': 105, 'delay_mean': 5.5238095238095237}, 'GSP': {'arrival': 6395, 'delay_mean': 5.3508991399530883}, 11274: {'arrival': 82, 'delay_mean': -5.5121951219512191}, '11193': {'arrival': 90, 'delay_mean': 0.45555555555555555}, 11278: {'arrival': 6384, 'delay_mean': -0.8771929824561403}, 'VLD': {'arrival': 921, 'delay_mean': 6.9478827361563518}, 15376: {'arrival': 1224, 'delay_mean': 0.28104575163398693}, 'BGR': {'arrival': 363, 'delay_mean': 2.2203856749311295}, 15380: {'arrival': 200, 'delay_mean': -1.7050000000000001}, '14576': {'arrival': 43, 'delay_mean': -6.5348837209302326}, '11057': {'arrival': 526, 'delay_mean': -5.4676806083650193}, '13360': {'arrival': 6, 'delay_mean': -13.166666666666666}, 11292: {'arrival': 17064, 'delay_mean': -0.90617674636661982}, 15389: {'arrival': 83, 'delay_mean': -0.26506024096385544}, 13342: {'arrival': 2487, 'delay_mean': -1.97708082026538}, '13367': {'arrival': 19, 'delay_mean': 9.5789473684210531}, 13344: {'arrival': 58, 'delay_mean': 4.5517241379310347}, 11298: {'arrival': 19337, 'delay_mean': -1.663494854424161}, '15607': {'arrival': 4, 'delay_mean': -10.75}, '14771': {'arrival': 805, 'delay_mean': -0.65217391304347827}, '11111': {'arrival': 8, 'delay_mean': 3.0}, 15401: {'arrival': 78, 'delay_mean': 5.1923076923076925}, 11308: {'arrival': 112, 'delay_mean': -1.7946428571428572}, 'BTM': {'arrival': 640, 'delay_mean': -3.9624999999999999}, 'DCA': {'arrival': 71613, 'delay_mean': 4.0565260497395723}, 13360: {'arrival': 110, 'delay_mean': -3.8909090909090911}, 11315: {'arrival': 57, 'delay_mean': -11.491228070175438}, 15412: {'arrival': 587, 'delay_mean': 0.074957410562180582}, 'MQT': {'arrival': 296, 'delay_mean': 16.152027027027028}, 13367: {'arrival': 287, 'delay_mean': -2.7038327526132404}, '15991': {'arrival': 3, 'delay_mean': -14.0}, 'RIC': {'arrival': 14903, 'delay_mean': 7.2139837616587261}, '12884': {'arrival': 11, 'delay_mean': -1.8181818181818181}, 13377: {'arrival': 262, 'delay_mean': 0.80534351145038163}, '10728': {'arrival': 4, 'delay_mean': 73.75}, 11337: {'arrival': 210, 'delay_mean': 1.0571428571428572}, 'IAG': {'arrival': 313, 'delay_mean': 9.8242811501597451}, 13495: {'arrival': 3458, 'delay_mean': -2.1171197223828804}, '10721': {'arrival': 549, 'delay_mean': -4.9581056466302371}, 'MMH': {'arrival': 140, 'delay_mean': 8.8785714285714281}, 'OAJ': {'arrival': 1120, 'delay_mean': 10.025}, '15356': {'arrival': 17, 'delay_mean': 1.0588235294117647}, 'GPT': {'arrival': 3528, 'delay_mean': 4.8347505668934243}, 'BNA': {'arrival': 46475, 'delay_mean': 4.3230338891877356}, '15295': {'arrival': 5, 'delay_mean': -23.800000000000001}, '10685': {'arrival': 13, 'delay_mean': -11.307692307692308}, 'LIT': {'arrival': 10180, 'delay_mean': 6.8814341846758351}, '12389': {'arrival': 11, 'delay_mean': -4.4545454545454541}, 'ITH': {'arrival': 31, 'delay_mean': 4.225806451612903}, 'BOI': {'arrival': 11529, 'delay_mean': 5.956457628588776}, 'PHF': {'arrival': 1242, 'delay_mean': 7.5499194847020936}, 13422: {'arrival': 409, 'delay_mean': 2.5721271393643033}, 'MEM': {'arrival': 14457, 'delay_mean': 5.2603583039358099}, 'PHX': {'arrival': 145378, 'delay_mean': 2.912139388353121}, 'TPA': {'arrival': 63157, 'delay_mean': 4.7839194388587174}, 'JAN': {'arrival': 6914, 'delay_mean': 5.7305467168064794}, 13433: {'arrival': 209, 'delay_mean': -0.37320574162679426}, '10754': {'arrival': 6, 'delay_mean': -0.83333333333333337}, '12264': {'arrival': 172, 'delay_mean': -4.3023255813953485}, '12265': {'arrival': 1, 'delay_mean': -14.0}, '14113': {'arrival': 3, 'delay_mean': 5.0}, 'CRP': {'arrival': 4710, 'delay_mean': 9.1229299363057326}, 'TUS': {'arrival': 14956, 'delay_mean': 6.0636533832575559}, 'CRW': {'arrival': 2322, 'delay_mean': 8.0534022394487508}, '14831': {'arrival': 180, 'delay_mean': 5.9000000000000004}, 15497: {'arrival': 13, 'delay_mean': -8.6923076923076916}, '11203': {'arrival': 7, 'delay_mean': 15.714285714285714}, 13459: {'arrival': 25, 'delay_mean': -3.7599999999999998}, 11413: {'arrival': 169, 'delay_mean': -5.224852071005917}, '10431': {'arrival': 17, 'delay_mean': -5.4117647058823533}, '14057': {'arrival': 265, 'delay_mean': 4.2716981132075471}, '10434': {'arrival': 6, 'delay_mean': -1.6666666666666667}, 11423: {'arrival': 655, 'delay_mean': -2.4137404580152673}, 13476: {'arrival': 217, 'delay_mean': 3.6036866359447006}, 'MFE': {'arrival': 3289, 'delay_mean': 6.8865916692003646}, 'BUR': {'arrival': 18571, 'delay_mean': 4.5923213612621829}, 11433: {'arrival': 9392, 'delay_mean': -0.39554940374787051}, 13485: {'arrival': 797, 'delay_mean': -2.604767879548306}, 13486: {'arrival': 161, 'delay_mean': -6.4223602484472053}, 13487: {'arrival': 10029, 'delay_mean': -1.8310898394655499}, 'MFR': {'arrival': 2428, 'delay_mean': 4.7376441515650738}, 'GEG': {'arrival': 9451, 'delay_mean': 3.4401650618982118}, 'EGE': {'arrival': 1198, 'delay_mean': 10.153589315525876}, 11447: {'arrival': 46, 'delay_mean': -12.717391304347826}, 'BUF': {'arrival': 16516, 'delay_mean': 5.3945265197384353}, 13502: {'arrival': 5, 'delay_mean': -2.0}, 'PIH': {'arrival': 650, 'delay_mean': -0.58923076923076922}, 'MHK': {'arrival': 1569, 'delay_mean': 4.7769279796048441}, '12915': {'arrival': 8, 'delay_mean': 20.625}, 11471: {'arrival': 58, 'delay_mean': 3.896551724137931}, 'LNK': {'arrival': 2569, 'delay_mean': 7.8423511093810818}, '11898': {'arrival': 3, 'delay_mean': -7.333333333333333}, '11996': {'arrival': 32, 'delay_mean': -3.375}, 11481: {'arrival': 352, 'delay_mean': -2.8693181818181817}, 'MHT': {'arrival': 6135, 'delay_mean': 7.1700081499592505}, 'OGG': {'arrival': 20521, 'delay_mean': 2.6033819014667903}, 'MSO': {'arrival': 2057, 'delay_mean': -0.56878949927078271}, 'RNO': {'arrival': 12885, 'delay_mean': 5.1757081878152889}, 13541: {'arrival': 11, 'delay_mean': -3.0909090909090908}, 'ANC': {'arrival': 15819, 'delay_mean': 1.7753966748846324}, 'PSC': {'arrival': 2399, 'delay_mean': 4.1875781575656523}, '10980': {'arrival': 21, 'delay_mean': -7.2857142857142856}, 11503: {'arrival': 3, 'delay_mean': -3.3333333333333335}, '12402': {'arrival': 31, 'delay_mean': -6.290322580645161}, '10868': {'arrival': 22, 'delay_mean': 0.18181818181818182}, 15607: {'arrival': 84, 'delay_mean': 4.7023809523809526}, '11292': {'arrival': 1001, 'delay_mean': 0.25974025974025972}, 'GRB': {'arrival': 4732, 'delay_mean': 4.2457734573119188}, 'PSP': {'arrival': 8902, 'delay_mean': 4.3001572680296567}, 'JLN': {'arrival': 640, 'delay_mean': 9.6812500000000004}, 'LCH': {'arrival': 1805, 'delay_mean': 2.5867036011080331}, 'MSN': {'arrival': 8867, 'delay_mean': 4.0976655012969436}, 'DVL': {'arrival': 475, 'delay_mean': -0.33473684210526317}, 'COS': {'arrival': 6744, 'delay_mean': 10.174080664294188}, '11097': {'arrival': 1, 'delay_mean': 17.0}, 'COU': {'arrival': 1256, 'delay_mean': 10.81608280254777}, 11525: {'arrival': 50, 'delay_mean': 0.12}, 15624: {'arrival': 382, 'delay_mean': -1.3717277486910995}, 13577: {'arrival': 472, 'delay_mean': 1.0127118644067796}, 'ILM': {'arrival': 1832, 'delay_mean': 4.6075327510917035}, 'FSM': {'arrival': 1911, 'delay_mean': 10.306122448979592}, 'SAF': {'arrival': 1446, 'delay_mean': 4.4170124481327804}, 11537: {'arrival': 184, 'delay_mean': 2.4184782608695654}, 'AVL': {'arrival': 2691, 'delay_mean': 6.4960981047937567}, 11540: {'arrival': 981, 'delay_mean': 1.8756371049949032}, 'COD': {'arrival': 644, 'delay_mean': 4.1878881987577641}, 'TWF': {'arrival': 806, 'delay_mean': 3.6066997518610422}, 11823: {'arrival': 482, 'delay_mean': -3.9751037344398341}, '12278': {'arrival': 39, 'delay_mean': -3.8205128205128207}, '14730': {'arrival': 44, 'delay_mean': -5.0}, 'CID': {'arrival': 6611, 'delay_mean': 7.4595371350779001}, '11587': {'arrival': 3, 'delay_mean': -9.3333333333333339}, 13256: {'arrival': 218, 'delay_mean': -1.6467889908256881}, 11577: {'arrival': 54, 'delay_mean': 2.6296296296296298}, '11423': {'arrival': 42, 'delay_mean': -3.4523809523809526}, 'GRI': {'arrival': 611, 'delay_mean': 1.1096563011456628}, '10918': {'arrival': 3, 'delay_mean': -1.3333333333333333}, 11587: {'arrival': 49, 'delay_mean': -7.5102040816326534}, '13964': {'arrival': 0, 'delay_mean': nan}, 14025: {'arrival': 12, 'delay_mean': 18.666666666666668}, '15389': {'arrival': 4, 'delay_mean': 7.0}, 11603: {'arrival': 311, 'delay_mean': 3.167202572347267}, '10279': {'arrival': 16, 'delay_mean': 24.9375}, '15016': {'arrival': 231, 'delay_mean': 1.722943722943723}, '13290': {'arrival': 9, 'delay_mean': 26.444444444444443}, 'MSP': {'arrival': 111146, 'delay_mean': 3.3743634498767388}, 11612: {'arrival': 376, 'delay_mean': -5.8962765957446805}, '13296': {'arrival': 33, 'delay_mean': -8.545454545454545}, '11146': {'arrival': 10, 'delay_mean': -6.5999999999999996}, 11617: {'arrival': 51, 'delay_mean': 3.4313725490196076}, 11618: {'arrival': 8994, 'delay_mean': 2.2587280409161665}, '11140': {'arrival': 20, 'delay_mean': 29.600000000000001}, '11525': {'arrival': 3, 'delay_mean': 10.333333333333334}, 11624: {'arrival': 141, 'delay_mean': -5.0354609929078018}, '13485': {'arrival': 39, 'delay_mean': -2.3333333333333335}, '14543': {'arrival': 3, 'delay_mean': -6.333333333333333}, '14524': {'arrival': 87, 'delay_mean': 3.9195402298850577}, 'INL': {'arrival': 567, 'delay_mean': -5.4091710758377429}, 11630: {'arrival': 165, 'delay_mean': 0.80000000000000004}, '11066': {'arrival': 115, 'delay_mean': -4.8347826086956518}, 'ABR': {'arrival': 657, 'delay_mean': -3.3896499238964992}, 'CDV': {'arrival': 654, 'delay_mean': 0.8883792048929664}, '15497': {'arrival': 1, 'delay_mean': -34.0}, '11638': {'arrival': 40, 'delay_mean': -5.7999999999999998}, 11637: {'arrival': 495, 'delay_mean': -1.709090909090909}, 11638: {'arrival': 615, 'delay_mean': 0.061788617886178863}, 11641: {'arrival': 141, 'delay_mean': -1.7872340425531914}, 'GSO': {'arrival': 6482, 'delay_mean': 7.0402653502005554}, '11637': {'arrival': 28, 'delay_mean': 3.9285714285714284}, '11630': {'arrival': 11, 'delay_mean': 3.7272727272727271}, 11648: {'arrival': 115, 'delay_mean': -6.0869565217391308}, 'HYA': {'arrival': 81, 'delay_mean': 3.5555555555555554}, 'MKE': {'arrival': 29227, 'delay_mean': 3.7196085811065109}, '13795': {'arrival': 6, 'delay_mean': 1.3333333333333333}, 'CIU': {'arrival': 596, 'delay_mean': 5.6208053691275168}, '13796': {'arrival': 217, 'delay_mean': 3.0875576036866361}, 'BDL': {'arrival': 18404, 'delay_mean': 6.5991088893718759}, 'PSG': {'arrival': 653, 'delay_mean': 7.2220520673813171}, '13184': {'arrival': 7, 'delay_mean': 9.5714285714285712}, 'HYS': {'arrival': 561, 'delay_mean': -3.2905525846702317}, 'CVG': {'arrival': 19565, 'delay_mean': 6.1907487860976236}, 'MOT': {'arrival': 1812, 'delay_mean': 4.1931567328918327}, '12819': {'arrival': 12, 'delay_mean': -3.4166666666666665}, 'MRY': {'arrival': 2818, 'delay_mean': 5.3942512420156143}, 'SMX': {'arrival': 652, 'delay_mean': 0.6426380368098159}, '12129': {'arrival': 5, 'delay_mean': -13.0}, 'FAY': {'arrival': 1617, 'delay_mean': 5.4050711193568333}, 'PPG': {'arrival': 107, 'delay_mean': 7.4018691588785046}, 'JAX': {'arrival': 17793, 'delay_mean': 5.8343730680604731}, 'STC': {'arrival': 77, 'delay_mean': 23.025974025974026}, 'HLN': {'arrival': 1421, 'delay_mean': -1.1527093596059113}, '15304': {'arrival': 310, 'delay_mean': -0.18709677419354839}, '11298': {'arrival': 1124, 'delay_mean': 19.34964412811388}, 'FAT': {'arrival': 6949, 'delay_mean': 7.6090084904302779}, 11695: {'arrival': 147, 'delay_mean': 7.8571428571428568}, 11697: {'arrival': 5389, 'delay_mean': 1.1152347374280942}, 'FAI': {'arrival': 2163, 'delay_mean': 2.0933888118354136}, '12217': {'arrival': 19, 'delay_mean': 12.315789473684211}, '14150': {'arrival': 4, 'delay_mean': -10.25}, '14098': {'arrival': 7, 'delay_mean': -8.0}, 'MBS': {'arrival': 1665, 'delay_mean': 6.0138138138138135}, '14908': {'arrival': 190, 'delay_mean': -1.0473684210526315}, '10268': {'arrival': 2, 'delay_mean': -13.5}, 11721: {'arrival': 388, 'delay_mean': -3.3324742268041239}, '15841': {'arrival': 4, 'delay_mean': -13.5}, '14905': {'arrival': 4, 'delay_mean': -16.75}, '14869': {'arrival': 478, 'delay_mean': -3.1359832635983262}, 'CLD': {'arrival': 626, 'delay_mean': 4.3945686900958467}, 'ELM': {'arrival': 2006, 'delay_mean': 7.5777666999002991}, '14842': {'arrival': 6, 'delay_mean': 8.8333333333333339}, 'MOB': {'arrival': 4913, 'delay_mean': 7.2495420313454098}, 'BFL': {'arrival': 2587, 'delay_mean': 5.5017394665635875}, '11995': {'arrival': 28, 'delay_mean': -1.5714285714285714}, '10469': {'arrival': 7, 'delay_mean': -17.285714285714285}, 15841: {'arrival': 57, 'delay_mean': 8.3684210526315788}, 13795: {'arrival': 106, 'delay_mean': 7.3018867924528301}, 13796: {'arrival': 3769, 'delay_mean': -1.2942425046431414}, '13076': {'arrival': 9, 'delay_mean': -0.55555555555555558}, '11259': {'arrival': 304, 'delay_mean': 15.75}, 'RKS': {'arrival': 665, 'delay_mean': -3.2616541353383459}, '11252': {'arrival': 7, 'delay_mean': -6.5714285714285712}, '13277': {'arrival': 15, 'delay_mean': 13.0}, 'SDF': {'arrival': 9959, 'delay_mean': 5.6409278039963855}, '11618': {'arrival': 545, 'delay_mean': -0.23119266055045873}, 'PWM': {'arrival': 4558, 'delay_mean': 7.0236946028960068}, '14574': {'arrival': 11, 'delay_mean': -6.1818181818181817}, 11775: {'arrival': 418, 'delay_mean': -0.062200956937799042}, 'EKO': {'arrival': 518, 'delay_mean': 1.3532818532818534}, 11778: {'arrival': 164, 'delay_mean': 0.88414634146341464}, 'CDC': {'arrival': 570, 'delay_mean': 7.2789473684210524}, 'GST': {'arrival': 74, 'delay_mean': 10.837837837837839}, 13830: {'arrival': 1697, 'delay_mean': 0.0088391278727165592}, 'ILG': {'arrival': 95, 'delay_mean': 21.989473684210527}, '11076': {'arrival': 4, 'delay_mean': -13.0}, 'BOS': {'arrival': 105052, 'delay_mean': 6.2506187411948364}, 'OAK': {'arrival': 41686, 'delay_mean': 4.706903996545603}, 'FWA': {'arrival': 5091, 'delay_mean': 5.5784718130033388}, '11577': {'arrival': 3, 'delay_mean': -11.333333333333334}, 14254: {'arrival': 56, 'delay_mean': 3.2678571428571428}, 'DRO': {'arrival': 2015, 'delay_mean': 1.6838709677419355}, 'FSD': {'arrival': 5180, 'delay_mean': 9.0440154440154448}, 13851: {'arrival': 1545, 'delay_mean': -0.93527508090614886}, '14262': {'arrival': 46, 'delay_mean': 10.326086956521738}, '10185': {'arrival': 12, 'delay_mean': 30.0}, '13264': {'arrival': 15, 'delay_mean': 64.799999999999997}, '11278': {'arrival': 368, 'delay_mean': -3.0108695652173911}, 'PIB': {'arrival': 561, 'delay_mean': 4.0374331550802136}, 'MYR': {'arrival': 4727, 'delay_mean': 7.8360482335519359}, 13871: {'arrival': 1417, 'delay_mean': -0.32815808045165845}, 13873: {'arrival': 53, 'delay_mean': 2.8679245283018866}, 'PIT': {'arrival': 23764, 'delay_mean': 4.8583992593839422}, 13891: {'arrival': 1637, 'delay_mean': 1.8277336591325595}, 'BQN': {'arrival': 1328, 'delay_mean': 12.19277108433735}, 'BQK': {'arrival': 868, 'delay_mean': 5.870967741935484}, 'HNL': {'arrival': 42925, 'delay_mean': 4.8108328479906817}, '14588': {'arrival': 4, 'delay_mean': 71.5}, 11193: {'arrival': 1811, 'delay_mean': -1.428492545554942}, 11865: {'arrival': 83, 'delay_mean': -6.4698795180722888}, 11867: {'arrival': 56, 'delay_mean': -2.9464285714285716}, '11109': {'arrival': 33, 'delay_mean': 15.393939393939394}, '14747': {'arrival': 617, 'delay_mean': 5.9335494327390599}, 13930: {'arrival': 25987, 'delay_mean': -1.4630776926925}, 13931: {'arrival': 863, 'delay_mean': 0.39513325608342992}, 11884: {'arrival': 781, 'delay_mean': -0.72727272727272729}, 13933: {'arrival': 53, 'delay_mean': -0.54716981132075471}, 'KTN': {'arrival': 2292, 'delay_mean': 4.3669284467713787}, '11778': {'arrival': 10, 'delay_mean': 13.699999999999999}, 'MKG': {'arrival': 617, 'delay_mean': 6.6158833063209075}, 15991: {'arrival': 58, 'delay_mean': 0.34482758620689657}, '14307': {'arrival': 66, 'delay_mean': 2.1515151515151514}, 11898: {'arrival': 85, 'delay_mean': 1.6470588235294117}, '11775': {'arrival': 25, 'delay_mean': 4.8399999999999999}, 11905: {'arrival': 55, 'delay_mean': -0.65454545454545454}, '13851': {'arrival': 82, 'delay_mean': 8.2560975609756095}, '10739': {'arrival': 3, 'delay_mean': -13.333333333333334}, 'GJT': {'arrival': 3140, 'delay_mean': 2.1659235668789809}, 13964: {'arrival': 32, 'delay_mean': 26.90625}, '10731': {'arrival': 4, 'delay_mean': -3.25}, 11921: {'arrival': 256, 'delay_mean': -2.62109375}, 13970: {'arrival': 56, 'delay_mean': 0.75}, '14487': {'arrival': 4, 'delay_mean': 0.25}, 'FLL': {'arrival': 72675, 'delay_mean': 5.5670450636394913}, 'DEN': {'arrival': 193033, 'delay_mean': 5.0829236451798394}, 'SMF': {'arrival': 36855, 'delay_mean': 5.4500067833401165}, 'FLG': {'arrival': 1607, 'delay_mean': 5.2028624766645928}, 'RHI': {'arrival': 938, 'delay_mean': -1.3646055437100213}, '12255': {'arrival': 3, 'delay_mean': -9.6666666666666661}, 'AMA': {'arrival': 4071, 'delay_mean': 6.6676492262343405}, 'JMS': {'arrival': 755, 'delay_mean': 3.6013245033112584}, 'ROC': {'arrival': 7272, 'delay_mean': 4.8711496149614959}, '13303': {'arrival': 368, 'delay_mean': 9.7853260869565215}, '15027': {'arrival': 5, 'delay_mean': -4.4000000000000004}, '12391': {'arrival': 22, 'delay_mean': 5.7727272727272725}, 11953: {'arrival': 221, 'delay_mean': 3.2895927601809953}, 14006: {'arrival': 56, 'delay_mean': -1.6964285714285714}, 'APN': {'arrival': 549, 'delay_mean': 1.5992714025500911}, '12156': {'arrival': 8, 'delay_mean': 0.875}, 'ROW': {'arrival': 916, 'delay_mean': 3.8231441048034935}, 'TRI': {'arrival': 1912, 'delay_mean': 2.0706066945606696}, '10135': {'arrival': 11, 'delay_mean': -12.090909090909092}, '10136': {'arrival': 11, 'delay_mean': 26.90909090909091}, 11973: {'arrival': 285, 'delay_mean': -1.5894736842105264}, 11977: {'arrival': 462, 'delay_mean': -1.1255411255411256}, 14027: {'arrival': 1637, 'delay_mean': -0.46548564447159441}, 11980: {'arrival': 53, 'delay_mean': -6.5094339622641506}, '14122': {'arrival': 112, 'delay_mean': -4.3214285714285712}, 11982: {'arrival': 341, 'delay_mean': -2.3753665689149561}, 11986: {'arrival': 874, 'delay_mean': -0.38443935926773454}, '10785': {'arrival': 18, 'delay_mean': -3.9444444444444446}, '10423': {'arrival': 122, 'delay_mean': 29.950819672131146}, 11995: {'arrival': 562, 'delay_mean': -1.0444839857651245}, 11996: {'arrival': 597, 'delay_mean': -1.1507537688442211}, '13256': {'arrival': 15, 'delay_mean': 32.600000000000001}, 12003: {'arrival': 180, 'delay_mean': -3.7444444444444445}, '12451': {'arrival': 100, 'delay_mean': -1.72}, '12519': {'arrival': 4, 'delay_mean': -10.5}, 12007: {'arrival': 83, 'delay_mean': -4.4337349397590362}, 14057: {'arrival': 4282, 'delay_mean': -3.0287248949089212}, 'GNV': {'arrival': 2643, 'delay_mean': 8.2569050321604234}, 'AVP': {'arrival': 1325, 'delay_mean': 9.797735849056604}, '12511': {'arrival': 3, 'delay_mean': 3.0}, 12016: {'arrival': 27, 'delay_mean': -17.925925925925927}, '13232': {'arrival': 429, 'delay_mean': -4.8438228438228439}, '11921': {'arrival': 14, 'delay_mean': 2.0714285714285716}, '13230': {'arrival': 15, 'delay_mean': 1.0666666666666667}, 'OTZ': {'arrival': 638, 'delay_mean': 2.8072100313479624}, '14222': {'arrival': 1, 'delay_mean': -6.0}, '14952': {'arrival': 8, 'delay_mean': 4.875}, 'ACK': {'arrival': 487, 'delay_mean': 4.9137577002053385}, 'SUN': {'arrival': 854, 'delay_mean': 3.3208430913348947}, 'XNA': {'arrival': 8986, 'delay_mean': 8.7720899176496765}, 14098: {'arrival': 105, 'delay_mean': 0.15238095238095239}, 14100: {'arrival': 5690, 'delay_mean': -0.53690685413005268}, 'ACV': {'arrival': 1267, 'delay_mean': 9.6756116811365427}, '10551': {'arrival': 5, 'delay_mean': 14.199999999999999}, '14633': {'arrival': 11, 'delay_mean': 2.0909090909090908}, 'ACY': {'arrival': 3530, 'delay_mean': 11.825495750708216}, 14107: {'arrival': 12156, 'delay_mean': -0.68130964132938465}, 14108: {'arrival': 446, 'delay_mean': -1.1726457399103138}, 14109: {'arrival': 48, 'delay_mean': 4.75}, '10257': {'arrival': 41, 'delay_mean': -8.2195121951219505}, 'IDA': {'arrival': 2229, 'delay_mean': 1.3257065948855988}, 14113: {'arrival': 54, 'delay_mean': -3.6111111111111112}, 'LFT': {'arrival': 5019, 'delay_mean': 5.6200438334329545}, 14122: {'arrival': 2014, 'delay_mean': -2.0511420059582921}, '10990': {'arrival': 12, 'delay_mean': -10.416666666666666}, '13127': {'arrival': 3, 'delay_mean': 2.3333333333333335}, 'ICT': {'arrival': 7905, 'delay_mean': 6.6154332700822263}, 12094: {'arrival': 20, 'delay_mean': 3.0499999999999998}, 'RDU': {'arrival': 31194, 'delay_mean': 5.8729883952042057}, 'OTH': {'arrival': 266, 'delay_mean': 16.7406015037594}, 14150: {'arrival': 58, 'delay_mean': -2.0172413793103448}, 'ADQ': {'arrival': 437, 'delay_mean': -0.66590389016018303}, '10732': {'arrival': 9, 'delay_mean': -1.4444444444444444}, '15096': {'arrival': 31, 'delay_mean': -8.8387096774193541}, 'SJU': {'arrival': 24447, 'delay_mean': 6.8803125127827549}, '14254': {'arrival': 4, 'delay_mean': -8.0}, '11413': {'arrival': 8, 'delay_mean': -0.375}, '14252': {'arrival': 12, 'delay_mean': 1.3333333333333333}, 'RDM': {'arrival': 2111, 'delay_mean': 2.5386072951207956}, 'SJC': {'arrival': 37737, 'delay_mean': 4.2067731934176011}, 16218: {'arrival': 166, 'delay_mean': -1.3493975903614457}, 'ADK': {'arrival': 89, 'delay_mean': -3.191011235955056}, '14709': {'arrival': 6, 'delay_mean': -0.83333333333333337}, 12129: {'arrival': 78, 'delay_mean': 8.0128205128205128}, 'BRW': {'arrival': 819, 'delay_mean': 4.0586080586080584}, 'MEI': {'arrival': 893, 'delay_mean': 9.5991041433370654}, 'BRO': {'arrival': 2266, 'delay_mean': 4.4713150926743159}, 14193: {'arrival': 447, 'delay_mean': -1.0425055928411633}, '10926': {'arrival': 4, 'delay_mean': 29.0}, '15370': {'arrival': 68, 'delay_mean': 16.779411764705884}, 'LEX': {'arrival': 5796, 'delay_mean': 4.0793650793650791}, '11433': {'arrival': 488, 'delay_mean': -1.6926229508196722}, 'BRD': {'arrival': 583, 'delay_mean': -3.6157804459691252}, 12156: {'arrival': 133, 'delay_mean': -3.1578947368421053}, 'HSV': {'arrival': 4442, 'delay_mean': 4.605583070688879}, '15380': {'arrival': 8, 'delay_mean': -14.625}, '12898': {'arrival': 8, 'delay_mean': 10.75}, 'MSY': {'arrival': 38318, 'delay_mean': 4.0378412234459002}, 'AZO': {'arrival': 1743, 'delay_mean': 1.2587492828456683}, 'LWS': {'arrival': 586, 'delay_mean': -6.5443686006825939}, '11884': {'arrival': 47, 'delay_mean': 6.8723404255319149}, '12891': {'arrival': 7, 'delay_mean': 25.285714285714285}, 12173: {'arrival': 3528, 'delay_mean': 2.1536281179138324}, 14222: {'arrival': 8, 'delay_mean': 15.5}, '12892': {'arrival': 1017, 'delay_mean': -0.22123893805309736}, '13342': {'arrival': 149, 'delay_mean': -1.2013422818791946}, 12177: {'arrival': 50, 'delay_mean': -2.5600000000000001}, 'UST': {'arrival': 146, 'delay_mean': 1.9315068493150684}, '13344': {'arrival': 4, 'delay_mean': -5.0}, 10135: {'arrival': 224, 'delay_mean': 1.8348214285714286}, 10136: {'arrival': 183, 'delay_mean': -0.32240437158469948}, 10140: {'arrival': 1706, 'delay_mean': 1.339976553341149}, 10141: {'arrival': 67, 'delay_mean': -1.6268656716417911}, 12191: {'arrival': 4440, 'delay_mean': -0.90653153153153154}, 10146: {'arrival': 82, 'delay_mean': 8.2195121951219505}, 'TXK': {'arrival': 915, 'delay_mean': 4.7661202185792346}, 12197: {'arrival': 674, 'delay_mean': 3.2195845697329375}, 'DAB': {'arrival': 1489, 'delay_mean': 2.0476830087306919}, 10158: {'arrival': 230, 'delay_mean': 4.1478260869565213}, 'DAL': {'arrival': 58605, 'delay_mean': 4.070864260728607}, 10154: {'arrival': 27, 'delay_mean': 7.0}, 10155: {'arrival': 135, 'delay_mean': -2.1407407407407408}, 14252: {'arrival': 197, 'delay_mean': -1.8730964467005076}, 10157: {'arrival': 110, 'delay_mean': 9.6363636363636367}, 12206: {'arrival': 267, 'delay_mean': 3.7415730337078652}, 14256: {'arrival': 55, 'delay_mean': 11.145454545454545}, '11648': {'arrival': 8, 'delay_mean': 4.25}, 'SEA': {'arrival': 110192, 'delay_mean': 1.9694714679831566}, 10165: {'arrival': 9, 'delay_mean': -15.777777777777779}, 14262: {'arrival': 627, 'delay_mean': 0.31259968102073366}, '11721': {'arrival': 23, 'delay_mean': -6.8695652173913047}, '11641': {'arrival': 9, 'delay_mean': -10.444444444444445}, 12217: {'arrival': 368, 'delay_mean': -6.0679347826086953}, 10170: {'arrival': 26, 'delay_mean': 1.7307692307692308}, 'EAU': {'arrival': 636, 'delay_mean': 6.9701257861635222}, 'DAY': {'arrival': 8383, 'delay_mean': 5.6334247882619586}, 'SGF': {'arrival': 6047, 'delay_mean': 8.7067967587233337}, 'HOB': {'arrival': 539, 'delay_mean': 3.0500927643784785}, '10170': {'arrival': 2, 'delay_mean': -14.5}, 10185: {'arrival': 266, 'delay_mean': 5.0789473684210522}, '10821': {'arrival': 454, 'delay_mean': -6.3149779735682818}, '14006': {'arrival': 4, 'delay_mean': -7.0}, 'CNY': {'arrival': 205, 'delay_mean': -11.575609756097561}, 'CPR': {'arrival': 1747, 'delay_mean': -0.83342873497424153}, 'HOU': {'arrival': 51014, 'delay_mean': 4.0768024463872665}, 'ESC': {'arrival': 551, 'delay_mean': 1.1578947368421053}, 'PBG': {'arrival': 281, 'delay_mean': 15.288256227758007}, '11315': {'arrival': 4, 'delay_mean': -6.0}, '10372': {'arrival': 4, 'delay_mean': -2.0}, 12255: {'arrival': 50, 'delay_mean': -5.8399999999999999}, 10208: {'arrival': 212, 'delay_mean': 0.50471698113207553}, 14307: {'arrival': 1026, 'delay_mean': 0.2953216374269006}, 'MLB': {'arrival': 1327, 'delay_mean': 2.7596081386586286}, '15070': {'arrival': 4, 'delay_mean': -0.75}, 12264: {'arrival': 2819, 'delay_mean': -2.7910606598084429}, 12265: {'arrival': 21, 'delay_mean': 3.4761904761904763}, 12266: {'arrival': 12360, 'delay_mean': -1.3632686084142396}, '10561': {'arrival': 11, 'delay_mean': -5.9090909090909092}, '15376': {'arrival': 79, 'delay_mean': 6.4303797468354427}, 14321: {'arrival': 451, 'delay_mean': 1.1419068736141906}, 'SRQ': {'arrival': 3336, 'delay_mean': 2.5068944844124701}, 12278: {'arrival': 648, 'delay_mean': -0.91049382716049387}, 'ROA': {'arrival': 2204, 'delay_mean': 6.7209618874773138}, 12280: {'arrival': 190, 'delay_mean': -2.8684210526315788}, '12206': {'arrival': 11, 'delay_mean': 27.0}, '10779': {'arrival': 4, 'delay_mean': 3.5}, 'RSW': {'arrival': 26983, 'delay_mean': 3.5580550717118187}}\n"
     ]
    }
   ],
   "source": [
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x109dc9490>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109bef6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log10\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x = log10([data_dict[k]['arrival'] for k in data_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x109f9a310>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEKZIIIvCIJQJojgC4IglAn/H5Hj8FSHspKVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x142a28310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yarray = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yarray_withoutnan = yarray[~numpy.isnan(yarray)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False], dtype=bool)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(yarray_withoutnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.linear_model in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.linear_model\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python2.7/site-packages/sklearn/linear_model/__init__.py\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.linear_model` module implements generalized linear models. It\n",
      "    includes Ridge regression, Bayesian Regression, Lasso and Elastic Net\n",
      "    estimators computed with Least Angle Regression and coordinate descent. It also\n",
      "    implements Stochastic Gradient Descent related algorithms.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base\n",
      "    bayes\n",
      "    cd_fast\n",
      "    coordinate_descent\n",
      "    huber\n",
      "    least_angle\n",
      "    logistic\n",
      "    omp\n",
      "    passive_aggressive\n",
      "    perceptron\n",
      "    randomized_l1\n",
      "    ransac\n",
      "    ridge\n",
      "    sag\n",
      "    sag_fast\n",
      "    setup\n",
      "    sgd_fast\n",
      "    stochastic_gradient\n",
      "    tests (package)\n",
      "    theil_sen\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(__builtin__.object)\n",
      "        sklearn.linear_model.huber.HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "        sklearn.linear_model.ransac.RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "    sklearn.base.MetaEstimatorMixin(__builtin__.object)\n",
      "        sklearn.linear_model.ransac.RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "    sklearn.base.RegressorMixin(__builtin__.object)\n",
      "        sklearn.linear_model.base.LinearRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.ARDRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.BayesianRidge(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.coordinate_descent.Lasso\n",
      "                sklearn.linear_model.coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model.coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.LassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskLassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.huber.HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model.least_angle.Lars(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.least_angle.LarsCV\n",
      "                sklearn.linear_model.least_angle.LassoLarsCV\n",
      "            sklearn.linear_model.least_angle.LassoLars\n",
      "                sklearn.linear_model.least_angle.LassoLarsIC\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuit(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuitCV(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ransac.RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.Ridge(sklearn.linear_model.ridge._BaseRidge, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeCV(sklearn.linear_model.ridge._BaseRidgeCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.theil_sen.TheilSenRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "    sklearn.feature_selection.from_model._LearntSelectorMixin(sklearn.base.TransformerMixin)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "        sklearn.linear_model.perceptron.Perceptron(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "        sklearn.linear_model.stochastic_gradient.SGDClassifier(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "        sklearn.linear_model.stochastic_gradient.SGDRegressor(sklearn.linear_model.stochastic_gradient.BaseSGDRegressor, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "    sklearn.linear_model.base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeClassifier(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidge)\n",
      "        sklearn.linear_model.ridge.RidgeClassifierCV(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model.base.LinearModel(abc.NewBase)\n",
      "        sklearn.linear_model.base.LinearRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.ARDRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.bayes.BayesianRidge(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.coordinate_descent.Lasso\n",
      "                sklearn.linear_model.coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model.coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model.huber.HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model.least_angle.Lars(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "            sklearn.linear_model.least_angle.LarsCV\n",
      "                sklearn.linear_model.least_angle.LassoLarsCV\n",
      "            sklearn.linear_model.least_angle.LassoLars\n",
      "                sklearn.linear_model.least_angle.LassoLarsIC\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuit(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.omp.OrthogonalMatchingPursuitCV(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.theil_sen.TheilSenRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.base.SparseCoefMixin(__builtin__.object)\n",
      "        sklearn.linear_model.logistic.LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "            sklearn.linear_model.logistic.LogisticRegressionCV(sklearn.linear_model.logistic.LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "    sklearn.linear_model.coordinate_descent.LinearModelCV(abc.NewBase)\n",
      "        sklearn.linear_model.coordinate_descent.ElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.LassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.coordinate_descent.MultiTaskLassoCV(sklearn.linear_model.coordinate_descent.LinearModelCV, sklearn.base.RegressorMixin)\n",
      "    sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel(abc.NewBase)\n",
      "        sklearn.linear_model.randomized_l1.RandomizedLasso\n",
      "        sklearn.linear_model.randomized_l1.RandomizedLogisticRegression\n",
      "    sklearn.linear_model.ridge._BaseRidge(abc.NewBase)\n",
      "        sklearn.linear_model.ridge.Ridge(sklearn.linear_model.ridge._BaseRidge, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeClassifier(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidge)\n",
      "    sklearn.linear_model.ridge._BaseRidgeCV(sklearn.linear_model.base.LinearModel)\n",
      "        sklearn.linear_model.ridge.RidgeCV(sklearn.linear_model.ridge._BaseRidgeCV, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.ridge.RidgeClassifierCV(sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model.sgd_fast.Classification(sklearn.linear_model.sgd_fast.LossFunction)\n",
      "        sklearn.linear_model.sgd_fast.Hinge\n",
      "        sklearn.linear_model.sgd_fast.Log\n",
      "        sklearn.linear_model.sgd_fast.ModifiedHuber\n",
      "    sklearn.linear_model.sgd_fast.Regression(sklearn.linear_model.sgd_fast.LossFunction)\n",
      "        sklearn.linear_model.sgd_fast.SquaredLoss\n",
      "    sklearn.linear_model.stochastic_gradient.BaseSGDClassifier(abc.NewBase)\n",
      "        sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier\n",
      "        sklearn.linear_model.perceptron.Perceptron(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "        sklearn.linear_model.stochastic_gradient.SGDClassifier(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "    sklearn.linear_model.stochastic_gradient.BaseSGDRegressor(sklearn.linear_model.stochastic_gradient.BaseSGD, sklearn.base.RegressorMixin)\n",
      "        sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor\n",
      "        sklearn.linear_model.stochastic_gradient.SGDRegressor(sklearn.linear_model.stochastic_gradient.BaseSGDRegressor, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "    \n",
      "    class ARDRegression(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "     |  the regression model are assumed to be in Gaussian distributions.\n",
      "     |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      "     |  weights) and alpha (precision of the distribution of the noise).\n",
      "     |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations. Default is 300\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Stop the algorithm if w has converged. Default is 1.e-3.\n",
      "     |  \n",
      "     |  alpha_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  alpha_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  compute_score : boolean, optional\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |      Default is False.\n",
      "     |  \n",
      "     |  threshold_lambda : float, optional\n",
      "     |      threshold for removing (pruning) weights with high precision from\n",
      "     |      the computation. Default is 1.e+4.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |      Default is True.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True.\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : array, shape = (n_features)\n",
      "     |     estimated precisions of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array, shape = (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.ARDRegression()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,\n",
      "     |          copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,\n",
      "     |          n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,\n",
      "     |          verbose=False)\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([ 1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  --------\n",
      "     |  See examples/linear_model/plot_ard.py for an example.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ARDRegression\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the ARDRegression model according to the given training data\n",
      "     |      and parameters.\n",
      "     |      \n",
      "     |      Iterative procedure to maximize the evidence\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      y : array, shape = [n_samples]\n",
      "     |          Target values (integers)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class BayesianRidge(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Bayesian ridge regression\n",
      "     |  \n",
      "     |  Fit a Bayesian ridge model and optimize the regularization parameters\n",
      "     |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations.  Default is 300.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Stop the algorithm if w has converged. Default is 1.e-3.\n",
      "     |  \n",
      "     |  alpha_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter. Default is 1.e-6\n",
      "     |  \n",
      "     |  alpha_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |      Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_1 : float, optional\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter. Default is 1.e-6.\n",
      "     |  \n",
      "     |  lambda_2 : float, optional\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |      Default is 1.e-6\n",
      "     |  \n",
      "     |  compute_score : boolean, optional\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |      Default is False\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |      Default is True.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : float\n",
      "     |     estimated precision of the weights.\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.BayesianRidge()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ... # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,\n",
      "     |          copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,\n",
      "     |          n_iter=300, normalize=False, tol=0.001, verbose=False)\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([ 1.])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See examples/linear_model/plot_bayesian_ridge.py for an example.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BayesianRidge\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples,n_features]\n",
      "     |          Training data\n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ElasticNet(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |          a * L1 + b * L2\n",
      "     |  \n",
      "     |  where::\n",
      "     |  \n",
      "     |          alpha = a + b and l1_ratio = a / (a + b)\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      "     |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      "     |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      "     |  unless you supply your own sequence of alpha.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      "     |      See the notes for the exact mathematical meaning of this\n",
      "     |      parameter.``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      "     |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      "     |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If ``False``, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  precompute : True | False | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``True`` to preserve sparsity.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
      "     |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like, shape (n_targets,)\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SGDRegressor: implements elastic net regression with incremental training.\n",
      "     |  SGDClassifier: implements logistic regression with elastic net penalty\n",
      "     |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19\n",
      "     |      \n",
      "     |      Decision function of the linear model\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              T : array, shape (n_samples,)\n",
      "     |                  The predicted decision function\n",
      "     |  \n",
      "     |  fit(self, X, y, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray or scipy.sparse matrix, (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ElasticNetCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Elastic Net model with iterative fitting along a regularization path\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or array of floats, optional\n",
      "     |      float between 0 and 1 passed to ElasticNet (scaling between\n",
      "     |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      "     |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      "     |  \n",
      "     |  alphas : numpy array, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If None alphas are set automatically\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      The compromise between l1 and l2 penalization chosen by\n",
      "     |      cross validation\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula),\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets, n_features)\n",
      "     |      Independent term in the decision function.\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      "     |      alpha.\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  for an example.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package\n",
      "     |  while alpha corresponds to the lambda parameter in glmnet.\n",
      "     |  More specifically, the optimization objective is::\n",
      "     |  \n",
      "     |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |      + alpha * l1_ratio * ||w||_1\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |      a * L1 + b * L2\n",
      "     |  \n",
      "     |  for::\n",
      "     |  \n",
      "     |      alpha = a + b and l1_ratio = a / (a + b).\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  enet_path\n",
      "     |  ElasticNet\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNetCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as float64, Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Hinge(Classification)\n",
      "     |  Hinge loss for binary classification tasks with y in {-1,1}\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  threshold : float > 0.0\n",
      "     |      Margin threshold. When threshold=1.0, one gets the loss used by SVM.\n",
      "     |      When threshold=0.0, one gets the loss used by the Perceptron.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Hinge\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __new__ = <built-in method __new__ of type object>\n",
      "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class HuberRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  The Huber Regressor optimizes the squared loss for the samples where\n",
      "     |  ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n",
      "     |  where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n",
      "     |  to be optimized. The parameter sigma makes sure that if y is scaled up\n",
      "     |  or down by a certain factor, one does not need to rescale epsilon to\n",
      "     |  achieve the same robustness. Note that this does not take into account\n",
      "     |  the fact that the different features of X may be of different scales.\n",
      "     |  \n",
      "     |  This makes sure that the loss function is not heavily influenced by the\n",
      "     |  outliers while not completely ignoring their effect.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <huber_regression>`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, greater than 1.0, default 1.35\n",
      "     |      The parameter epsilon controls the number of samples that should be\n",
      "     |      classified as outliers. The smaller the epsilon, the more robust it is\n",
      "     |      to outliers.\n",
      "     |  \n",
      "     |  max_iter : int, default 100\n",
      "     |      Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b\n",
      "     |      should run for.\n",
      "     |  \n",
      "     |  alpha : float, default 0.0001\n",
      "     |      Regularization parameter.\n",
      "     |  \n",
      "     |  warm_start : bool, default False\n",
      "     |      This is useful if the stored attributes of a previously used model\n",
      "     |      has to be reused. If set to False, then the coefficients will\n",
      "     |      be rewritten for every call to fit.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default True\n",
      "     |      Whether or not to fit the intercept. This can be set to False\n",
      "     |      if the data is already centered around the origin.\n",
      "     |  \n",
      "     |  tol : float, default 1e-5\n",
      "     |      The iteration will stop when\n",
      "     |      ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n",
      "     |      where pg_i is the i-th component of the projected gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Features got by optimizing the Huber loss.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Bias.\n",
      "     |  \n",
      "     |  scale_ : float\n",
      "     |      The value by which ``|y - X'w - c|`` is scaled down.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations that fmin_l_bfgs_b has run for.\n",
      "     |      Not available if SciPy version is 0.9 and below.\n",
      "     |  \n",
      "     |  outliers_: array, shape (n_samples,)\n",
      "     |      A boolean mask which is set to True where the samples are identified\n",
      "     |      as outliers.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n",
      "     |         Concomitant scale estimates, pg 172\n",
      "     |  .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n",
      "     |         http://statweb.stanford.edu/~owen/reports/hhu.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuberRegressor\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples in the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Weight given to each sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "    \n",
      "    class Lars(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Least Angle Regression model a.k.a. LAR\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, optional\n",
      "     |      Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  fit_path : boolean\n",
      "     |      If True the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller.\n",
      "     |  \n",
      "     |  active_ : list, length = n_alphas | list of n_targets such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas + 1)         | list of n_targets such arrays\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lars(n_nonzero_coefs=1)\n",
      "     |  >>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,\n",
      "     |     n_nonzero_coefs=1, normalize=True, positive=False, precompute='auto',\n",
      "     |     verbose=False)\n",
      "     |  >>> print(clf.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  [ 0. -1.11...]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, verbose=False, normalize=True, precompute='auto', n_nonzero_coefs=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LarsCV(Lars)\n",
      "     |  Cross-validated Least Angle Regression model\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter: integer, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  max_n_alphas : integer, optional\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array, shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array, shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  cv_mse_path_ : array, shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Lasso(ElasticNet)\n",
      "     |  Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Technically the Lasso model is optimizing the same objective function as\n",
      "     |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the L1 term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  precompute : True | False | array-like, default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument. For sparse input\n",
      "     |      this option is always ``True`` to preserve sparsity.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)\n",
      "     |      ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int | array-like, shape (n_targets,)\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "     |     normalize=False, positive=False, precompute=False, random_state=None,\n",
      "     |     selection='cyclic', tol=0.0001, warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [ 0.85  0.  ]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  0.15\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19\n",
      "     |      \n",
      "     |      Decision function of the linear model\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              T : array, shape (n_samples,)\n",
      "     |                  The predicted decision function\n",
      "     |  \n",
      "     |  fit(self, X, y, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray or scipy.sparse matrix, (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Lasso linear model with iterative fitting along a regularization path\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  alphas : numpy array, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If ``None`` alphas are set automatically\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs.\n",
      "     |  \n",
      "     |  positive : bool, optional\n",
      "     |      If positive, restrict regression coefficients to be positive\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, default True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) | (n_targets, n_features)\n",
      "     |      parameter vector (w in the cost function formula)\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting\n",
      "     |  \n",
      "     |  dual_gap_ : ndarray, shape ()\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha\n",
      "     |      (``alpha_``).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  for an example.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  LassoLars\n",
      "     |  Lasso\n",
      "     |  LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, optional\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      for an example.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ---------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[ 0.          0.          0.46874778]\n",
      "     |       [ 0.2159048   0.4425765   0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[ 0.          0.          0.46915237]\n",
      "     |       [ 0.2159048   0.4425765   0.23668876]]\n",
      "     |      \n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as float64, Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoLars(Lars)\n",
      "     |  Lasso model fit with Least Angle Regression a.k.a. Lars\n",
      "     |  \n",
      "     |  It is a Linear Model trained with an L1 prior as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the penalty term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by :class:`LinearRegression`. For numerical reasons, using\n",
      "     |      ``alpha = 0`` with the LassoLars object is not advised and you\n",
      "     |      should prefer the LinearRegression object.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients will not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  fit_path : boolean\n",
      "     |      If ``True`` the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller.\n",
      "     |  \n",
      "     |  active_ : list, length = n_alphas | list of n_targets such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas + 1) or list\n",
      "     |      If a list is passed it's expected to be one of n_targets such arrays.\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int.\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.LassoLars(alpha=0.01)\n",
      "     |  >>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n",
      "     |  ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,\n",
      "     |       fit_path=True, max_iter=500, normalize=True, positive=False,\n",
      "     |       precompute='auto', verbose=False)\n",
      "     |  >>> print(clf.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  [ 0.         -0.963257...]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path\n",
      "     |  lasso_path\n",
      "     |  Lasso\n",
      "     |  LassoCV\n",
      "     |  LassoLarsCV\n",
      "     |  sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoLarsCV(LarsCV)\n",
      "     |  Cross-validated Lasso, using the LARS algorithm\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsCV only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  max_n_alphas : integer, optional\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  coef_path_ : array, shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array, shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array, shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  cv_mse_path_ : array, shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  The object solves the same problem as the LassoCV object. However,\n",
      "     |  unlike the LassoCV, it find the relevant alphas values by itself.\n",
      "     |  In general, because of this property, it will be more stable.\n",
      "     |  However, it is more fragile to heavily multicollinear datasets.\n",
      "     |  \n",
      "     |  It is more efficient than the LassoCV if only a small number of\n",
      "     |  features are selected compared to the total number, for instance if\n",
      "     |  there are very few samples compared to the number of features.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LarsCV, LassoCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsCV\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LarsCV:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LarsCV:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LassoLarsIC(LassoLars)\n",
      "     |  Lasso model fit with Lars using BIC or AIC for model selection\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  AIC is the Akaike information criterion and BIC is the Bayes\n",
      "     |  Information criterion. Such criteria are useful to select the value\n",
      "     |  of the regularization parameter by making a trade-off between the\n",
      "     |  goodness of fit and the complexity of the model. A good model should\n",
      "     |  explain well the data while being simple.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : 'bic' | 'aic'\n",
      "     |      The type of criterion to use.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  positive : boolean (default=False)\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsIC only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto' | array-like\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform. Can be used for\n",
      "     |      early stopping.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the alpha parameter chosen by the information criterion\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by lars_path to find the grid of\n",
      "     |      alphas.\n",
      "     |  \n",
      "     |  criterion_ : array, shape (n_alphas,)\n",
      "     |      The value of the information criteria ('aic', 'bic') across all\n",
      "     |      alphas. The alpha which has the smallest information criteria\n",
      "     |      is chosen.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.LassoLarsIC(criterion='bic')\n",
      "     |  >>> clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  LassoLarsIC(copy_X=True, criterion='bic', eps=..., fit_intercept=True,\n",
      "     |        max_iter=500, normalize=True, positive=False, precompute='auto',\n",
      "     |        verbose=False)\n",
      "     |  >>> print(clf.coef_) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n",
      "     |  [ 0.  -1.11...]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The estimation of the number of degrees of freedom is given by:\n",
      "     |  \n",
      "     |  \"On the degrees of freedom of the lasso\"\n",
      "     |  Hui Zou, Trevor Hastie, and Robert Tibshirani\n",
      "     |  Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Akaike_information_criterion\n",
      "     |  https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  lars_path, LassoLars, LassoLarsCV\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsIC\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='aic', fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  fit(self, X, y, copy_X=True)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          target values.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class LinearRegression(LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Ordinary least squares Linear Regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  n_jobs : int, optional, default 1\n",
      "     |      The number of jobs to use for the computation.\n",
      "     |      If -1 all CPUs are used. This will only provide speedup for\n",
      "     |      n_targets > 1 and sufficient large problems.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features, ) or (n_targets, n_features)\n",
      "     |      Estimated coefficients for the linear regression problem.\n",
      "     |      If multiple targets are passed during the fit (y 2D), this\n",
      "     |      is a 2D array of shape (n_targets, n_features), while if only\n",
      "     |      one target is passed, this is a 1D array of length n_features.\n",
      "     |  \n",
      "     |  residues_ : array, shape (n_targets,) or (1,) or empty\n",
      "     |      Sum of residuals. Squared Euclidean 2-norm for each target passed\n",
      "     |      during the fit. If the linear regression problem is under-determined\n",
      "     |      (the number of linearly independent rows of the training matrix is less\n",
      "     |      than its number of linearly independent columns), this is an empty\n",
      "     |      array. If the target vector passed during the fit is 1-dimensional,\n",
      "     |      this is a (1,) shape array.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  intercept_ : array\n",
      "     |      Independent term in the linear model.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  From the implementation point of view, this is just plain Ordinary\n",
      "     |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearRegression\n",
      "     |      LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples, n_targets]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : numpy array of shape [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             parameter *sample_weight* support to LinearRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  residues_\n",
      "     |      DEPRECATED: ``residues_`` is deprecated and will be removed in 0.19\n",
      "     |      \n",
      "     |      Get the residues of the fitted model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Log(Classification)\n",
      "     |  Logistic regression loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Log\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __new__ = <built-in method __new__ of type object>\n",
      "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      "     |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "     |  scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n",
      "     |  entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "     |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "     |  'sag' and 'newton-cg' solvers.)\n",
      "     |  \n",
      "     |  This class implements regularized logistic regression using the\n",
      "     |  'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n",
      "     |  both dense and sparse input. Use C-ordered arrays or CSR matrices\n",
      "     |  containing 64-bit floats for optimal performance; any other input format\n",
      "     |  will be converted (and copied).\n",
      "     |  \n",
      "     |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "     |  with primal formulation. The 'liblinear' solver supports both L1 and L2\n",
      "     |  regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : str, 'l1' or 'l2', default: 'l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      "     |  \n",
      "     |  dual : bool, default: False\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  C : float, default: 1.0\n",
      "     |      Inverse of regularization strength; must be a positive float.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default: True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default 1.\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default: None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *class_weight='balanced'* instead of deprecated\n",
      "     |         *class_weight='auto'*.\n",
      "     |  \n",
      "     |  max_iter : int, default: 100\n",
      "     |      Useful only for the newton-cg, sag and lbfgs solvers.\n",
      "     |      Maximum number of iterations taken for the solvers to converge.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, default: None\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data. Used only in solvers 'sag' and 'liblinear'.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}, default: 'liblinear'\n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' is\n",
      "     |          faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag' and 'lbfgs' handle\n",
      "     |          multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |          schemes.\n",
      "     |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty.\n",
      "     |  \n",
      "     |      Note that 'sag' fast convergence is only guaranteed on features with\n",
      "     |      approximately the same scale. You can preprocess the data with a\n",
      "     |      scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |  \n",
      "     |  tol : float, default: 1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  multi_class : str, {'ovr', 'multinomial'}, default: 'ovr'\n",
      "     |      Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
      "     |      chosen is 'ovr', then a binary problem is fit for each label. Else\n",
      "     |      the loss minimised is the multinomial loss fit across\n",
      "     |      the entire probability distribution. Works only for the 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |  \n",
      "     |  verbose : int, default: 0\n",
      "     |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      "     |      number for verbosity.\n",
      "     |  \n",
      "     |  warm_start : bool, default: False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      Useless for liblinear solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* to support *lbfgs*, *newton-cg*, *sag* solvers.\n",
      "     |  \n",
      "     |  n_jobs : int, default: 1\n",
      "     |      Number of CPU cores used during the cross-validation loop. If given\n",
      "     |      a value of -1, all cores are used.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |  \n",
      "     |  n_iter_ : array, shape (n_classes,) or (1, )\n",
      "     |      Actual number of iterations for all classes. If binary or multinomial,\n",
      "     |      it returns only 1 element. For liblinear solver, only the maximum\n",
      "     |      number of iteration across all classes is given.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  SGDClassifier : incrementally trained logistic regression (when given\n",
      "     |      the parameter ``loss=\"log\"``).\n",
      "     |  sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  LIBLINEAR -- A Library for Large Linear Classification\n",
      "     |      http://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "     |  \n",
      "     |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "     |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "     |      https://hal.inria.fr/hal-00860051/document\n",
      "     |  \n",
      "     |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "     |      methods for logistic regression and maximum entropy models.\n",
      "     |      Machine Learning 85(1-2):41-75.\n",
      "     |      http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) optional\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to LogisticRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "    \n",
      "    class LogisticRegressionCV(LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "     |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      "     |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      "     |  regularization with primal formulation. The liblinear solver supports both\n",
      "     |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  \n",
      "     |  For the grid of Cs values (that are set by default to be ten values in\n",
      "     |  a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is\n",
      "     |  selected by the cross-validator StratifiedKFold, but it can be changed\n",
      "     |  using the cv parameter. In the case of newton-cg and lbfgs solvers,\n",
      "     |  we warm start along the path i.e guess the initial coefficients of the\n",
      "     |  present fit to be the coefficients got after convergence in the previous\n",
      "     |  fit, so it is supposed to be faster for high-dimensional dense data.\n",
      "     |  \n",
      "     |  For a multiclass problem, the hyperparameters for each class are computed\n",
      "     |  using the best scores got by doing a one-vs-rest in parallel across all\n",
      "     |  folds and classes. Hence this is not the true multinomial loss.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  Cs : list of floats | int\n",
      "     |      Each of the values in Cs describes the inverse of regularization\n",
      "     |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      "     |      in a logarithmic scale between 1e-4 and 1e4.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default: True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', optional\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         class_weight == 'balanced'\n",
      "     |  \n",
      "     |  cv : integer or cross-validation generator\n",
      "     |      The default cross-validation generator used is Stratified K-Folds.\n",
      "     |      If an integer is provided, then it is the number of folds used.\n",
      "     |      See the module :mod:`sklearn.model_selection` module for the\n",
      "     |      list of possible cross-validation objects.\n",
      "     |  \n",
      "     |  penalty : str, 'l1' or 'l2'\n",
      "     |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      "     |  \n",
      "     |  dual : bool\n",
      "     |      Dual or primal formulation. Dual formulation is only implemented for\n",
      "     |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  scoring : callabale\n",
      "     |      Scoring function to use as cross-validation criteria. For a list of\n",
      "     |      scoring functions that can be used, look at :mod:`sklearn.metrics`.\n",
      "     |      The default scoring option used is accuracy_score.\n",
      "     |  \n",
      "     |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}\n",
      "     |      Algorithm to use in the optimization problem.\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' is\n",
      "     |          faster for large ones.\n",
      "     |      - For multiclass problems, only 'newton-cg', 'sag' and 'lbfgs' handle\n",
      "     |          multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      "     |          schemes.\n",
      "     |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty.\n",
      "     |      - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
      "     |          not handle warm-starting.\n",
      "     |  \n",
      "     |      Note that 'sag' fast convergence is only guaranteed on features with\n",
      "     |      approximately the same scale. You can preprocess the data with a\n",
      "     |      scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations of the optimization algorithm.\n",
      "     |  \n",
      "     |  n_jobs : int, optional\n",
      "     |      Number of CPU cores used during the cross-validation loop. If given\n",
      "     |      a value of -1, all cores are used.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      "     |      positive number for verbosity.\n",
      "     |  \n",
      "     |  refit : bool\n",
      "     |      If set to True, the scores are averaged across all folds, and the\n",
      "     |      coefs and the C that corresponds to the best score is taken, and a\n",
      "     |      final refit is done using these parameters.\n",
      "     |      Otherwise the coefs, intercepts and C that correspond to the\n",
      "     |      best scores across folds are averaged.\n",
      "     |  \n",
      "     |  multi_class : str, {'ovr', 'multinomial'}\n",
      "     |      Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
      "     |      chosen is 'ovr', then a binary problem is fit for each label. Else\n",
      "     |      the loss minimised is the multinomial loss fit across\n",
      "     |      the entire probability distribution. Works only for the 'newton-cg',\n",
      "     |      'sag' and 'lbfgs' solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default 1.\n",
      "     |      Useful only when the solver 'liblinear' is used\n",
      "     |      and self.fit_intercept is set to True. In this case, x becomes\n",
      "     |      [x, self.intercept_scaling],\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      intercept_scaling is appended to the instance vector.\n",
      "     |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "     |      as all other features.\n",
      "     |      To lessen the effect of regularization on synthetic feature weight\n",
      "     |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |      `coef_` is readonly property derived from `raw_coef_` that\n",
      "     |      follows the internal memory layout of liblinear.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |      It is available only when parameter intercept is set to True\n",
      "     |      and is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  Cs_ : array\n",
      "     |      Array of C i.e. inverse of regularization parameter values used\n",
      "     |      for cross-validation.\n",
      "     |  \n",
      "     |  coefs_paths_ : array, shape ``(n_folds, len(Cs_), n_features)`` or                    ``(n_folds, len(Cs_), n_features + 1)``\n",
      "     |      dict with classes as the keys, and the path of coefficients obtained\n",
      "     |      during cross-validating across each fold and then across each Cs\n",
      "     |      after doing an OvR for the corresponding class as values.\n",
      "     |      If the 'multi_class' option is set to 'multinomial', then\n",
      "     |      the coefs_paths are the coefficients corresponding to each class.\n",
      "     |      Each dict value has shape ``(n_folds, len(Cs_), n_features)`` or\n",
      "     |      ``(n_folds, len(Cs_), n_features + 1)`` depending on whether the\n",
      "     |      intercept is fit or not.\n",
      "     |  \n",
      "     |  scores_ : dict\n",
      "     |      dict with classes as the keys, and the values as the\n",
      "     |      grid of scores obtained during cross-validating each fold, after doing\n",
      "     |      an OvR for the corresponding class. If the 'multi_class' option\n",
      "     |      given is 'multinomial' then the same scores are repeated across\n",
      "     |      all classes, since this is the multinomial class.\n",
      "     |      Each dict value has shape (n_folds, len(Cs))\n",
      "     |  \n",
      "     |  C_ : array, shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of C that maps to the best scores across every class. If refit is\n",
      "     |      set to False, then for each class, the best C is the average of the\n",
      "     |      C's that correspond to the best scores for each fold.\n",
      "     |  \n",
      "     |  n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      "     |      Actual number of iterations for all classes, folds and Cs.\n",
      "     |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LogisticRegression\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegressionCV\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=1, verbose=0, refit=True, intercept_scaling=1.0, multi_class='ovr', random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,) optional\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LogisticRegression:\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function.\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "    \n",
      "    class ModifiedHuber(Classification)\n",
      "     |  Modified Huber loss for binary classification with y in {-1, 1}\n",
      "     |  \n",
      "     |  This is equivalent to quadratically smoothed SVM with gamma = 2.\n",
      "     |  \n",
      "     |  See T. Zhang 'Solving Large Scale Linear Prediction Problems Using\n",
      "     |  Stochastic Gradient Descent', ICML'04.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ModifiedHuber\n",
      "     |      Classification\n",
      "     |      LossFunction\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __new__ = <built-in method __new__ of type object>\n",
      "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class MultiTaskElasticNet(Lasso)\n",
      "     |  Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it\n",
      "     |      is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula). If a 1D y is         passed in at fit (non multi-task usage), ``coef_`` is then a 1D array\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  MultiTaskElasticNet(alpha=0.1, copy_X=True, fit_intercept=True,\n",
      "     |          l1_ratio=0.5, max_iter=1000, normalize=False, random_state=None,\n",
      "     |          selection='cyclic', tol=0.0001, warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.45663524  0.45612256]\n",
      "     |   [ 0.45663524  0.45612256]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.0872422  0.0872422]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  ElasticNet, MultiTaskLasso\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskLasso model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray, shape (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      y : ndarray, shape (n_samples, n_tasks)\n",
      "     |          Target\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19\n",
      "     |      \n",
      "     |      Decision function of the linear model\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              T : array, shape (n_samples,)\n",
      "     |                  The predicted decision function\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultiTaskElasticNetCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  alphas : array-like, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  l1_ratio : float or array of floats\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it\n",
      "     |      is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs. Note that this is used only if multiple values for\n",
      "     |      l1_ratio are given.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      best l1_ratio obtained by cross-validation.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNetCV()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]],\n",
      "     |  ...         [[0, 0], [1, 1], [2, 2]])\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,\n",
      "     |         fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,\n",
      "     |         n_jobs=1, normalize=False, random_state=None, selection='cyclic',\n",
      "     |         tol=0.0001, verbose=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.52875032  0.46958558]\n",
      "     |   [ 0.52875032  0.46958558]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.00166409  0.00166409]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskLassoCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as float64, Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultiTaskLasso(MultiTaskElasticNet)\n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      parameter vector (W in the cost function formula)\n",
      "     |  \n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "     |          normalize=False, random_state=None, selection='cyclic', tol=0.0001,\n",
      "     |          warm_start=False)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[ 0.89393398  0.        ]\n",
      "     |   [ 0.89393398  0.        ]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [ 0.10606602  0.10606602]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Lasso, MultiTaskElasticNet\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLasso\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskLasso model with coordinate descent\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      -----------\n",
      "     |      X : ndarray, shape (n_samples, n_features)\n",
      "     |          Data\n",
      "     |      y : ndarray, shape (n_samples, n_tasks)\n",
      "     |          Target\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      \n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "     |      Compute elastic net path with coordinate descent\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      l1_ratio : float, optional\n",
      "     |          float between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "     |      \n",
      "     |      eps : float\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      check_input : bool, default True\n",
      "     |          Skip input validation checks, including the Gram matrix when provided\n",
      "     |          assuming there are handled by the caller when check_input=False.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      ElasticNet\n",
      "     |      ElasticNetCV\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19\n",
      "     |      \n",
      "     |      Decision function of the linear model\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              T : array, shape (n_samples,)\n",
      "     |                  The predicted decision function\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      sparse representation of the fitted ``coef_``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultiTaskLassoCV(LinearModelCV, sklearn.base.RegressorMixin)\n",
      "     |  Multi-task L1/L2 Lasso with built-in cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskLasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, optional\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  alphas : array-like, optional\n",
      "     |      List of alphas where to compute the models.\n",
      "     |      If not provided, set automatically.\n",
      "     |  \n",
      "     |  n_alphas : int, optional\n",
      "     |      Number of alphas along the regularization path\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If ``True``, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when ``fit_intercept`` is set to ``False``.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator\n",
      "     |      with ``normalize=False``.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  verbose : bool or integer\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs. Note that this is used only if multiple values for\n",
      "     |      l1_ratio are given.\n",
      "     |  \n",
      "     |  selection : str, default 'cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator that selects\n",
      "     |      a random feature to update. Useful only when selection is set to\n",
      "     |      'random'.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : array, shape (n_tasks,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_tasks, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation\n",
      "     |  \n",
      "     |  mse_path_ : array, shape (n_alphas, n_folds)\n",
      "     |      mean square error for the test set on each fold, varying alpha\n",
      "     |  \n",
      "     |  alphas_ : numpy array, shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet\n",
      "     |  ElasticNetCV\n",
      "     |  MultiTaskElasticNetCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLassoCV\n",
      "     |      LinearModelCV\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "     |      Compute Lasso path with coordinate descent\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      eps : float, optional\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``\n",
      "     |      \n",
      "     |      n_alphas : int, optional\n",
      "     |          Number of alphas along the regularization path\n",
      "     |      \n",
      "     |      alphas : ndarray, optional\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically\n",
      "     |      \n",
      "     |      precompute : True | False | 'auto' | array-like\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like, optional\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : boolean, optional, default True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features, ) | None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or integer\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      params : kwargs\n",
      "     |          keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      positive : bool, default False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |      \n",
      "     |      return_n_iter : bool\n",
      "     |          whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : array, shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : array, shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : array-like, shape (n_alphas,)\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      See examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "     |      for an example.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ---------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[ 0.          0.          0.46874778]\n",
      "     |       [ 0.2159048   0.4425765   0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[ 0.          0.          0.46915237]\n",
      "     |       [ 0.2159048   0.4425765   0.23668876]]\n",
      "     |      \n",
      "     |      \n",
      "     |      See also\n",
      "     |      --------\n",
      "     |      lars_path\n",
      "     |      Lasso\n",
      "     |      LassoLars\n",
      "     |      LassoCV\n",
      "     |      LassoLarsCV\n",
      "     |      sklearn.decomposition.sparse_encode\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model with coordinate descent\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like}, shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as float64, Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class OrthogonalMatchingPursuit(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Orthogonal Matching Pursuit model (OMP)\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, optional\n",
      "     |      Desired number of non-zero entries in the solution. If None (by\n",
      "     |      default) this value is set to 10% of n_features.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to `False`.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  precompute : {True, False, 'auto'}, default 'auto'\n",
      "     |      Whether to use a precomputed Gram and Xy matrix to speed up\n",
      "     |      calculations. Improves performance when `n_targets` or `n_samples` is\n",
      "     |      very large. Note that if you already have such matrices, you can pass\n",
      "     |      them directly to the fit method.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_features, n_targets)\n",
      "     |      parameter vector (w in the formula)\n",
      "     |  \n",
      "     |  intercept_ : float or array, shape (n_targets,)\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "     |  Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "     |  Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "     |  (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "     |  \n",
      "     |  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "     |  M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "     |  Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "     |  http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuit\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute='auto')\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class OrthogonalMatchingPursuitCV(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Cross-validated Orthogonal Matching Pursuit model (OMP)\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, optional\n",
      "     |      Whether the design matrix X must be copied by the algorithm. A false\n",
      "     |      value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "     |      copy is made anyway.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to `False`.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum numbers of iterations to perform, therefore maximum features\n",
      "     |      to include. 10% of ``n_features`` but at least 5 if available.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 3-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : float or array, shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : array, shape (n_features,) or (n_features, n_targets)\n",
      "     |      Parameter vector (w in the problem formulation).\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      Estimated number of non-zero coefficients giving the best mean squared\n",
      "     |      error over the cross-validation folds.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target for the model refit with\n",
      "     |      the best hyperparameters got by cross-validating across all folds.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  orthogonal_mp\n",
      "     |  orthogonal_mp_gram\n",
      "     |  lars_path\n",
      "     |  Lars\n",
      "     |  LassoLars\n",
      "     |  OrthogonalMatchingPursuit\n",
      "     |  LarsCV\n",
      "     |  LassoLarsCV\n",
      "     |  decomposition.sparse_encode\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuitCV\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=1, verbose=False)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class PassiveAggressiveClassifier(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Passive Aggressive Classifier\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=False\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation. -1 means 'all CPUs'. Defaults\n",
      "     |      to 1.\n",
      "     |  \n",
      "     |  loss : string, optional\n",
      "     |      The loss function to be used:\n",
      "     |      hinge: equivalent to PA-I in the reference paper.\n",
      "     |      squared_hinge: equivalent to PA-II in the reference paper.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         parameter *class_weight* to automatically weight samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDClassifier\n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveClassifier\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGDClassifier\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1.0, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, loss='hinge', n_jobs=1, random_state=None, warm_start=False, class_weight=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_classes,n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [n_classes]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape = [n_classes]\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<type 'sklearn.linear_model....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class PassiveAggressiveRegressor(sklearn.linear_model.stochastic_gradient.BaseSGDRegressor)\n",
      "     |  Passive Aggressive Regressor\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      If the difference between the current prediction and the correct label\n",
      "     |      is below this threshold, the model is not updated.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  loss : string, optional\n",
      "     |      The loss function to be used:\n",
      "     |      epsilon_insensitive: equivalent to PA-I in the reference paper.\n",
      "     |      squared_epsilon_insensitive: equivalent to PA-II in the reference\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveRegressor\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGDRegressor\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1.0, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False)\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [1]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              array, shape (n_samples,)\n",
      "     |                 Predicted target values per element in X.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model.stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<type 'sklearn.linear_model....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class Perceptron(sklearn.linear_model.stochastic_gradient.BaseSGDClassifier, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "     |  Perceptron\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <perceptron>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  penalty : None, 'l2' or 'l1' or 'elasticnet'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to None.\n",
      "     |  \n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the regularization term if regularization is\n",
      "     |      used. Defaults to 0.0001\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs).\n",
      "     |      Defaults to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, optional, default True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation. -1 means 'all CPUs'. Defaults\n",
      "     |      to 1.\n",
      "     |  \n",
      "     |  eta0 : double\n",
      "     |      Constant by which the updates are multiplied. Defaults to 1.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  `Perceptron` and `SGDClassifier` share the same underlying implementation.\n",
      "     |  In fact, `Perceptron()` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      "     |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  \n",
      "     |  SGDClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Perceptron\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGDClassifier\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.stochastic_gradient.BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty=None, alpha=0.0001, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, eta0=1.0, n_jobs=1, random_state=0, class_weight=None, warm_start=False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape (n_classes,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape (n_classes,)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model.stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<type 'sklearn.linear_model....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.stochastic_gradient.BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class RANSACRegressor(sklearn.base.BaseEstimator, sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin)\n",
      "     |  RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  \n",
      "     |  RANSAC is an iterative algorithm for the robust estimation of parameters\n",
      "     |  from a subset of inliers from the complete data set. More information can\n",
      "     |  be found in the general documentation of linear models.\n",
      "     |  \n",
      "     |  A detailed description of the algorithm can be found in the documentation\n",
      "     |  of the ``linear_model`` sub-package.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ransac_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, optional\n",
      "     |      Base estimator object which implements the following methods:\n",
      "     |  \n",
      "     |       * `fit(X, y)`: Fit model to given training data and target values.\n",
      "     |       * `score(X, y)`: Returns the mean accuracy on the given test data,\n",
      "     |         which is used for the stop criterion defined by `stop_score`.\n",
      "     |         Additionally, the score is used to decide which of two equally\n",
      "     |         large consensus sets is chosen as the better one.\n",
      "     |  \n",
      "     |      If `base_estimator` is None, then\n",
      "     |      ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for\n",
      "     |      target values of dtype float.\n",
      "     |  \n",
      "     |      Note that the current implementation only supports regression\n",
      "     |      estimators.\n",
      "     |  \n",
      "     |  min_samples : int (>= 1) or float ([0, 1]), optional\n",
      "     |      Minimum number of samples chosen randomly from original data. Treated\n",
      "     |      as an absolute number of samples for `min_samples >= 1`, treated as a\n",
      "     |      relative number `ceil(min_samples * X.shape[0]`) for\n",
      "     |      `min_samples < 1`. This is typically chosen as the minimal number of\n",
      "     |      samples necessary to estimate the given `base_estimator`. By default a\n",
      "     |      ``sklearn.linear_model.LinearRegression()`` estimator is assumed and\n",
      "     |      `min_samples` is chosen as ``X.shape[1] + 1``.\n",
      "     |  \n",
      "     |  residual_threshold : float, optional\n",
      "     |      Maximum residual for a data sample to be classified as an inlier.\n",
      "     |      By default the threshold is chosen as the MAD (median absolute\n",
      "     |      deviation) of the target values `y`.\n",
      "     |  \n",
      "     |  is_data_valid : callable, optional\n",
      "     |      This function is called with the randomly selected data before the\n",
      "     |      model is fitted to it: `is_data_valid(X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |  \n",
      "     |  is_model_valid : callable, optional\n",
      "     |      This function is called with the estimated model and the randomly\n",
      "     |      selected data: `is_model_valid(model, X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |      Rejecting samples with this function is computationally costlier than\n",
      "     |      with `is_data_valid`. `is_model_valid` should therefore only be used if\n",
      "     |      the estimated model is needed for making the rejection decision.\n",
      "     |  \n",
      "     |  max_trials : int, optional\n",
      "     |      Maximum number of iterations for random sample selection.\n",
      "     |  \n",
      "     |  stop_n_inliers : int, optional\n",
      "     |      Stop iteration if at least this number of inliers are found.\n",
      "     |  \n",
      "     |  stop_score : float, optional\n",
      "     |      Stop iteration if score is greater equal than this threshold.\n",
      "     |  \n",
      "     |  stop_probability : float in range [0, 1], optional\n",
      "     |      RANSAC iteration stops if at least one outlier-free set of the training\n",
      "     |      data is sampled in RANSAC. This requires to generate at least N\n",
      "     |      samples (iterations)::\n",
      "     |  \n",
      "     |          N >= log(1 - probability) / log(1 - e**m)\n",
      "     |  \n",
      "     |      where the probability (confidence) is typically set to high value such\n",
      "     |      as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n",
      "     |      the total number of samples.\n",
      "     |  \n",
      "     |  residual_metric : callable, optional\n",
      "     |      Metric to reduce the dimensionality of the residuals to 1 for\n",
      "     |      multi-dimensional target values ``y.shape[1] > 1``. By default the sum\n",
      "     |      of absolute differences is used::\n",
      "     |  \n",
      "     |          lambda dy: np.sum(np.abs(dy), axis=1)\n",
      "     |  \n",
      "     |      NOTE: residual_metric is deprecated from 0.18 and will be removed in 0.20\n",
      "     |      Use ``loss`` instead.\n",
      "     |  \n",
      "     |  loss : string, callable, optional, default \"absolute_loss\"\n",
      "     |      String inputs, \"absolute_loss\" and \"squared_loss\" are supported which\n",
      "     |      find the absolute loss and squared loss per sample\n",
      "     |      respectively.\n",
      "     |  \n",
      "     |      If ``loss`` is a callable, then it should be a function that takes\n",
      "     |      two arrays as inputs, the true and predicted value and returns a 1-D\n",
      "     |      array with the ``i``th value of the array corresponding to the loss\n",
      "     |      on `X[i]`.\n",
      "     |  \n",
      "     |      If the loss on a sample is greater than the ``residual_threshold``, then\n",
      "     |      this sample is classified as an outlier.\n",
      "     |  \n",
      "     |  random_state : integer or numpy.RandomState, optional\n",
      "     |      The generator used to initialize the centers. If an integer is\n",
      "     |      given, it fixes the seed. Defaults to the global numpy random\n",
      "     |      number generator.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : object\n",
      "     |      Best fitted model (copy of the `base_estimator` object).\n",
      "     |  \n",
      "     |  n_trials_ : int\n",
      "     |      Number of random selection trials until one of the stop criteria is\n",
      "     |      met. It is always ``<= max_trials``.\n",
      "     |  \n",
      "     |  inlier_mask_ : bool array of shape [n_samples]\n",
      "     |      Boolean mask of inliers classified as ``True``.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] https://en.wikipedia.org/wiki/RANSAC\n",
      "     |  .. [2] http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf\n",
      "     |  .. [3] http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RANSACRegressor\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, residual_metric=None, loss='absolute_loss', random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit estimator using RANSAC algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |          raises error if sample_weight is passed and base_estimator\n",
      "     |          fit method does not support it.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If no valid consensus set could be found. This occurs if\n",
      "     |          `is_data_valid` and `is_model_valid` return False for all\n",
      "     |          `max_trials` randomly chosen sub-samples.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the estimated model.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.predict(X)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Returns the score of the prediction.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.score(X, y)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array or sparse matrix of shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |          Score of the prediction.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RandomizedLasso(BaseRandomizedLinearModel)\n",
      "     |  Randomized Lasso.\n",
      "     |  \n",
      "     |  Randomized Lasso works by subsampling the training data and\n",
      "     |  computing a Lasso estimate where the penalty of a random subset of\n",
      "     |  coefficients has been scaled. By performing this double\n",
      "     |  randomization several times, the method assigns high scores to\n",
      "     |  features that are repeatedly selected across randomizations. This\n",
      "     |  is known as stability selection. In short, features selected more\n",
      "     |  often are considered good features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <randomized_l1>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, 'aic', or 'bic', optional\n",
      "     |      The regularization parameter alpha parameter in the Lasso.\n",
      "     |      Warning: this is not the alpha parameter in the stability selection\n",
      "     |      article which is scaling.\n",
      "     |  \n",
      "     |  scaling : float, optional\n",
      "     |      The s parameter used to randomly scale the penalty of different\n",
      "     |      features (See :ref:`User Guide <randomized_l1>` for details ).\n",
      "     |      Should be between 0 and 1.\n",
      "     |  \n",
      "     |  sample_fraction : float, optional\n",
      "     |      The fraction of samples to be used in each randomized design.\n",
      "     |      Should be between 0 and 1. If 1, all samples are used.\n",
      "     |  \n",
      "     |  n_resampling : int, optional\n",
      "     |      Number of randomized models.\n",
      "     |  \n",
      "     |  selection_threshold: float, optional\n",
      "     |      The score above which features should be selected.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learned more robust and almost independent of\n",
      "     |      the number of samples. The same property is not valid for\n",
      "     |      standardized data. However, if you wish to standardize, please\n",
      "     |      use `preprocessing.StandardScaler` before calling `fit` on an\n",
      "     |      estimator with `normalize=False`.\n",
      "     |  \n",
      "     |  precompute : True | False | 'auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to 'auto' let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : integer, optional\n",
      "     |      Maximum number of iterations to perform in the Lars algorithm.\n",
      "     |  \n",
      "     |  eps : float, optional\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the 'tol' parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the resampling. If '-1', use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or string, optional\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A string, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  memory : Instance of joblib.Memory or string\n",
      "     |      Used for internal caching. By default, no caching is done.\n",
      "     |      If a string is given, it is the path to the caching directory.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array, shape = [n_features]\n",
      "     |      Feature scores between 0 and 1.\n",
      "     |  \n",
      "     |  all_scores_ : array, shape = [n_features, n_reg_parameter]\n",
      "     |      Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max of         ``all_scores_``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RandomizedLasso\n",
      "     |  >>> randomized_lasso = RandomizedLasso()\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See examples/linear_model/plot_sparse_recovery.py for an example.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Stability selection\n",
      "     |  Nicolai Meinshausen, Peter Buhlmann\n",
      "     |  Journal of the Royal Statistical Society: Series B\n",
      "     |  Volume 72, Issue 4, pages 417-473, September 2010\n",
      "     |  DOI: 10.1111/j.1467-9868.2010.00740.x\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RandomizedLogisticRegression, Lasso, ElasticNet\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedLasso\n",
      "     |      BaseRandomizedLinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha='aic', scaling=0.5, sample_fraction=0.75, n_resampling=200, selection_threshold=0.25, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, random_state=None, n_jobs=1, pre_dispatch='3*n_jobs', memory=Memory(cachedir=None))\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseRandomizedLinearModel:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Return a mask, or list, of the features/indices selected.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Transform a new matrix using the selected features\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform a new matrix using the selected features\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class RandomizedLogisticRegression(BaseRandomizedLinearModel)\n",
      "     |  Randomized Logistic Regression\n",
      "     |  \n",
      "     |  Randomized Logistic Regression works by subsampling the training\n",
      "     |  data and fitting a L1-penalized LogisticRegression model where the\n",
      "     |  penalty of a random subset of coefficients has been scaled. By\n",
      "     |  performing this double randomization several times, the method\n",
      "     |  assigns high scores to features that are repeatedly selected across\n",
      "     |  randomizations. This is known as stability selection. In short,\n",
      "     |  features selected more often are considered good features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <randomized_l1>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, optional, default=1\n",
      "     |      The regularization parameter C in the LogisticRegression.\n",
      "     |  \n",
      "     |  scaling : float, optional, default=0.5\n",
      "     |      The s parameter used to randomly scale the penalty of different\n",
      "     |      features (See :ref:`User Guide <randomized_l1>` for details ).\n",
      "     |      Should be between 0 and 1.\n",
      "     |  \n",
      "     |  sample_fraction : float, optional, default=0.75\n",
      "     |      The fraction of samples to be used in each randomized design.\n",
      "     |      Should be between 0 and 1. If 1, all samples are used.\n",
      "     |  \n",
      "     |  n_resampling : int, optional, default=200\n",
      "     |      Number of randomized models.\n",
      "     |  \n",
      "     |  selection_threshold : float, optional, default=0.25\n",
      "     |      The score above which features should be selected.\n",
      "     |  \n",
      "     |  fit_intercept : boolean, optional, default=True\n",
      "     |      whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  verbose : boolean or integer, optional\n",
      "     |      Sets the verbosity amount\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  tol : float, optional, default=1e-3\n",
      "     |       tolerance for stopping criteria of LogisticRegression\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      Number of CPUs to use during the resampling. If '-1', use\n",
      "     |      all the CPUs\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or string, optional\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A string, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  memory : Instance of joblib.Memory or string\n",
      "     |      Used for internal caching. By default, no caching is done.\n",
      "     |      If a string is given, it is the path to the caching directory.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array, shape = [n_features]\n",
      "     |      Feature scores between 0 and 1.\n",
      "     |  \n",
      "     |  all_scores_ : array, shape = [n_features, n_reg_parameter]\n",
      "     |      Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max         of ``all_scores_``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RandomizedLogisticRegression\n",
      "     |  >>> randomized_logistic = RandomizedLogisticRegression()\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  See examples/linear_model/plot_sparse_recovery.py for an example.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Stability selection\n",
      "     |  Nicolai Meinshausen, Peter Buhlmann\n",
      "     |  Journal of the Royal Statistical Society: Series B\n",
      "     |  Volume 72, Issue 4, pages 417-473, September 2010\n",
      "     |  DOI: 10.1111/j.1467-9868.2010.00740.x\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RandomizedLasso, LogisticRegression\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedLogisticRegression\n",
      "     |      BaseRandomizedLinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, C=1, scaling=0.5, sample_fraction=0.75, n_resampling=200, selection_threshold=0.25, tol=0.001, fit_intercept=True, verbose=False, normalize=True, random_state=None, n_jobs=1, pre_dispatch='3*n_jobs', memory=Memory(cachedir=None))\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseRandomizedLinearModel:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Return a mask, or list, of the features/indices selected.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Transform a new matrix using the selected features\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform a new matrix using the selected features\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class Ridge(_BaseRidge, sklearn.base.RegressorMixin)\n",
      "     |  Linear least squares with l2 regularization.\n",
      "     |  \n",
      "     |  This model solves a regression model where the loss function is\n",
      "     |  the linear least squares function and regularization is given by\n",
      "     |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      "     |  This estimator has built-in support for multi-variate regression\n",
      "     |  (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : {float, array-like}, shape (n_targets)\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as \n",
      "     |      LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      "     |      assumed to be specific to the targets. Hence they must correspond in\n",
      "     |      number.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "     |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than\n",
      "     |        'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      "     |        in old scipy versions. It also uses an iterative procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent. It also uses an\n",
      "     |        iterative procedure, and is often faster than other solvers when\n",
      "     |        both n_samples and n_features are large. Note that 'sag' fast\n",
      "     |        convergence is only guaranteed on features with approximately the\n",
      "     |        same scale. You can preprocess the data with a scaler from\n",
      "     |        sklearn.preprocessing.\n",
      "     |  \n",
      "     |      All last four solvers support both dense and sparse data. However,\n",
      "     |      only 'sag' supports sparse input when `fit_intercept` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |  \n",
      "     |  tol : float\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data. Used only in 'sag' solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *random_state* to support Stochastic Average Gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : array or None, shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge`\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = Ridge(alpha=1.0)\n",
      "     |  >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE\n",
      "     |  Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "     |        normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ridge\n",
      "     |      _BaseRidge\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or numpy array of shape [n_samples]\n",
      "     |          Individual weights for each sample\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RidgeCV(_BaseRidgeCV, sklearn.base.RegressorMixin)\n",
      "     |  Ridge regression with built-in cross-validation.\n",
      "     |  \n",
      "     |  By default, it performs Generalized Cross-Validation, which is a form of\n",
      "     |  efficient Leave-One-Out cross-validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : numpy array of shape [n_alphas]\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as \n",
      "     |      LogisticRegression or LinearSVC. \n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  scoring : string, callable or None, optional, default: None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`sklearn.model_selection.StratifiedKFold` is used, else, \n",
      "     |      :class:`sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n",
      "     |      Flag indicating which strategy to use when performing\n",
      "     |      Generalized Cross-Validation. Options are::\n",
      "     |  \n",
      "     |          'auto' : use svd if n_samples > n_features or when X is a sparse\n",
      "     |                   matrix, otherwise use eigen\n",
      "     |          'svd' : force computation via singular value decomposition of X\n",
      "     |                  (does not work for sparse matrices)\n",
      "     |          'eigen' : force computation via eigendecomposition of X^T X\n",
      "     |  \n",
      "     |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      "     |      option of the two depending upon the shape and format of the training\n",
      "     |      data.\n",
      "     |  \n",
      "     |  store_cv_values : boolean, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the `cv_values_` attribute (see\n",
      "     |      below). This flag is only compatible with `cv=None` (i.e. using\n",
      "     |      Generalized Cross-Validation).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional\n",
      "     |      Cross-validation values for each alpha (if `store_cv_values=True` and         `cv=None`). After `fit()` has been called, this attribute will         contain the mean squared errors (by default) or the values of the         `{loss,score}_func` function (if provided in the constructor).\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_features] or [n_targets, n_features]\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge: Ridge regression\n",
      "     |  RidgeClassifier: Ridge classifier\n",
      "     |  RidgeClassifierCV: Ridge classifier with built-in cross validation\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeCV\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape [n_samples]\n",
      "     |          Sample weight\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class RidgeClassifier(sklearn.linear_model.base.LinearClassifierMixin, _BaseRidge)\n",
      "     |  Classifier using Ridge regression.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as \n",
      "     |      LogisticRegression or LinearSVC.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', optional\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set to false, no\n",
      "     |      intercept will be used in calculations (e.g. data is expected to be\n",
      "     |      already centered).\n",
      "     |  \n",
      "     |  max_iter : int, optional\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      The default value is determined by scipy.sparse.linalg.\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'}\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. More stable for singular matrices than\n",
      "     |        'cholesky'.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      "     |        in old scipy versions. It also uses an iterative procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent. It also uses an\n",
      "     |        iterative procedure, and is faster than other solvers when both\n",
      "     |        n_samples and n_features are large.\n",
      "     |  \n",
      "     |        .. versionadded:: 0.17\n",
      "     |           Stochastic Average Gradient descent solver.\n",
      "     |  \n",
      "     |  tol : float\n",
      "     |      Precision of the solution.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data. Used in 'sag' solver.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,) or (n_classes, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : array or None, shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge, RidgeClassifierCV\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifier\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidge\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver='auto', random_state=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples,n_features]\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      sample_weight : float or numpy array of shape (n_samples,)\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to Classifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "    \n",
      "    class RidgeClassifierCV(sklearn.linear_model.base.LinearClassifierMixin, _BaseRidgeCV)\n",
      "     |  Ridge classifier with built-in cross-validation.\n",
      "     |  \n",
      "     |  By default, it performs Generalized Cross-Validation, which is a form of\n",
      "     |  efficient Leave-One-Out cross-validation. Currently, only the n_features >\n",
      "     |  n_samples case is handled efficiently.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : numpy array of shape [n_alphas]\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``C^-1`` in other linear models such as \n",
      "     |      LogisticRegression or LinearSVC. \n",
      "     |  \n",
      "     |  fit_intercept : boolean\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (e.g. data is expected to be already centered).\n",
      "     |  \n",
      "     |  normalize : boolean, optional, default False\n",
      "     |      If True, the regressors X will be normalized before regression.\n",
      "     |      This parameter is ignored when `fit_intercept` is set to False.\n",
      "     |      When the regressors are normalized, note that this makes the\n",
      "     |      hyperparameters learnt more robust and almost independent of the number\n",
      "     |      of samples. The same property is not valid for standardized data.\n",
      "     |      However, if you wish to standardize, please use\n",
      "     |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      "     |      with `normalize=False`.\n",
      "     |  \n",
      "     |  scoring : string, callable or None, optional, default: None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, optional\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - An object to be used as a cross-validation generator.\n",
      "     |      - An iterable yielding train/test splits.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', optional\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_values_ : array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional\n",
      "     |      Cross-validation values for each alpha (if `store_cv_values=True` and\n",
      "     |  `cv=None`). After `fit()` has been called, this attribute will contain     the mean squared errors (by default) or the values of the     `{loss,score}_func` function (if provided in the constructor).\n",
      "     |  \n",
      "     |  coef_ : array, shape = [n_features] or [n_targets, n_features]\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float | array, shape = (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge: Ridge regression\n",
      "     |  RidgeClassifier: Ridge classifier\n",
      "     |  RidgeCV: Ridge regression with built-in cross validation\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifierCV\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the ridge classifier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or numpy array of shape (n_samples,)\n",
      "     |          Sample weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "    \n",
      "    class SGDClassifier(BaseSGDClassifier, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "     |  Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
      "     |  \n",
      "     |  This estimator implements regularized linear models with stochastic\n",
      "     |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "     |  each sample at a time and the model is updated along the way with a\n",
      "     |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "     |  (online/out-of-core) learning, see the partial_fit method.\n",
      "     |  For best results using the default learning rate schedule, the data should\n",
      "     |  have zero mean and unit variance.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense or sparse arrays\n",
      "     |  of floating point values for the features. The model it fits can be\n",
      "     |  controlled with the loss parameter; by default, it fits a linear support\n",
      "     |  vector machine (SVM).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, 'hinge', 'log', 'modified_huber', 'squared_hinge',                'perceptron', or a regression loss: 'squared_loss', 'huber',                'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      "     |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      "     |      linear SVM.\n",
      "     |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      "     |      'modified_huber' is another smooth loss that brings tolerance to\n",
      "     |      outliers as well as probability estimates.\n",
      "     |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      "     |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "     |      The other losses are designed for regression but can be useful in\n",
      "     |      classification as well; see SGDRegressor for a description.\n",
      "     |  \n",
      "     |  penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the regularization term. Defaults to 0.0001\n",
      "     |      Also used to compute learning_rate when set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Defaults to 0.15.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs). The number\n",
      "     |      of iterations is set to 1 if using partial_fit.\n",
      "     |      Defaults to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, optional\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |      Defaults to True.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation. -1 means 'all CPUs'. Defaults\n",
      "     |      to 1.\n",
      "     |  \n",
      "     |  learning_rate : string, optional\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': eta = eta0\n",
      "     |      - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]\n",
      "     |      - 'invscaling': eta = eta0 / pow(t, power_t)\n",
      "     |  \n",
      "     |      where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |  \n",
      "     |  eta0 : double\n",
      "     |      The initial learning rate for the 'constant' or 'invscaling'\n",
      "     |      schedules. The default value is 0.0 as eta0 is not used by the\n",
      "     |      default schedule 'optimal'.\n",
      "     |  \n",
      "     |  power_t : double\n",
      "     |      The exponent for inverse scaling learning rate [default 0.5].\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None, optional\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So ``average=10`` will begin averaging after seeing 10\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> Y = np.array([1, 1, 2, 2])\n",
      "     |  >>> clf = linear_model.SGDClassifier()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "     |          eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "     |          learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "     |          penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "     |          verbose=0, warm_start=False)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  LinearSVC, LogisticRegression, Perceptron\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDClassifier\n",
      "     |      BaseSGDClassifier\n",
      "     |      abc.NewBase\n",
      "     |      BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.linear_model.base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  predict_log_proba\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "     |      and ones, so taking the logarithm is not possible.\n",
      "     |      \n",
      "     |      See ``predict_proba`` for details.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in\n",
      "     |          `self.classes_`.\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "     |      estimates by simple normalization, as recommended by Zadrozny and\n",
      "     |      Elkan.\n",
      "     |      \n",
      "     |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "     |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "     |      it is necessary to perform proper probability calibration by wrapping\n",
      "     |      the classifier with\n",
      "     |      :class:`sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in `self.classes_`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "     |      probability estimates\", SIGKDD'02,\n",
      "     |      http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      "     |      \n",
      "     |      The justification for the formula in the loss=\"modified_huber\"\n",
      "     |      case is in the appendix B in:\n",
      "     |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape (n_classes,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Subset of the target values\n",
      "     |      \n",
      "     |      classes : array, shape (n_classes,)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<type 'sklearn.linear_model....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is the signed distance of that\n",
      "     |      sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      "     |          Confidence scores per (sample, class) combination. In the binary\n",
      "     |          case, confidence score for self.classes_[1] where >0 means this\n",
      "     |          class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted class label per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class SGDRegressor(BaseSGDRegressor, sklearn.feature_selection.from_model._LearntSelectorMixin)\n",
      "     |  Linear model fitted by minimizing a regularized empirical loss with SGD\n",
      "     |  \n",
      "     |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      "     |  estimated each sample at a time and the model is updated along the way with\n",
      "     |  a decreasing strength schedule (aka learning rate).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays of\n",
      "     |  floating point values for the features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, 'squared_loss', 'huber', 'epsilon_insensitive',                 or 'squared_epsilon_insensitive'\n",
      "     |      The loss function to be used. Defaults to 'squared_loss' which refers\n",
      "     |      to the ordinary least squares fit. 'huber' modifies 'squared_loss' to\n",
      "     |      focus less on getting outliers correct by switching from squared to\n",
      "     |      linear loss past a distance of epsilon. 'epsilon_insensitive' ignores\n",
      "     |      errors less than epsilon and is linear past that; this is the loss\n",
      "     |      function used in SVR. 'squared_epsilon_insensitive' is the same but\n",
      "     |      becomes squared loss past a tolerance of epsilon.\n",
      "     |  \n",
      "     |  penalty : str, 'none', 'l2', 'l1', or 'elasticnet'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'.\n",
      "     |  \n",
      "     |  alpha : float\n",
      "     |      Constant that multiplies the regularization term. Defaults to 0.0001\n",
      "     |      Also used to compute learning_rate when set to 'optimal'.\n",
      "     |  \n",
      "     |  l1_ratio : float\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Defaults to 0.15.\n",
      "     |  \n",
      "     |  fit_intercept : bool\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      The number of passes over the training data (aka epochs). The number\n",
      "     |      of iterations is set to 1 if using partial_fit.\n",
      "     |      Defaults to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, optional\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |      Defaults to True.\n",
      "     |  \n",
      "     |  random_state : int seed, RandomState instance, or None (default)\n",
      "     |      The seed of the pseudo random number generator to use when\n",
      "     |      shuffling the data.\n",
      "     |  \n",
      "     |  verbose : integer, optional\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  epsilon : float\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |  \n",
      "     |  learning_rate : string, optional\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': eta = eta0\n",
      "     |      - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]\n",
      "     |      - 'invscaling': eta = eta0 / pow(t, power_t)\n",
      "     |  \n",
      "     |      where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |  \n",
      "     |  eta0 : double, optional\n",
      "     |      The initial learning rate [default 0.01].\n",
      "     |  \n",
      "     |  power_t : double, optional\n",
      "     |      The exponent for inverse scaling learning rate [default 0.25].\n",
      "     |  \n",
      "     |  warm_start : bool, optional\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |  \n",
      "     |  average : bool or int, optional\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So ``average=10`` will begin averaging after seeing 10\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape (1,)\n",
      "     |      The intercept term.\n",
      "     |  \n",
      "     |  average_coef_ : array, shape (n_features,)\n",
      "     |      Averaged weights assigned to the features.\n",
      "     |  \n",
      "     |  average_intercept_ : array, shape (1,)\n",
      "     |      The averaged intercept term.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> np.random.seed(0)\n",
      "     |  >>> y = np.random.randn(n_samples)\n",
      "     |  >>> X = np.random.randn(n_samples, n_features)\n",
      "     |  >>> clf = linear_model.SGDRegressor()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  ... #doctest: +NORMALIZE_WHITESPACE\n",
      "     |  SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "     |               fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "     |               loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
      "     |               random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  Ridge, ElasticNet, Lasso, SVR\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDRegressor\n",
      "     |      BaseSGDRegressor\n",
      "     |      BaseSGD\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.linear_model.base.SparseCoefMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_loss', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, n_iter=5, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              array, shape (n_samples,)\n",
      "     |                 Predicted target values per element in X.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data\n",
      "     |      \n",
      "     |      y : numpy array, shape (n_samples,)\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_features,)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape (1,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of training data\n",
      "     |      \n",
      "     |      y : numpy array of shape (n_samples,)\n",
      "     |          Subset of target values\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      array, shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<type 'sklearn.linear_model....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGD:\n",
      "     |  \n",
      "     |  set_params(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self: estimator\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "    \n",
      "    class SquaredLoss(Regression)\n",
      "     |  Squared loss traditional used in linear regression.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SquaredLoss\n",
      "     |      Regression\n",
      "     |      LossFunction\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __new__ = <built-in method __new__ of type object>\n",
      "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "     |  \n",
      "     |  __pyx_vtable__ = <capsule object NULL>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunction:\n",
      "     |  \n",
      "     |  dloss(...)\n",
      "     |      Evaluate the derivative of the loss function with respect to\n",
      "     |      the prediction `p`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : double\n",
      "     |          The prediction, p = w^T x\n",
      "     |      y : double\n",
      "     |          The true value (aka target)\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      double\n",
      "     |          The derivative of the loss function with regards to `p`.\n",
      "    \n",
      "    class TheilSenRegressor(sklearn.linear_model.base.LinearModel, sklearn.base.RegressorMixin)\n",
      "     |  Theil-Sen Estimator: robust multivariate regression model.\n",
      "     |  \n",
      "     |  The algorithm calculates least square solutions on subsets with size\n",
      "     |  n_subsamples of the samples in X. Any value of n_subsamples between the\n",
      "     |  number of features and samples leads to an estimator with a compromise\n",
      "     |  between robustness and efficiency. Since the number of least square\n",
      "     |  solutions is \"n_samples choose n_subsamples\", it can be extremely large\n",
      "     |  and can therefore be limited with max_subpopulation. If this limit is\n",
      "     |  reached, the subsets are chosen randomly. In a final step, the spatial\n",
      "     |  median (or L1 median) is calculated of all least square solutions.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : boolean, optional, default True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations.\n",
      "     |  \n",
      "     |  copy_X : boolean, optional, default True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_subpopulation : int, optional, default 1e4\n",
      "     |      Instead of computing with a set of cardinality 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples (at least\n",
      "     |      number of features), consider only a stochastic subpopulation of a\n",
      "     |      given maximal size if 'n choose k' is larger than max_subpopulation.\n",
      "     |      For other than small problem sizes this parameter will determine\n",
      "     |      memory usage and runtime if n_subsamples is not changed.\n",
      "     |  \n",
      "     |  n_subsamples : int, optional, default None\n",
      "     |      Number of samples to calculate the parameters. This is at least the\n",
      "     |      number of features (plus 1 if fit_intercept=True) and the number of\n",
      "     |      samples as a maximum. A lower number leads to a higher breakdown\n",
      "     |      point and a low efficiency while a high number leads to a low\n",
      "     |      breakdown point and a high efficiency. If None, take the\n",
      "     |      minimum number of subsamples leading to maximal robustness.\n",
      "     |      If n_subsamples is set to n_samples, Theil-Sen is identical to least\n",
      "     |      squares.\n",
      "     |  \n",
      "     |  max_iter : int, optional, default 300\n",
      "     |      Maximum number of iterations for the calculation of spatial median.\n",
      "     |  \n",
      "     |  tol : float, optional, default 1.e-3\n",
      "     |      Tolerance when calculating spatial median.\n",
      "     |  \n",
      "     |  random_state : RandomState or an int seed, optional, default None\n",
      "     |      A random number generator instance to define the state of the\n",
      "     |      random permutations generator.\n",
      "     |  \n",
      "     |  n_jobs : integer, optional, default 1\n",
      "     |      Number of CPUs to use during the cross validation. If ``-1``, use\n",
      "     |      all the CPUs.\n",
      "     |  \n",
      "     |  verbose : boolean, optional, default False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = (n_features)\n",
      "     |      Coefficients of the regression model (median of distribution).\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Estimated intercept of regression model.\n",
      "     |  \n",
      "     |  breakdown_ : float\n",
      "     |      Approximated breakdown point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations needed for the spatial median.\n",
      "     |  \n",
      "     |  n_subpopulation_ : int\n",
      "     |      Number of combinations taken into account from 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n",
      "     |    Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n",
      "     |    http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TheilSenRegressor\n",
      "     |      sklearn.linear_model.base.LinearModel\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=1, verbose=False)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training data\n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model.base.LinearModel:\n",
      "     |  \n",
      "     |  decision_function(*args, **kwargs)\n",
      "     |      DEPRECATED:  and will be removed in 0.19.\n",
      "     |      \n",
      "     |      Decision function of the linear model.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |                  Samples.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              C : array, shape = (n_samples,)\n",
      "     |                  Returns predicted values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    enet_path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "        Compute elastic net path with coordinate descent\n",
      "        \n",
      "        The elastic net optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "            + alpha * l1_ratio * ||w||_1\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "            + alpha * l1_ratio * ||W||_21\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <elastic_net>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like}, shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Target values\n",
      "        \n",
      "        l1_ratio : float, optional\n",
      "            float between 0 and 1 passed to elastic net (scaling between\n",
      "            l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso\n",
      "        \n",
      "        eps : float\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``\n",
      "        \n",
      "        n_alphas : int, optional\n",
      "            Number of alphas along the regularization path\n",
      "        \n",
      "        alphas : ndarray, optional\n",
      "            List of alphas where to compute the models.\n",
      "            If None alphas are set automatically\n",
      "        \n",
      "        precompute : True | False | 'auto' | array-like\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like, optional\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : boolean, optional, default True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : array, shape (n_features, ) | None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or integer\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        params : kwargs\n",
      "            keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        return_n_iter : bool\n",
      "            whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "        \n",
      "        check_input : bool, default True\n",
      "            Skip input validation checks, including the Gram matrix when provided\n",
      "            assuming there are handled by the caller when check_input=False.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array, shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : array, shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : array-like, shape (n_alphas,)\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "            (Is returned when ``return_n_iter`` is set to True).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        MultiTaskElasticNet\n",
      "        MultiTaskElasticNetCV\n",
      "        ElasticNet\n",
      "        ElasticNetCV\n",
      "    \n",
      "    lars_path(X, y, Xy=None, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=2.2204460492503131e-16, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lars', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1])\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        -----------\n",
      "        X : array, shape: (n_samples, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        y : array, shape: (n_samples)\n",
      "            Input targets.\n",
      "        \n",
      "        positive : boolean (default=False)\n",
      "            Restrict coefficients to be >= 0.\n",
      "            When using this option together with method 'lasso' the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha (neither will they when using method 'lar'\n",
      "            ..). Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "            0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "            algorithm are typically in congruence with the solution of the\n",
      "            coordinate descent lasso_path function.\n",
      "        \n",
      "        max_iter : integer, optional (default=500)\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        Gram : None, 'auto', array, shape: (n_features, n_features), optional\n",
      "            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram\n",
      "            matrix is precomputed from the given X, if there are more samples\n",
      "            than features.\n",
      "        \n",
      "        alpha_min : float, optional (default=0)\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, optional (default='lar')\n",
      "            Specifies the returned model. Select ``'lar'`` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        eps : float, optional (default=``np.finfo(np.float).eps``)\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems.\n",
      "        \n",
      "        copy_X : bool, optional (default=True)\n",
      "            If ``False``, ``X`` is overwritten.\n",
      "        \n",
      "        copy_Gram : bool, optional (default=True)\n",
      "            If ``False``, ``Gram`` is overwritten.\n",
      "        \n",
      "        verbose : int (default=0)\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, optional (default=True)\n",
      "            If ``return_path==True`` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, optional (default=False)\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        --------\n",
      "        alphas : array, shape: [n_alphas + 1]\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "            number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : array, shape [n_alphas]\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : array, shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if return_n_iter is set\n",
      "            to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lasso_path\n",
      "        LassoLars\n",
      "        Lars\n",
      "        LassoLarsCV\n",
      "        LarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Effron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "    \n",
      "    lasso_path(X, y, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "        Compute Lasso path with coordinate descent\n",
      "        \n",
      "        The Lasso optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <lasso>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : ndarray, shape (n_samples,), or (n_samples, n_outputs)\n",
      "            Target values\n",
      "        \n",
      "        eps : float, optional\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``\n",
      "        \n",
      "        n_alphas : int, optional\n",
      "            Number of alphas along the regularization path\n",
      "        \n",
      "        alphas : ndarray, optional\n",
      "            List of alphas where to compute the models.\n",
      "            If ``None`` alphas are set automatically\n",
      "        \n",
      "        precompute : True | False | 'auto' | array-like\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like, optional\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : boolean, optional, default True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : array, shape (n_features, ) | None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or integer\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        params : kwargs\n",
      "            keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        positive : bool, default False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "        \n",
      "        return_n_iter : bool\n",
      "            whether to return the number of iterations or not.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : array, shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : array, shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : array-like, shape (n_alphas,)\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      "        for an example.\n",
      "        \n",
      "        To avoid unnecessary memory duplication the X argument of the fit method\n",
      "        should be directly passed as a Fortran-contiguous numpy array.\n",
      "        \n",
      "        Note that in certain cases, the Lars solver may be significantly\n",
      "        faster to implement this functionality. In particular, linear\n",
      "        interpolation can be used to retrieve model coefficients between the\n",
      "        values output by lars_path\n",
      "        \n",
      "        Examples\n",
      "        ---------\n",
      "        \n",
      "        Comparing lasso_path and lars_path with interpolation:\n",
      "        \n",
      "        >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "        >>> y = np.array([1, 2, 3.1])\n",
      "        >>> # Use lasso_path to compute a coefficient path\n",
      "        >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "        >>> print(coef_path)\n",
      "        [[ 0.          0.          0.46874778]\n",
      "         [ 0.2159048   0.4425765   0.23689075]]\n",
      "        \n",
      "        >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "        >>> # same path\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "        >>> from scipy import interpolate\n",
      "        >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "        ...                                             coef_path_lars[:, ::-1])\n",
      "        >>> print(coef_path_continuous([5., 1., .5]))\n",
      "        [[ 0.          0.          0.46915237]\n",
      "         [ 0.2159048   0.4425765   0.23668876]]\n",
      "        \n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        lars_path\n",
      "        Lasso\n",
      "        LassoLars\n",
      "        LassoCV\n",
      "        LassoLarsCV\n",
      "        sklearn.decomposition.sparse_encode\n",
      "    \n",
      "    lasso_stability_path(X, y, scaling=0.5, random_state=None, n_resampling=200, n_grid=100, sample_fraction=0.75, eps=8.8817841970012523e-16, n_jobs=1, verbose=False)\n",
      "        Stability path based on randomized Lasso estimates\n",
      "        \n",
      "        Read more in the :ref:`User Guide <randomized_l1>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape = [n_samples, n_features]\n",
      "            training data.\n",
      "        \n",
      "        y : array-like, shape = [n_samples]\n",
      "            target values.\n",
      "        \n",
      "        scaling : float, optional, default=0.5\n",
      "            The alpha parameter in the stability selection article used to\n",
      "            randomly scale the features. Should be between 0 and 1.\n",
      "        \n",
      "        random_state : integer or numpy.random.RandomState, optional\n",
      "            The generator used to randomize the design.\n",
      "        \n",
      "        n_resampling : int, optional, default=200\n",
      "            Number of randomized models.\n",
      "        \n",
      "        n_grid : int, optional, default=100\n",
      "            Number of grid points. The path is linearly reinterpolated\n",
      "            on a grid between 0 and 1 before computing the scores.\n",
      "        \n",
      "        sample_fraction : float, optional, default=0.75\n",
      "            The fraction of samples to be used in each randomized design.\n",
      "            Should be between 0 and 1. If 1, all samples are used.\n",
      "        \n",
      "        eps : float, optional\n",
      "            Smallest value of alpha / alpha_max considered\n",
      "        \n",
      "        n_jobs : integer, optional\n",
      "            Number of CPUs to use during the resampling. If '-1', use\n",
      "            all the CPUs\n",
      "        \n",
      "        verbose : boolean or integer, optional\n",
      "            Sets the verbosity amount\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas_grid : array, shape ~ [n_grid]\n",
      "            The grid points between 0 and 1: alpha/alpha_max\n",
      "        \n",
      "        scores_path : array, shape = [n_features, n_grid]\n",
      "            The scores for each feature along the path.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See examples/linear_model/plot_sparse_recovery.py for an example.\n",
      "    \n",
      "    logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True, max_iter=100, tol=0.0001, verbose=0, solver='lbfgs', coef=None, copy=False, class_weight=None, dual=False, penalty='l2', intercept_scaling=1.0, multi_class='ovr', random_state=None, check_input=True, max_squared_sum=None, sample_weight=None)\n",
      "        Compute a Logistic Regression model for a list of regularization\n",
      "        parameters.\n",
      "        \n",
      "        This is an implementation that uses the result of the previous model\n",
      "        to speed up computations along the set of solutions, making it faster\n",
      "        than sequentially calling LogisticRegression for the different parameters.\n",
      "        Note that there will be no speedup with liblinear solver, since it does\n",
      "        not handle warm-starting.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "            Input data.\n",
      "        \n",
      "        y : array-like, shape (n_samples,)\n",
      "            Input data, target values.\n",
      "        \n",
      "        Cs : int | array-like, shape (n_cs,)\n",
      "            List of values for the regularization parameter or integer specifying\n",
      "            the number of regularization parameters that should be used. In this\n",
      "            case, the parameters will be chosen in a logarithmic scale between\n",
      "            1e-4 and 1e4.\n",
      "        \n",
      "        pos_class : int, None\n",
      "            The class with respect to which we perform a one-vs-all fit.\n",
      "            If None, then it is assumed that the given problem is binary.\n",
      "        \n",
      "        fit_intercept : bool\n",
      "            Whether to fit an intercept for the model. In this case the shape of\n",
      "            the returned array is (n_cs, n_features + 1).\n",
      "        \n",
      "        max_iter : int\n",
      "            Maximum number of iterations for the solver.\n",
      "        \n",
      "        tol : float\n",
      "            Stopping criterion. For the newton-cg and lbfgs solvers, the iteration\n",
      "            will stop when ``max{|g_i | i = 1, ..., n} <= tol``\n",
      "            where ``g_i`` is the i-th component of the gradient.\n",
      "        \n",
      "        verbose : int\n",
      "            For the liblinear and lbfgs solvers set verbose to any positive\n",
      "            number for verbosity.\n",
      "        \n",
      "        solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag'}\n",
      "            Numerical solver to use.\n",
      "        \n",
      "        coef : array-like, shape (n_features,), default None\n",
      "            Initialization value for coefficients of logistic regression.\n",
      "            Useless for liblinear solver.\n",
      "        \n",
      "        copy : bool, default False\n",
      "            Whether or not to produce a copy of the data. A copy is not required\n",
      "            anymore. This parameter is deprecated and will be removed in 0.19.\n",
      "        \n",
      "        class_weight : dict or 'balanced', optional\n",
      "            Weights associated with classes in the form ``{class_label: weight}``.\n",
      "            If not given, all classes are supposed to have weight one.\n",
      "        \n",
      "            The \"balanced\" mode uses the values of y to automatically adjust\n",
      "            weights inversely proportional to class frequencies in the input data\n",
      "            as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "        \n",
      "            Note that these weights will be multiplied with sample_weight (passed\n",
      "            through the fit method) if sample_weight is specified.\n",
      "        \n",
      "        dual : bool\n",
      "            Dual or primal formulation. Dual formulation is only implemented for\n",
      "            l2 penalty with liblinear solver. Prefer dual=False when\n",
      "            n_samples > n_features.\n",
      "        \n",
      "        penalty : str, 'l1' or 'l2'\n",
      "            Used to specify the norm used in the penalization. The 'newton-cg',\n",
      "            'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      "        \n",
      "        intercept_scaling : float, default 1.\n",
      "            Useful only when the solver 'liblinear' is used\n",
      "            and self.fit_intercept is set to True. In this case, x becomes\n",
      "            [x, self.intercept_scaling],\n",
      "            i.e. a \"synthetic\" feature with constant value equal to\n",
      "            intercept_scaling is appended to the instance vector.\n",
      "            The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "        \n",
      "            Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "            as all other features.\n",
      "            To lessen the effect of regularization on synthetic feature weight\n",
      "            (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "        \n",
      "        multi_class : str, {'ovr', 'multinomial'}\n",
      "            Multiclass option can be either 'ovr' or 'multinomial'. If the option\n",
      "            chosen is 'ovr', then a binary problem is fit for each label. Else\n",
      "            the loss minimised is the multinomial loss fit across\n",
      "            the entire probability distribution. Works only for the 'lbfgs' and\n",
      "            'newton-cg' solvers.\n",
      "        \n",
      "        random_state : int seed, RandomState instance, or None (default)\n",
      "            The seed of the pseudo random number generator to use when\n",
      "            shuffling the data. Used only in solvers 'sag' and 'liblinear'.\n",
      "        \n",
      "        check_input : bool, default True\n",
      "            If False, the input arrays X and y will not be checked.\n",
      "        \n",
      "        max_squared_sum : float, default None\n",
      "            Maximum squared sum of X over samples. Used only in SAG solver.\n",
      "            If None, it will be computed, going through all the samples.\n",
      "            The value should be precomputed to speed up cross validation.\n",
      "        \n",
      "        sample_weight : array-like, shape(n_samples,) optional\n",
      "            Array of weights that are assigned to individual samples.\n",
      "            If not provided, then each sample is given unit weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)\n",
      "            List of coefficients for the Logistic Regression model. If\n",
      "            fit_intercept is set to True then the second dimension will be\n",
      "            n_features + 1, where the last item represents the intercept.\n",
      "        \n",
      "        Cs : ndarray\n",
      "            Grid of Cs used for cross-validation.\n",
      "        \n",
      "        n_iter : array, shape (n_cs,)\n",
      "            Actual number of iteration for each Cs.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        You might get slightly different results with the solver liblinear than\n",
      "        with the others since this uses LIBLINEAR which penalizes the intercept.\n",
      "    \n",
      "    orthogonal_mp(X, y, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)\n",
      "        Orthogonal Matching Pursuit (OMP)\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        An instance of the problem has the form:\n",
      "        \n",
      "        When parametrized by the number of non-zero coefficients using\n",
      "        `n_nonzero_coefs`:\n",
      "        argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n",
      "        \n",
      "        When parametrized by error using the parameter `tol`:\n",
      "        argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array, shape (n_samples, n_features)\n",
      "            Input data. Columns are assumed to have unit norm.\n",
      "        \n",
      "        y : array, shape (n_samples,) or (n_samples, n_targets)\n",
      "            Input targets\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        precompute : {True, False, 'auto'},\n",
      "            Whether to perform precomputations. Improves performance when n_targets\n",
      "            or n_samples is very large.\n",
      "        \n",
      "        copy_X : bool, optional\n",
      "            Whether the design matrix X must be copied by the algorithm. A false\n",
      "            value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        return_path : bool, optional. Default: False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, optional default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp_gram\n",
      "        lars_path\n",
      "        decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)\n",
      "        Gram Orthogonal Matching Pursuit (OMP)\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems using only\n",
      "        the Gram matrix X.T * X and the product X.T * y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Gram : array, shape (n_features, n_features)\n",
      "            Gram matrix of the input data: X.T * X\n",
      "        \n",
      "        Xy : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Input targets multiplied by X: X.T * y\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float\n",
      "            Maximum norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        norms_squared : array-like, shape (n_targets,)\n",
      "            Squared L2 norms of the lines of y. Required if tol is not None.\n",
      "        \n",
      "        copy_Gram : bool, optional\n",
      "            Whether the gram matrix must be copied by the algorithm. A false\n",
      "            value is only helpful if it is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        copy_Xy : bool, optional\n",
      "            Whether the covariance vector Xy must be copied by the algorithm.\n",
      "            If False, it may be overwritten.\n",
      "        \n",
      "        return_path : bool, optional. Default: False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, optional default False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit\n",
      "        orthogonal_mp\n",
      "        lars_path\n",
      "        decomposition.sparse_encode\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "    \n",
      "    ridge_regression(X, y, alpha, sample_weight=None, solver='auto', max_iter=None, tol=0.001, verbose=0, random_state=None, return_n_iter=False, return_intercept=False)\n",
      "        Solve the ridge equation by the method of normal equations.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix, LinearOperator},\n",
      "            shape = [n_samples, n_features]\n",
      "            Training data\n",
      "        \n",
      "        y : array-like, shape = [n_samples] or [n_samples, n_targets]\n",
      "            Target values\n",
      "        \n",
      "        alpha : {float, array-like},\n",
      "            shape = [n_targets] if array-like\n",
      "            Regularization strength; must be a positive float. Regularization\n",
      "            improves the conditioning of the problem and reduces the variance of\n",
      "            the estimates. Larger values specify stronger regularization.\n",
      "            Alpha corresponds to ``C^-1`` in other linear models such as \n",
      "            LogisticRegression or LinearSVC. If an array is passed, penalties are\n",
      "            assumed to be specific to the targets. Hence they must correspond in\n",
      "            number.\n",
      "        \n",
      "        max_iter : int, optional\n",
      "            Maximum number of iterations for conjugate gradient solver.\n",
      "            For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "            by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "        \n",
      "        sample_weight : float or numpy array of shape [n_samples]\n",
      "            Individual weights for each sample. If sample_weight is not None and\n",
      "            solver='auto', the solver will be set to 'cholesky'.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'}\n",
      "            Solver to use in the computational routines:\n",
      "        \n",
      "            - 'auto' chooses the solver automatically based on the type of data.\n",
      "        \n",
      "            - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "              coefficients. More stable for singular matrices than\n",
      "              'cholesky'.\n",
      "        \n",
      "            - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "              obtain a closed-form solution via a Cholesky decomposition of\n",
      "              dot(X.T, X)\n",
      "        \n",
      "            - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "              scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "              more appropriate than 'cholesky' for large-scale data\n",
      "              (possibility to set `tol` and `max_iter`).\n",
      "        \n",
      "            - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "              scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n",
      "              in old scipy versions. It also uses an iterative procedure.\n",
      "        \n",
      "            - 'sag' uses a Stochastic Average Gradient descent. It also uses an\n",
      "              iterative procedure, and is often faster than other solvers when\n",
      "              both n_samples and n_features are large. Note that 'sag' fast\n",
      "              convergence is only guaranteed on features with approximately the\n",
      "              same scale. You can preprocess the data with a scaler from\n",
      "              sklearn.preprocessing.\n",
      "        \n",
      "            All last four solvers support both dense and sparse data. However,\n",
      "            only 'sag' supports sparse input when `fit_intercept` is True.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Stochastic Average Gradient descent solver.\n",
      "        \n",
      "        tol : float\n",
      "            Precision of the solution.\n",
      "        \n",
      "        verbose : int\n",
      "            Verbosity level. Setting verbose > 0 will display additional\n",
      "            information depending on the solver used.\n",
      "        \n",
      "        random_state : int seed, RandomState instance, or None (default)\n",
      "            The seed of the pseudo random number generator to use when\n",
      "            shuffling the data. Used only in 'sag' solver.\n",
      "        \n",
      "        return_n_iter : boolean, default False\n",
      "            If True, the method also returns `n_iter`, the actual number of\n",
      "            iteration performed by the solver.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        return_intercept : boolean, default False\n",
      "            If True and if X is sparse, the method also returns the intercept,\n",
      "            and the solver is automatically changed to 'sag'. This is only a\n",
      "            temporary fix for fitting the intercept with sparse data. For dense\n",
      "            data, use sklearn.linear_model._preprocess_data before your regression.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : array, shape = [n_features] or [n_targets, n_features]\n",
      "            Weight vector(s).\n",
      "        \n",
      "        n_iter : int, optional\n",
      "            The actual number of iteration performed by the solver.\n",
      "            Only returned if `return_n_iter` is True.\n",
      "        \n",
      "        intercept : float or array, shape = [n_targets]\n",
      "            The intercept of the model. Only returned if `return_intercept`\n",
      "            is True and if X is a scipy sparse array.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function won't compute the intercept.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNet...\n",
      "    __warningregistry__ = {('numpy.dtype size changed, may indicate binary...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-26bf3783970e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myarray_withoutnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 512\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "lm.fit(x,yarray_withoutnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x[~numpy.isnan(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-26bf3783970e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myarray_withoutnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 512\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "lm.fit(x,yarray_withoutnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x[numpy.isfinite(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xwithoutnan = x[~numpy.isnan(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yarray = yarray[numpy.isfinite(yarray)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ywithoutnan = yarray[~numpy.isnan(yarray)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 929]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-d66f9570b697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxwithoutnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mywithoutnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 512\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 929]"
     ]
    }
   ],
   "source": [
    "lm.fit(xwithoutnan,ywithoutnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(xwithoutnan.reshape(len(xwithoutnan),1),ywithoutnan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slope,intercept = np.polyfit(xwithoutnan,ywithoutnan,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10a4be510>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEKZIIIvCIJQJojgC4IglAn/H5Hj8FSHspKVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a4fc1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xwithoutnan,ywithoutnan,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x108b5fb90>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAELCAYAAADHksFtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a4da110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Arrival_count')\n",
    "plt.ylabel('Mean_Arrival_delay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAELCAYAAAAoUKpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXmcXXWV4L/nvaoKCVSSIntSWQhLhBSCSQyJ2oIgttgo\nCNgIbnSLaA/d6jg9ijrSke5xsKedwWmZQRrbbUJAdkRCy5JEmSZbhSULBEJIJZWVJJWkICFV9d6Z\nP+5S9966b6t6a9X5fj71eXXvu+/e8+5775zf72w/UVUMwzAMIxuJSgtgGIZhVD9mLAzDMIycmLEw\nDMMwcmLGwjAMw8iJGQvDMAwjJ2YsDMMwjJyYsTAMwzByYsbCMAzDyIkZC8MwDCMndZUWoFiMHTtW\nZ8yYUWkxDMMwaorW1tb9qjou13GDxljMmDGDtWvXVloMwzCMmkJE2vI5ztxQhmEYRk7MWBiGYRg5\nMWNhGIZh5MSMhWEYhpETMxaGYRhGTsxYGIZhGDkxY1EFtLZ1cPuyLbS2dVRaFMMwjFgGTZ1FrdLa\n1sFn7lpJV0+ahroEi69fwNzpTZUWyzAMI4TNLCrMyq0H6OpJk1bo7kmzcuuBSotkGIbRBzMWFWbB\nzDE01CVICtTXJVgwc0ylRTIMw+iDuaEqzNzpTdx86WyWbtjNJS2TzAVlGEZVYsaiwrS2dXDLYxvp\n6kmzZttBZk1sNINhGEbVYW6oCmMxC8MwagEzFhXGYhaGYdQC5oaqMHOnN7H4+gWs3HqABTPHmAvK\nMIyqxIxFFTB3epMZCcMwqhpzQxmGYRg5MWNhGIZh5MSMhWEYhpETMxaGYRhGTsxYGIZhGDkxY2EY\nhmHkxIyFYRiGkRMzFoYtvmQYRk6sKG+IY4svGYaRDzazGOJYI0PDMPLBjMUQxxoZGoaRD+aGGuJY\nI0PDMPLBjIVhjQwNw8iJuaEMwzCMnFTcWIjIaBG5X0ReEZGXRWShiJwsIk+KyGvuow17DcMwKkjF\njQXwY+AJVX0XcA7wMnAT8LSqng487W4bhmEYFaKixkJERgEfBH4GoKpdqnoIuAz4pXvYL4HLKyOh\nYRiGAZWfWZwCvAn8XESeF5G7ROREYIKq7naP2QNMqJiEhmEYRsWNRR0wB/g/qvoe4G0iLidVVUDj\nXiwiN4jIWhFZ++abb5ZcWMMwjKFKpY1FO9Cuqqvc7ftxjMdeEZkE4D7ui3uxqt6pqvNUdd64cePK\nIrBhGMZQpKLGQlX3ADtEZJa76yJgE/Ao8AV33xeARyognmEYhuFSDUV5fwMsFpEGYCvwFzhG7Dci\n8kWgDfjzCspnGIYx5Km4sVDVF4B5MU9dVG5ZDMMwjHgqHbMwDMMwagAzFoZhGEZOzFgYhmEYOTFj\nYRiGYeTEjIVhGIaREzMWhmEYRk7MWBiGYRg5MWNhGIZh5MSMhWEYhpETMxaGYRhGTsxYGIZhGDkx\nY2EYhmHkxIxFDdPa1sHty7bQ2tZRaVEMwxjkVLzrrNE/Wts6+MxdK+nqSdNQl2Dx9QuYO72p0mIZ\nhjFIsZlFjbJy6wG6etKkFbp70qzceqCk17NZjGEMbWxmUaMsmDmGhroE3T1p6usSLJg5pmTXslmM\nYRhmLGqUudObWHz9AlZuPcCCmWNKqrzjZjFmLAxjaGHGooaZO72pLEq7nLMYwzCqEzMWRk7KOYsx\nDKM6MWNh5EW5ZjGGYVQnlg1lGIZh5CRvYyEi5qg2DMMYohQys1gpIveJyMdEREomkWEYhlF1FGIs\nzgDuBD4HvCYiPxCRM0ojlmEYhlFN5G0s1OFJVb0G+BLwBWC1iKwQkYUlk9AwDMOoOHlnQ7kxi8/i\nzCz2An8DPAqcC9wHnFIKAQ3DMIzKU0jq7HPAr4HLVbU9sH+tiNxRXLEMwzCMaqIQYzFLVTXuCVX9\n4UCEEJEksBbYqaqXisgpwD3AGKAV+Jyqdg3kGqWkta1jQAVrA329YRhGqSnEWIwVkW8Cs4ETvJ2q\nemER5Pga8DIw0t3+IfA/VfUed9byReD/FOE6RWegTfasSZ9hGLVAIdlQi4FXcGIT3we2AWsGKoCI\nNAN/BtzlbgtwIXC/e8gvgcsHep1SMdBW4eVuNW4YhtEfCjEWY1T1Z0C3qq5Q1b/EUeoD5Tbgm0Da\nuw5wSFV73O12YEoRrlMSvCZ7SaFfTfYG+nrDMIxyUIgbqtt93C0ifwbsAk4eyMVF5FJgn6q2isgF\n/Xj9DcANANOmTRuIKP1moE32rEmfYRi1gGSIWfc90FHsfwSmAv+ME1/4vqo+2u+Li/w3nFTcHpw4\nyEjgIeBPgYmq2uPWcCxS1T/Ndq558+bp2rVr+yuKYRjGkEREWlV1Xq7jCinKe0xVD6vqBlX9kKrO\nHYihcM/5bVVtVtUZwKeBZ1T1M8Ay4Cr3sC8AjwzkOoZhGMbAyOmGEpF/BjJOP1T1q0WVyOFbwD0i\n8g/A88DPSnANwzAMI0/yiVmUxbejqsuB5e7/W4H55bjuUMFqOQzDGAg5jYWq/jK4LSIjVPVo6UQy\nio3VchiGMVAKWc9ioYhswqm1QETOEZH/XTLJjKJhtRyGYQyUQuosbsPJUjoAoKovAh8shVBGcbFa\nDsMwBkpBa3Cr6o7Iukep4opjlAKr5aguLH5k1CKFGIsdIvI+QEWknt5+TkYNMHd6kymmKsDiR0at\nUogb6ivAjTitN3birGNxYymEMozBisWPjFol75mFqu4HPlNCWQxj0OPFj7p70hY/MmqKai3KM0pM\n0G8OmA+9TFj8yKhVCinKez9wFnCvu/0pYFMphDJKS9BvXpcQEKEnZT70cmHxI6MWybsoT0T+CviA\n1zrcXZToj6UVzygFIb95SgFF6fWhmyIzDCNKIQHuJnpXsgM4yd1n1BihuoukUG81GIZh5KCQ1Nlb\ngedFZBkgOAV5i0ohlFFaon5zsJiFYRjZKSQb6ucishQ4z931LVXd4z0vIrNVdWOxBTSKSzCwfeOH\nTvP3ZzIStVRAVkuyGkatUWgF9x4yry3xa2DOgCUagpRLyRVaEFZLBWS1JKth1CKFxCxyIbkPMaJ4\nSu5Hv9/MZ+5aSWtbR8muVWhBWC0VkNWSrIZRixTTWOS3PqsRopxKrtCGgrXUgLCWZDWMWqQgN5RR\nfMpZ0VtoQVgtFZDVkqyGUYuIanEmBCKyUlUXFOVk/WDevHm6dm1ZFvUrOuWIWdy9ajtLN+zmkpZJ\nXHvetJJcwwLMhlF7iEirqs7LdVw+7T6yBq1VdZ37WDFDUesUu6I3qrTvXrWd7zy0HoA/vrYfoOgG\nwwLMhjG4yccN9aMszylwYZFkMYpAnNJeumF36JilG3YXxVgEjVJc7MWMhWEMHvJp9/GhcghiFE6c\n2ydOaV/SMsmfUQBc0jKpKNcOGqWbL51t3VQNYxBTUIBbRFpwmgme4O1T1V8VWygjN5ncPpkC5nUJ\n6Ek7j7MmNmY8Z74xh6hR6jjaZQFmwxjE5G0sROTvgAtwjMXjwCXAs4AZiwqQye0TlxV0+7ItpN08\nBlViXUSFxhzijNJg6KZqQXrDiKeQmcVVwDnA86r6FyIyAfi/pRHLyEW2lNuo0s4nPbfQmMNgTFW1\nIL1hZKYQY3FMVdMi0iMiI4F9wNQSyWXkoBBlnc+x/an3KMdMopwjfQvSG0ZmCjEWa0VkNPAvQCvw\nFvBcSaQy8qIQZZ3r2GqcKZR7pG9LnhpGZgrpOvsf3H/vEJEngJGq+lJpxDIqQbXFHDKN9Es126hG\ng2kY1UIhAe5HgXuAR1R1W8kkMgyXuJF+qWcb1WYwDaNaKKSR4I+ADwCbROR+EblKRE7I9aJsiMhU\nEVkmIptEZKOIfM3df7KIPCkir7mP9uutMK1tHdy+bEtJu+JG8Ub63/jILN8oWHdZw6gMhbihVgAr\nRCSJU7X9JeBfCS+1Wig9wH9S1XUi0gi0isiTwHXA06p6q4jcBNwEfGsA1xn0lDIQXMksof5kdhmG\nUXwKLcobDnwcuBpnoaNfDuTiqrob2O3+3ykiLwNTgMtwajpwr7EcMxYZKbUyr6YsIYsrGEZlKCRm\n8RtgPvAE8BNghaqmiyWIiMwA3gOsAia4hgRgDzChWNcZjJRamRcymi9HqutQiitYkaBRLRQys/gZ\ncI2qpoothIicBDwAfF1Vj4j0LrqnqioisX3UReQG4AaAadNK03a7Fii1Ms93NG9FbcXF7qdRTeTT\novxCVX0GOBG4LKjIAVT1wYEIICL1OIZiceBce0VkkqruFpFJOAWAfVDVO4E7wVnPYiBy1DLlUOb5\njOaryV01GLD7aVQT+cwszgeewYlVRFGg38ZCHMvzM+BlVf0fgaceBb4A3Oo+PtLfawwmss0KqkGZ\nW/C5uNj9NKqJfFqU/52IJIClqvqbIl///cDngPUi8oK77zs4RuI3IvJFoA348yJf16dWfMLFcEn0\nV/nke4+8Gc6D69oH5YLs5f6ulDKYXyvfe6N6yCtm4faE+iZQVGOhqs8CkuHpi4p5rThqySdcjFlB\nf5RPf+7RA+va6epJ8+C69n7d02pQZFEZKvVdKUUwv5a+90b1UEiA+ykR+VvgXuBtb6eqHiy6VGWi\n2n3CQYVVLJdEocqn0Hs00HtaDYosKENdMsFVc5sBqvq7UgjV/r03qpNCjMXV7uONgX0KzCyeOOWl\nmn3CcUozOisIGhOgJKPxQu9R04gGEiKgmvc9rbblWYMydPWkuXvVdpIJISHONLjaviuFEv1Mm0Y0\ncPuyLeaSMrKSl7FwYxafVdX/V2J5yko1F3jFKc0bP3SaL2No9JsQEKEnVfzReCH3qLWtg1se20ha\nlURCuPnS2TnlqMblWT1l+k53bxlRKq0kE8LV86dy5ZzmqvquFErwM20a0cAtj200l5SRk0JiFj/B\nKZobVFRrgVeuEX3ImKQUUJTijMaj/vp871FQJkHpONpV0GuqZXnWudObuPnS2XzvkQ2k0r2helVl\nyujhJZOpnLEa7zO9fdmWis/kjNqgEDfU0yJyJfCgqg7GZJeqwlNYSzfs5pKWSX1+wEFjknRnFqnU\nwEfjA4kZ9Mett2DmGOoSQnfKGblXy/KsHUe7CH7NBWgo4UynUrGaanbFGtVFIcbiy8A3gB4ReQfn\n96OqOpBGgkYGPJdOV0+aNdsOMmtiY0h5RN1D0Ddm0Z+R6kBiBv1264kA6j4Wh4GO0kPG2A1yl9L9\nVKlYTTW7Yo3qopCus42lFMQIk4/yiI7Ag/8XMlItZtZVf7KtunvSKNBTJCVZjFF6uZVoJUf41TCT\nM6qfgrrOeojIqcA1OL2iZhdXJAMGrjzyHalmy7pqGtHgrxdRKmXSNKLBL+BLA53Hugd8zmKN0sup\nRAtp2WKzAKMSFNJ1djLwaRwjcTbw39xtowQMdGSbr7HJlHUFlMWH3nG0C9cJBcBdz77BxbMnDuha\n+b73alO8uYxTqeMa1XY/jOoin0aCN+AYiCk4FdxfxFla9fsllm3IM5CRbSZjE1UImRRruXzoTSMa\ncMsyAEindcDXysfQVkPxX6E8uK6d493pomW9BanF+2GUl3xmFj8BngOuVdW1AJlahhuVI25UGDU2\ncTUNHUe7/Mfga8vhQ/eC+J6hEKChvvdaAxnp5jK0pTCGpV6t8N412/0ZWDLZuyZ5pmsWIk8x7ofN\nTAY3+RiLScCngB+JyESc2UV9SaUqI+X8gpfqWvmOCqOVyTc/soG0auxrShXgjavWViAh8P7TxvL1\nD59Rll5MxTaGhcob13sq7l57+5dv3kdPYKmx888YB8S7ClvbOnhwXTv3rd1BTzr+8y32/Wht6+Ca\nO5+jO6XUJ4UlNywcUgZjKBjKfLrOHgDuAO4QkWacth973SVQH1LV75RYxpJRzql3Ka7lfUF3HTqW\n16gwqBBEhFR6YIV8hf5AclVre4YCChvplnJBp7hrePIFX5ePvN45olXTN186m1se28jxbqdm5pbL\nWrj2vGn+/fJcT0EkwzWBPq/J5/MdaID9gXXtdKWcK3allAfWtQ9apRllqLjwCsqGUtV24Ec4s4wz\nCAS4ReRiVX2yyPKVlHLmtme7Vq5RZaZeUMF2H3XJRM6ivLg2D5lGkrl+AP35gWSq1o5raV5IkDrT\n6DqX4ss3JpRPa5Vc8gbPkYgY6qUbdvvKvSet3PzIBmZNbAzNvKI8s3kfF8wa3+ea0ddk62WVT6V+\n0MBt3HWY+9bu8AsoPaPmXSfIlr2dOe/rYKEa+pmVg36lzgKo6qvALYFdPwRqyliUM7c907WyKbvo\nKDw4Gr1iTrP/BU2llavnT2XK6OF5rTnhPe8ppDgjddtTr2b9AQR/IMe7nXbkuX4gme6Bp4DuX7vD\nd194hu3Bde282XmcO1a8zvjGYVwRKYzLNrou1kgvn9YquRIKgrM/cJStus0WL2mZxL9v2Y87MCeV\nVm576lVmTxpJQpzjogajJ6Us37yPK+Y0IxC6L9mKCeNmNwkJK36P1rYOrvkX5z5G8YwaOBltsyeP\nIpkQvz3K6m0d3L1qu3/OweymGSpV8P02FjEUr/y2TJSz8CrTtTKNSqL7l27YHdr22k94X9C46uL+\nBD+jro+ExI9MF8wcQ10y4Y9i71u7o48iz+cefOeh9VndF/eu3UFPSkPb9wb84XE/1KghK9Qlki1j\nLNpaJdqxNdPsKzr7CyYVAEhC8KyFAs++tp8/vrYfgLqEcOm7J/Hbl3aHelX9ftNeBBhW7wweMt3j\n1rYObl+2haYRDSz6rTObTCSEtDu7SWvvbCYo/4PuuiSZSKWV7z2ygXTaMXqnjBnBljf91Qu4d832\nkDttsLppyqlHKkkxjUVNZkiVs/Aqjkyjkuj+S1omsWbbQX/7ijnNXDGnOasxyPQDzfZcKOhMOOgc\nZO70Jq6a28ySVU6GTirPlNfo/Y6OMILbD6xrDxkKcEbUt/x2Izd/3Olou3lPJ7MmNDJh5Al8+fxT\nAdh56JijDFOOMry/tT3vVh2Z7k1ca5VcHVtXbj3Q615KKRedNZ53ulPMnjQylH12+7ItpNPh9xnc\nUlVGDKvj6vdOZfUbB9my763QcdGZX/CzfHLjHu569g3SqgiEZi/Be51OO4Y6+H3K9IMWnM4sXtwL\nnISJI++ECyo37DzMdx9a7z9fqAvWc0/WQpff4D0PbheTSs/OimksjCxkUkJzp8c3DMw0Wok7Lo5s\nftRsz0WNVJyh8GhxXQ/eyDJXKqd3H4LPz548KvT87Mmj/GP2dx6Pve6L7Yf5zF0ruW7hDO74w1Z3\n72Fmjj2RXzy3jS535uWRSuXvR850b6JGzlPy2Vx10er0pzbtRSE0Y7jlshb/nnd1p5GEIChphbQ6\nBjuZECezKaXUJYWGpNCTVv/56Mwv+F1LZxnCTR8zgu0Hj6IKiQTcu2YHqbT6crVMHhUqmPRICHx6\n/jT2dR7nyU17/f37OsNdhlMKi1dtpy5BaFblzcYyGduo+yvongwykPVciql4+5N91p9rVHp2Vkxj\nsa2I5xp0ZFJC2RoGBhVUrsaC+RbbQXYfayFZMdG1KyB7rCDuCx+s4E4AG3cd9t9nXUKoT4obJwjT\n3ZPm4Rd2hvY9sXGPf48TOMpYNT9DFvTl5+t/zuWr7jjaRULwFXafuIPr97/3ywu5+dLZ3Oy2RE8k\nhA+fOZ4LZo2n42gXL+44xO9dpdydUj5y1gTOmTqaphENfepjIPxdy8b2A0d7jZk67ihPru89vJ5E\nQmJnF955xzUOC72/TPSkYf60UQyrTzJ70kj/8w2eKxhv8uJlHt2ue/KBde3s7zzOuMZhzJ48KvQ9\nKWQ9l2Iq3riMtWyuz/7WxVRDEL0gYyEi7wNmBF+nqr9yH68oqmSDjIFWSufKpsrlOomOivuzFnfw\n+KA83toVud5L3PMLZo5hWH3vfVF6XRaptHLhmRN45pV9IV99QpyitDcjM4+Pzp7IL57b5p8rGhfw\nftTRTJ5MxYr5JAtku4/BuE4mvIr1nYeO+RlSqbTyzCv7+PL5p/pxnSBjG4f5LVk8+YNxE2+1wnSO\nlQSCUkUPTSmkYow0OEZvyert/oxENLcPeu22DhBCgfwonce6/c8oSDLhxD9Sgd0Jt+pfiU86AMeN\nGQ3+Q/8Ub3AwEfxuxGWsZXJ9ZqtFifsNe7IumFm8ZZUHQiG9oX4NnAq8AKTc3Qr8qgRyDToyuZvy\n/RJkOy5f10lUnkwjn7gvrbcvmRDOmjSShRnkyfZefHdLj1Pn0TSiASCU0QOO28FLzxQIGQrBiaNM\nPXkEd6/a7u+/+KwJ3PSxM7l49kRfSQRnX7cv2xKbnhqXTOApg3z8z7liXul0ZkMBUJd07sOPn34t\npHCCbU9aIq664HZrWwdX3/kcPSklmYAL3zWBFa++6cxQCBuEYuJ9JKk0TBl9ArsOvZPVYKQhq0VJ\nKfzUdSkGDxNgeEOSzndSoePTCknX1ygJcQyHO4tsGtHANXc+5ydO3NfazpIvOd9jL7uuzs3cykfx\n3r1qOzc/soEe900nBP+3EfxdEojhdPf0zRDMVosSjG91uTMTL8Eg07LK5aaQmcU84Cxb+Kh/ZHIj\n5TvKz3bcgpl9FxDKR564c2VKRfX2pVPKi+2HebH9MJefO5nTJzSGzpHtvXgG83sPr3cV9noSid4s\no9mTRzFrYqO/vkUap54gSDIBX//wGTy5cU9o/6ljT/T/935kD6xrD9VBJBPi/+B70sqNi1v56kVn\n9DHETSMasiYH5BuTWbn1QGg0HMen5k2l42gXPZEDg21PNu467O9PCKEVCH+49GU/CSCVJhRDKBQB\nxp7UQCqtHDyaf/ffnYfe6fc1g8QpFoU+hsLjojMnMK5xmD/rEOC6hTPoONoVcl1296S55bcb2bT7\niL8/mYBTxp7IzHEnZZXp7lXb+S8Prw+52qJNN4O1S4t+u9GfaTy5dhNfHNXKzMOr4PVn+MFbe/jB\nCYGTv+j+ATcCN0ae+0ESSLrbP3ce5gKsiBH0i0/B1PdmfS8DpRBjsQGYCOwukSyDmqAS7orJXhno\nSCHl5uKn8rDl2Xy2mWYwce6UR1/cxT9cfrafdbNx9xEuaZnUx0USVK4bdh323RA9acAdeXt+8qvn\nT6Mn5fzY4twgKcW/VhBvO9ss65bLWkI//D1HjvOdh9bzkbMmcN3CGb78G3cd9tffDn5W3ggzlVaG\n1eeOyVy3cEaoSWKUhqRwxZxmNu/pW8B23cIZfkzrvrU7/P11gZ5QD65rZ822jviT4yhEx62V8ZAQ\nCrz5Vu6lcAvBS4BA4F0TGnl9/9v0pHLHU3JRl4Avn3+qkzHnvj8F/uWPW/n7y8+mPin+KF6BAzu3\n8ELDNzmxPuC67HT/tpKRa4Frh2V4coXzNxdXiQPX1hHWqn8o9J31A0nCqCklv0whxmIssElEVgP+\nHVfVTxRdqhojn8yKphEN/g8krfgumHxfn03B/3TF675CSKWd7Ts/Py+jjNl8tnEzmLtXbScVo3FU\n8ZWn99v3Mn0y5ddnK8ZJKezvPN6npiFopFThjj9s5fJzJ4dee0nLJCC7u27WxMbY63vBYwFWvXGQ\nnsD1vM+qta0j5Iro6s4ek+nqSbspq/Hvdf6MJi5/TzM/XfE6T7+yr89xQePnXVOAq+Y6rrpMbUCC\njBxeT8fb/V8f5E9OH8vw+qR/fzxOG38SLZNH8uiLu3Iq/alNw9l24CgovLynk/fOaOLdY5TvbfxY\nv+Xy+bmjpEOjdYClcG09tdPB7uSZHBh9Nj/dMoplPWezNzmBn1//QaCwDK9SU4ixWFQqIWqZfDMr\ngq4DCWz3pwlgVMHvPRJ2A0S34wK4dcneKt+4wjLv3HHTcE/hehW7UX2xdMNurj1vWqzMV8xpZsnq\n7RmVzLjGYSFjtXlPJ99zDVKQA2938YNPnu3HgLxgdTZ33cqtB7IqNy+4HmXZ5n0s3RAuiBMhY0zG\n673Vk+ViowIuizhmTxrZ55xe8WVcUDWOgxkMxWnjT2Lb/rfoSYOQ5o0TPht/Am9CE1XGR5y/2zKN\nuIO8HXn9HvevBng9PYlvd1/Paj0ztF+AdzeP4uXdR/qkyra2dXD1T//dn+001CVY9PHZocytT82b\n2ifoPgb407YORkW+t9VgJDwKWVY1zlM25Mk3syI4k9DAdjb3VJBsI+aFM8fwYvvh0HY2GTfuOuz7\nRtLpNIt+u5GeVJq6SHsIbzQd1HleDn7H0S4/Tz46ws02yp87vYkPnzmhz2gVet0mnltm855Olm7Y\nTVyYzDMQ0RYVkNmtF41b5Mszr+zr03IjkXBM5t2rtocMlmeoXtvbycMv7Mp4Tq/mIspjDd+hJbEN\nVgOrnZHzKxHf9VzgxnwUdSaOAA05j6o6DjVfyOKZP6TpxBNCGUmeS27DzsOs33k49H1NiKPc0+r8\nH/RsvndGE4LTmsTD66WFKqm0kkwmOGdiY+j3pRDajnZeCE7Co90XulLK3au2h+JpHsVwR5eSQrKh\nFgD/DJyJ81VLAm+r6sgSyVYT5JvNtCEQpAxuZ3NPBck2Ym4cXu/XKoi7HZUxGAD3MoKcR/y4QVdP\nmiWrtvOg+0V2vvjhlNVgR9SVWw/4aaadx7p9n3+uUf6Xzz+VpyPpsLhiBDOcgiQEPxPr6vfGG4kg\nrW0dfVIn505v4voPnBIo5IvHqx34SvJRbqq/J/OBruK+FpxR+NKw//q26Ih8kPHi1av5zStdfaqs\n/Zlsdxpx72Xwk24IpI0WUncwGliQpbg1WoyYgD6p0N4AJPg99Qz+7EkjaRxe36fID/DfD5G6kmCj\nxta2DnYeOkYyKX7SQTIpfvcFb1AVTPEthnEoV2V3IW6on+B0mb0PJzPq88AZpRCqlsg3mylTW4to\nUVrQXRVl855OP+simg0VrFWINViBDKP9ncczFlMFv8jeebu6nV5C0dbZ+RQ1xY2W5k5v4u8va+G7\nD68PBX8zjfcF+OT4vfzo8H+E/cBS9y8LQaXtZZwA3ATcNAiV+HVd/5nl6fdkfF4E7v/K+1i59QD/\n9G+bQ11pyRKE9zineZTfYsWjp62DyaPj63i8IsO0amhELzgZYLmSOzJ9x/KNt8UVLLa2ddBxtKtP\nV4JZExsAlN23AAAfOElEQVRj62qC/8d1aw42aoTe9PKE9A44EiLMmtjI4usX8MC6du5vbc/ZHboQ\nylnZXWiL8i0iklTVFPBzEXke+HYpBBORjwI/xpnB3KWqt5biOsUgn+njFXOaua+1PdTbCfJU9Dij\nH684KxhE9q4fV8PhsXLrAT/DqCelPLlpr+tG6Q2cThg5jP1vHSedxq+ByGQIW1/dzivJT/dJ6yuE\nrFkmcRzOfUi18EjqfXyt+69D+5IJOGN8I+2HjvHWOz3+qoCLr1/A5j2docK74CxxwshhvLt5tJ/5\nk2nmlQtVJ8329AmNfdwxuQxFQ1L6GIpbH3+ZO/+4lbTCCTGZYR1Hu0hrb0uSuoS4hkP6tHiJI5NR\nyDWTL9T45Ktsg+eN69YcbP0SvJ9eq5kbP3Qac6c3+TGnYs0CylnZXYixOCoiDcALIvKPOCm0iVII\nJSJJ4HbgYqAdWCMij6rqplJcrxzMnd7Eoo/H94DKpug97l2zvc/2tedNA1XS/zCea1NdIXdIkBvJ\nw8fdRdiP7Y7e/RF6IGJ1Q45TVRundi0JubySMa6RKJ574ZzmUazbfghNKw314YWKosVj2c6XSjvZ\nQB7TTh7B6BH1bN7TyfJILYmXbqs46b17Nu3l6Vf2ctG7JlCXcM6VTDhGPa4VSibWbOtgbVsHiYSQ\nVKeIzWuV0R0TMBfgA6f3bSR596rtIVfe8ZjMsKhSv27hDO569g1SaWXRoxvYuOswsyePylgpn8ko\n5DuTj5JJqfZH2cYZJK9qHnX6dwU7E0fb6RRTmZezsrsQY/E5HOPw18B/BKYCV5ZCKGA+sEVVtwKI\nyD3AZUD1G4vdL8JDfwX7NvZ56lqIVeiZ9gd5BMJZJfvx89NKYrFLxbd3cvcLB/uk3Hp47jnJ4Cbz\naEhKoNVD2A8O4ZhFy87DoYBkPvrVy4zy6hhEnNoHbzb3v55+lT1HjoeOL4S2g0dpOwgvtq/v81zc\n+06lnRTf+qRwzXlTaZk8ioefd+os8r224o5608qHz5zAse4Ul7RMYtbERn664nWe2rTXr/hO4Mx8\n4hpJLt0QLrWKywyLKnUv/qU4Qd7F7gzJa7EebW+RzSj0R+FmUqrFULbRPmmLPtGSca2YYtNf49kf\nCsmGahOR4cAkVf1+ySRymEJv4h44s4vzSnKlQzvgtpaSnLra2fTxxzg2toXP3LXS98F6WSD1gSn5\n7cu28KPfb3b7QDkouL7Z3mZ93mJFd7tty5MC3/jIrD5FesF6hSgicM38ab2N4rrTsS0ruiIa/9yp\no/1qc3CUjudL3td5nIH6sVThrmffYNqYE7nlsY1+0V656U4p+zuPZ027DTK2sYEpo4ZzytgT/eys\ntMIzr+wlrbBm20FuvnQ2z7yyz7/PArzfnVEAfOeh9aFEgUtaJvmuUIBPnOPUvATTryHctrvzWHfG\nCu2ubqfCOi4V1XMVRc9dKJmUajGUbVyftHJmNpXrWoVkQ30c+CccZ8UpInIucEsli/JE5AZcr8i0\nadmzYzJSJYbiquM3s1bfxQ8+eXZspk8wZgGEjosqcy+45jUrA/qseJYU+MaRSdw4N36thuAPJzj6\nihqUaLO++9bu8JVCMtl3pHbHitezpq6mFSaPHu6/N2fJ0VQovTGO1a6LpS6ZIK1Kym3nraqk0s5I\nefzIYUw7eURBo/GwbOqnQVaSvUfeySmD5xY7+FYXb73Tw+wp4XbjqXRvMsPSDbtDTQcT4rjJNu/p\nZNGjG/r0WLr2vGlsP/A2d/5xK6rw+IY9PL5+d0jRA35AtyflrMaXyVWXJnMqaiEB3FxZQZmU6kCV\nbTU0+SsHhRblzQeWA6jqCyJySglkAtiJ4+byaHb3hVDVO4E7AebNm9ef3z8sOgyL3IBb3XCY/yWY\nex2MObVfp8uE16Pf+0It+VLvqN3LTgkW63mv8b780am/V/gGvd1NnWKwXjeL16zsyjnNoYZ20dXv\noj+WuOylTAYFnF5MD6xrB+hTbRz1defqXSQ4/t/vPrTeXx8gkRB/GdLggjtRPL+z92zQn+/5//d1\nHs9oKKaMPoHh9cnQam+eTOI2j7ukZRKrth7oM7MpFZ7x9y5XlxROGXtiSLnG0XhCHZ3v9IRWVvTa\nvSeTQgJ8g39JyyRWvXHQ//4kEsKS1dtJRAoLu3vS3PbUq3z9w2fQebynt+ura7i8/73vQzCuE1cr\nM39GU6zhztaVObpAk0cl13sopyuokhRiLLpV9bBIKAm0VL+YNcDprjHaiZOye22JruUYjAGQT55z\nMCMpuBhPcHquOG2avXMGv/wfnT0xdD6vwtfH+zFGPhGBUP8cgPGNw/jqRZkXNYojzqC0tnXwabfl\nMjhB12A3zyvnNGc1eHGc0zyqTwA57Z6/LiFc/4FT+Nf/90a/lXW2OMi+zuN8/xMtodG0F+gOpkie\nOWmkr6wTwIhhSd46Ht/sLhu5guIA15w3jSvnNIdqRm576tWc5z7yTk/vdRISMpIJERZ9fLZfWLlx\n12HSafd+q5Mx58Q3NCSj4iz3umrrAdKB/YmEkJRe4+PFe4LpuXHvc3RgcSiPaIquHzh21y33ZiqZ\nVniMNsAslwKv9oK6YlCIsdgoItcCSRE5Hfgq8O+lEEpVe0Tkr4F/w0nQ/FdV7RsxrgLyHdFkmqrm\n2xDvwNtdoZTKYOGd1z/Iey64et0Vc5p50B31e+w9cpxbHtvYZwGlQnlwXXto9J5Kw0VnjeecqaPD\nxUyBxnpBX3ccjcPrM7ayUFWOHO8JKesok5uGs7PjWL/eT3dKnWJJ6VVvTrpxmimjhwN93XkN9Qk+\ne970nIV+cQTfY33S+cyCNrAu0VvsFvycojEDgGF1CY4H5Aoq6HRaQxXjqVSaDbsOI8CPn3qV7pSG\nDEL09YIz69rptiEPHu+8SFl02dkhd+SD69r9/l5nThrJS+2HwxXw4rR2aQg0/KuPpOh6geNU2jEU\nF8waz1Mv781rhcdsnYON/lGIsfgb4Ls4TQSX4Cjyvy+FUACq+jjweKnOXyzyTb3LlCIb/eFnapUR\nXYM76BeNHhtdvCfaztsLKg40JztOoe898o5/3WDueVdPmo27j/CVD87kiY17aAus0uZRH6h29eIj\nF5wxjuWb9/kKwxtZgjOql0TYLTXqhLq+/kqXfEby+zuP92kX7lXWe63Pg7x7yiimjTmRgdKTctxt\nwST9RCI+z81zP967ZjvD6hK8sOMQ3ak0SVcBL5g5hsc37OmzGh30Dibud2t+4u6H4Cy5uu3AUcC5\nZ7uPHKc+Kf7nkNLedUZUHfdpMJEh6rb0KqDT9K4H4a0jn2uRIm+WM7ZxWN4rPJaz/mCoUEg21FEc\nY/Hd0olTe+Qb3Mq0nsWsiY3Uue0B6pJOtSfE+0EzpeNFDVE0QP5cIEvII43j8orLMsl3zYaWyaOo\nSxByca3f6ayP7S0M48VS0uq4MLzsm7iMovdMHc21500LvU9wFKA3a/KaEAowbcwIzpjQGOozVZ/M\nnEiccAvjXo5pCe7hKaSgG8xb7nXDzr6zmdXbOli3/VDG8+WL4swAgmRbO9zri3X7si2sbevwDcLe\nI8d5YuMezj9jXGg24cUsPjXPCQUuWb09o5uovi7BDR88NZS1pqp86r3TmDx6uN824+ZHNpB260+8\nzyr43bnxQ6f1aQsTV1mdSYnHNVHMVtQWnYENhaBzOclpLETk0WzPD/UW5fkGt7IVBXlKIrg6WqZr\nZVLei37rtCBY9Ubf9bknjDyBuNRRp4V2bwbL5j2d3LtmO5t2HyGVjl94Phqov+Wys1m+eR8vtR9i\n75HjpNUp0npwXbtTpa5hF4e3Et3i6xdw4+LWUK2C50YJpkx6roQ61w1Rl0z46yG0HThKe8fR0Hta\nOHMMLwcWuQkqwVQaXsliKOoSwpVzmmmZPIr/EmhFkkwK963dkTFOknaL2wptUDhx5DD//Qfdh97o\nOx8l5ynUaN+h8Y3D/M4AyZgGkQ94bqJkgjMnNvpuIi8xwRtweC07vJlAUMFHBy9x3Y29AVJ/XEHZ\n0l37+1qj/+Qzs1iIU/OwBFhF3zZHQ558glvBUXYwpTRTI8FCsjuC7pGunr7LOV4wa3xsl1dPuXX3\npLljxet9MpW8zJbgDy56reWb9/GH194MZ76Av2BPUIE6WUW9rUQuP3dKyNefrVtul9umpD4pnD1l\nlK/cohmkr+9/m0WfaOF7D6+PLb4L7hIcQ5BOO20obrnMSaO+d024hfr0k0fweiRDCno7mnr+9L1H\n3gkp3WBhYbQXVzIhfPWiM/w+Q0H3YdzoOxOeUoz2HfJcPJlmonFuouAIHugzy8s1ko8OiILdVvvr\nChpI4HgoBJ3LST7GYiJO241rcDKSfgcsqdaAczWTdkfZwZz2TN1oC/G5RnVidDvYrDCIp+zq6xLs\nO9J3acxkMuGnr3oGK3oOL+c/ut+r1vW63SZc7ZlKO+tfbz/wdh/32JHjvRk8XgdPz8D6o+aU0jJl\nFJv3dtIVGE17PPOK0zojaChOG+fEFIIpsSLwXy8/21eGXlZQMBPK4+QTG8KvxXHpXDBrPADLX32T\np17eS10y4fv1EwnxV/oTnMK1x9fvpiulJAX+3m3KWIxKX08pxrlo8p2lFqNaupA4m1F75DQWbtPA\nJ4AnRGQYjtFYLiLfV9WflFrAUnPr4y/zxMY9fHT2RG762Jm5X9BPHlzX7rct7kmpP/rP1I22kEKf\nK+c085s12+lJ92bQBGmKSVEEuOFPZtI4vJ6mEQ0s27yPoKsqIXD+GeN4OpJ9cuWcZu5fu8M3AAtn\njvEVt4gTcFY3E6tl8ijul3ZAQZwRvDMb0Njsof2djksm6n5618TeOIMCjcPq/K6m0ZqLVFr7GL7z\nZo7hzc7jIYV/8ZkTQrGdTCvP1SeF426dgjdjOHXcibxx4ChPbtrruI7chnndPWkuPstZF/qe1dv9\nimgFHntpt78OSFSZF2v0W+lReCFxNqP2yCvA7RqJP8MxFDOA/wU8VDqxysOtj7/sKy3vsVQGY1/n\n8djtxmHhj8Dbzsfn6gUPm0Y0IG66Z7AOxuvTf7y7bw1AXVK42K3dCLZW9nSv4NRjRNMRV249wF++\n/xQ/3vGL57b5TeLSqiRFuGr+VH+U69WWeIYiG8s37/PfkzerSqWVYxH5N+4+QuPwen+mFmXU8PpQ\n+vDsyaNY9NveiXBdUvjy+b1Fl9GV54Izh+Wb97F+52G/hXxdUnjjwFHfSKXS7qwJxygsf/VNrprb\n3KeeI5V20nJ/8Mmzi9K+olqJq8cZbO9xqJJPgPtXQAtOGuv3VXVDyaUqE09EUkqf2LinZMZifOOw\n2O2oKya4ne2HFhx9B6uau91ZS7TtdRR1g+lAaLGYukRvr6dgaqPXj8hr3eC5mbrdlFhvdJ1yaxLm\nTm9i855OX2nmE/pNuTJFZ1XnTh3tp3GCU5AYDewGeXbLfn9ltL983ww6jnaF0mEvnDU+dF+j7Uy8\nYPDKrQf8vP6EwPtPG8u0k0ewZHVvB+BkQrjwXeN50s08SqXSfnO8aBzn/tZ2WryeV5b/b9QY+cws\nPouzku7XgK8GRq4CaC2vlPfR2RND7pBolXQxifbw97ajmUrOdm6Co+/oggRKTGfQwHMJwpk22Wo0\nWts6uL81Wl+gfvuNbL7pTLGSOLzYSVzH0ZVbD/QpSPSOueW3G/sU6AUTBu569g1uuayFumTCfw/e\nDCboCso0iwveG6+x3gNukN8Lis+a2MgfXnsztFaJF1x+ccchP7mgJ1WcoK9hVIJ8YhY11QG7ELxZ\nRDliFplWxItmKnlB01wEs6igd42G+qST/rl5T2eo2O/LH+yNT0T95tncXdFCNKHvUpVB37TnqoLe\nhZ3iRv9REgI3Xzo71pe/eU+nHxuoSybYeeiYr+xv/vhsrrnzObpSTmuKy86dzGMv7fazsNLqdAG9\nam4zS9xuuN4MJpe7JJMRyXff3OlN3L1qu//5ptWZFVnQ16hFClopbzBy08fOLKmR8Fgwcwz17ii1\nLsPoO9pIMBtR4/Pp+b0FU57i237g7YyGMOo3zzS6jcZaTh1/Ej+88t2xx+86dIwfP/VqKHsqGIjO\ntUZF3HsPtnxwYirKPat71wmfO72JJTcsDCnq+aeMCdUHRFtQFKKkMxmRfPZ57yn4OTUOr7f8f6Mm\nGfLGopykAo0EPYKZSkpvnUUcwerY6HKs0VYJrW0d/OK5bXT1pPnFc9u4ePZE//lCajiisZbzTjm5\nz7He+YIziGAzNy8QLRBbvBZ0QUUJBp/TCqR6YyXZqpvjsnAqoaSjn1Mu42wY1YoZizJxx4rX/dz/\nlDrb//L5eaHRdLaZRZyCz6b8stVpFFLDkWnt8LhrhbKJssRENuw67BeQJRLCWZNGcvV7p8XKEAo+\nJ/ouV5nJ8BUy+i8lVklsDBbMWJSJaO6/tx2cSURnFsGZRJyC9xaBj9La1sGuQ8dC7cKDvXsyPZeJ\nT81tRuntgBolUzZRthH9lW433PvW7mD9zsNs3hvfBTeu2jh4rmCzwmoNGNtMwhgMmLEoE1e/d1po\nveWr3+sUhD38fLh9+MPPt3PtedNi++zk27DQL2hLJrjarXmI9u5JJoSWKaMyjuij52oItIGIkmv0\nnGmU77VWz6djbzQY7TFUVikzjEpjxqJMbD/wduz29oPhRnjednQm4TXfK6RhYbDmIfpcOqW81J55\nRB8nQ3+aHGbDW9jGWwEvW7wmEwNx8+SzaJVhGA5mLMpEpgLABTPH8PALu/z93sg4rvFgPgo500g7\nY6+lLEaglKN2L8vJC3an0trvBZn6Y6gquQynYdQiZizKRKYCwNMnNIaOC257S11662fnMxKOG2lH\ney1dfNYEf0GhbEagFMFZ7z3sPHQsVL+Ry3AVG1scxzAKw4xFEchHid/0sTPZc+Qdlr/6JhecMc6v\ne4i6Xrzt4LrZPWkne2rF5n10p5T6pLDkhoV5u4SivZbOmTqaL59/al5GoJjB2Wg8pS7hLPpUyPoN\nxcJiHYZRGEPeWAzUb52vO6O1rYPfrd9Nd0r53frdfG7hDOZOb2JjpEW5tx3tRvvG/rf91tldKeWB\nyJoV2YhTjJXI0InGUy46cwLHulPMnjSSxuH1ZY8dXDGnOXY5T8Mw+jKkjUUx/NYrtx7wi9GyrWv9\n0xWv+6u3daeUn654nTs/Py/jWhRXzGnmN24r8PqkMHPsiWzZ95Z/XNSYxBm94L5qyPWPptgu37yP\nnrSyZtvBssYMop97XO2IYRhhhrSxKIbfOliBnSZzBfbeSJ2Ftx1cH8Lr6+Qhgb8LZo1n+atvxhbH\nxRk9oM++Gz90WkHvrdgEYyA7Dx1z1nyoQMzA4hWGUThD2lgUw2/dcbTLXwciIZkrsDPVWcyd3sSi\nT7SwdMNuLmmZFEpz7XHbgKfSTjO8JV+Knx3EKT+gKhWi5/5qbesoqFdTMdNcLV5hGIUzpI1FMbJ9\ngimudcnMimfWxEbqkk5Aty4pzJroZD15KaRdPWnWbDvop44WEmeIHustEVpIlXa5KeTeFzvN1Vpw\nGEbhDGljAUXK9vHWk9BoBKKXlVsPkPbaZgdaZGeKeRSi0ILHNo1o8I1PtIK72sj33pfCbWQtOAyj\nMIa8sRgoUXdRJkUWXH8irb2xjWwxj1wKLeqaifZKilZw1yqlcBtZ9bZhFIYZiwGSryLLtPhRptTZ\nXGRyzQxGf3yx3UZWvW0YhWPGYoDkq8ji1jWAvkuO5rMEKWR2zZTTH1/O0Xkx3UaWDWUYhWPGogjk\no8gyKfFsqbPZyDaDKIc/vpZH54Nx9mUYpaZixkJE/jvwcaALeB34C1U95D73beCLQAr4qqr+W6Xk\nLCaZWnVHlwXN91yVzOip5dF5pe+dYdQilZxZPAl8W1V7ROSHwLeBb4nIWcCngdnAZOApETlDVVMV\nlLWk9HcmUMmMnlofnVs2lGEURsWMhar+PrC5ErjK/f8y4B5VPQ68ISJbgPnAc2UW0ciCjc4NY2hR\nLTGLvwTudf+fgmM8PNrdfX0QkRuAGwCmTZtWSvmMGGx0bhhDh0QpTy4iT4nIhpi/ywLHfBfoARYX\nen5VvVNV56nqvHHjxhVT9JLQ2tbB7cu20NrWUWlRDMMwCqKkMwtV/XC250XkOuBS4CJVv/x5JzA1\ncFizu6+mqeXsIcMwjJLOLLIhIh8Fvgl8QlWDC1E/CnxaRIaJyCnA6cDqSshYTDI1+zMMw6gFKhmz\n+AkwDHhSRABWqupXVHWjiPwG2ITjnrpxMGRC1Xr2kGEYQxvRLM3vaol58+bp2rVrKy1GVgqteLb+\nRYZhlBoRaVXVebmOq5ZsqCFBIdlDFuMwDKOaqFjMwsiOxTgMw6gmzFhUKV6MIylYjMMwjIpjbqgq\nxSqkDcOoJsxYVDFWIW0YRrVgbijDMAwjJ2YsDMMwjJyYsTAMwzByYsbCMAzDyIkZC8MwDCMnZiwM\nwzCMnJixMAzDMHJixsIwDMPIiRkLwzAMIydmLAzDMIycmLEwDMMwcmLGwjAMw8iJGQvDMAwjJ0Pe\nWLS2dXD7si20tnVUWhTDMIyqZUi3KLelSw3DMPJjSM8sbOlSwzCM/BjSxsKWLjUMw8iPIe2GsqVL\nDcMw8mNIGwuwpUsNwzDyYUi7oQzDMIz8MGNhGIZh5MSMhWEYhpETMxaGYRhGTsxYGIZhGDkxY2EY\nhmHkRFS10jIUBRF5E2jr58vHAvuLKE41Ye+tNrH3VpvU4nubrqrjch00aIzFQBCRtao6r9JylAJ7\nb7WJvbfaZDC/N3NDGYZhGDkxY2EYhmHkxIyFw52VFqCE2HurTey91SaD9r1ZzMIwDMPIic0sDMMw\njJwMeWMhIh8Vkc0iskVEbqq0PMVCRP5VRPaJyIZKy1JsRGSqiCwTkU0islFEvlZpmYqFiJwgIqtF\n5EX3vX2/0jIVGxFJisjzIvJYpWUpJiKyTUTWi8gLIrK20vIUmyHthhKRJPAqcDHQDqwBrlHVTRUV\nrAiIyAeBt4BfqWpLpeUpJiIyCZikqutEpBFoBS4fJJ+bACeq6lsiUg88C3xNVVdWWLSiISLfAOYB\nI1X10krLUyxEZBswT1Vrrc4iL4b6zGI+sEVVt6pqF3APcFmFZSoKqvoH4GCl5SgFqrpbVde5/3cC\nLwNTKitVcVCHt9zNevdv0IzoRKQZ+DPgrkrLYhTGUDcWU4Adge12BonSGSqIyAzgPcCqykpSPFw3\nzQvAPuBJVR007w24DfgmkK60ICVAgd+LSKuI3FBpYYrNUDcWRg0jIicBDwBfV9UjlZanWKhqSlXP\nBZqB+SIyKNyIInIpsE9VWystS4n4gKrOAS4BbnRdwYOGoW4sdgJTA9vN7j6jynH9+Q8Ai1X1wUrL\nUwpU9RCwDPhopWUpEu8HPuH69u8BLhSR/1tZkYqHqu50H/cBD+G4uQcNQ91YrAFOF5FTRKQB+DTw\naIVlMnLgBoF/Brysqv+j0vIUExEZJyKj3f+H4yRfvFJZqYqDqn5bVZtVdQbOb+0ZVf1shcUqCiJy\noptsgYicCHwEGFSZiEPaWKhqD/DXwL/hBEl/o6obKytVcRCRJcBzwCwRaReRL1ZapiLyfuBzOCPT\nF9y/j1VaqCIxCVgmIi/hDGaeVNVBlWI6SJkAPCsiLwKrgd+p6hMVlqmoDOnUWcMwDCM/hvTMwjAM\nw8gPMxaGYRhGTsxYGIZhGDkxY2EYhmHkxIyFYRiGkRMzFoZhGEZOzFgYQwYRuVxEVETeVcBrHveK\n5PpxvbdyH1U6ROQ7lby+MbiwOgtjyCAi9wKTcSqH/y7yXJ1bpOltC87vo98N70TkLVU9qd8CD5BK\nX98YXNjMwhgSuE0HPwB8EafVBCJygYj8UUQeBTaJyAx3Iaxf4bRqmOouaDNWRG4VkRsD51skIn8r\nIieJyNMiss5d+CbvFvci8i33NS+KyK3uvnNFZKWIvCQiD4lIk7t/uYjMc/8f6/ZXQkSuE5EHReQJ\nEXlNRP7R3X8rMNytbl888DtoDHXMWBhDhcuAJ1T1VeCAiMx198/BWVzoDHf7dOB/q+psVW0LvP5e\n4M8D23/u7nsH+KTbbfRDwI/cWUlWROQSV6bzVPUc4B/dp34FfEtV3w2sB/4uwymCnAtcDZwNXC0i\nU1X1JuCYqp6rqp/J4xyGkRUzFsZQ4RqcTqe4j9e4/69W1TcCx7XFrUqnqs8D40VksoicA3So6g5A\ngB+4vZyewlkPZUIe8nwY+LmqHnXPf1BERgGjVXWFe8wvgXzaXD+tqodV9R1gEzA9j9cYRkHUVVoA\nwyg1InIycCFwtogokMRZqOZ3wNuRw6PbQe4DrgIm4swqAD4DjAPmqmq36x46oXjS+/TQO7iLnv94\n4P8U9rs2SoDNLIyhwFXAr1V1uqrOUNWpwBvAnxR4nntx4h1X4RgOgFE4C/p0i8iHyH9U/yTwFyIy\nAhyDpqqHgQ4R8eT6HODNMrYBnuvsqjyv0e2u+2EYA8aMhTEUuAZnMZogD9DrisoLt319I7BTVXe7\nuxcD80RkPfB58lx7wm1f/Siw1l1C9W/dp74A/HfXrXUucIu7/5+AvxKR54GxeYp8J/CSBbiNYmCp\ns4ZhGEZObGZhGIZh5MQCYYZRQkTkbODXkd3HVfW8SshjGP3F3FCGYRhGTswNZRiGYeTEjIVhGIaR\nEzMWhmEYRk7MWBiGYRg5MWNhGIZh5OT/A+CXUHFfw1zbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d45390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xwithoutnan,ywithoutnan,'.')\n",
    "plt.xlabel('Arrival_count')\n",
    "plt.ylabel('Mean_Arrival_delay')\n",
    "l = plt.plot(xwithoutnan, slope*xwithoutnan + intercept, '-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
